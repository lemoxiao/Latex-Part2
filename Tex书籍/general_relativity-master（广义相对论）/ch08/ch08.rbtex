<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '08',
    %q{Sources},
    'ch:sources'
  )
%>

<% begin_sec("Sources in general relativity") %>
<% begin_sec("Point sources in a background-independent theory") %>
The Schr\"{o}dinger equation and Maxwell's equations treat spacetime as a stage on which particles and
fields act out their roles. General relativity, however, is essentially a theory of spacetime itself.
The role played by atoms or rays of light is so peripheral that by the time Einstein had derived
an approximate version of the Schwarzschild metric, and used it to find the precession of Mercury's
perihelion, he still had only vague ideas of how light and matter would fit into the picture. In his calculation,
Mercury played the role of a test particle: a lump of mass so tiny that it can be tossed into spacetime
in order to measure spacetime's curvature, without worrying about its effect on the spacetime, which is
assumed to be negligible. Likewise the sun was treated as in one of those orchestral pieces in which
some of the brass play from off-stage, so as to produce the effect of a second band heard from a distance.
Its mass appears simply as an adjustable parameter $m$ in the metric, and if we had never heard of the Newtonian
theory we would have had no way of knowing how to interpret $m$.

When Schwarzschild published his exact solution to the vacuum field equations, Einstein suffered from philosophical
indigestion. His strong belief in Mach's principle\index{Mach's principle} led him to believe that there was
a paradox implicit in an exact spacetime with only one mass in it. If Einstein's field equations were to mean
anything, he believed that they had to be interpreted in terms of the motion of one body relative to another.
In a universe with only one massive particle, there would be no relative motion, and so, it seemed to him,
no motion of any kind, and no meaningful interpretation for the surrounding spacetime.

Not only that, but Schwarzschild's solution had a singularity at its center. When a classical field theory
contains singularities, Einstein believed, it contains the seeds of its own destruction. As we've seen on page
\pageref{slime-and-socks}, this issue is still far from being resolved, a century later.

However much he might have liked to disown it, Einstein was now in possession of a solution to his field
equations for a point source. In a linear, background-dependent theory like electromagnetism, knowledge of such
a solution leads directly to the ability to write down the field equations with sources included. If Coulomb's
law tells us the $1/r^2$ variation of the electric field of a point charge, then we can infer Gauss's law.
The situation in general relativity is not this simple. The field equations of general relativity, unlike
the Gauss's law, are nonlinear, so we can't simply say that a planet or a star is a solution
to be found by adding up a large number of point-source solutions. It's also not clear how one could
represent a moving source, since the singularity is a point that isn't even part of the continuous structure
of spacetime (and its location is also hidden behind an event horizon, so it can't be observed from the outside).
<% end_sec %> % Point sources in a background-independent theory

<% begin_sec("The Einstein field equation") %>
<% begin_sec("The Einstein tensor") %>
Given these difficulties, it's not surprising that Einstein's first attempt at incorporating sources into
his field equation was a dead end. He postulated that the field equation would have the Ricci tensor on
one side, and the stress-energy tensor $T^{ab}$ (page \pageref{sec:energy-momentum-tensor}) on the other,
\begin{equation*}
  R_{ab} = 8\pi T_{ab}\eqquad,
\end{equation*}
where a factor of $G/c^4$ on the right is suppressed by our choice of units, and the $8\pi$ is determined on
the basis of consistency with Newtonian gravity in the limit of weak fields and low velocities. The problem
with this version of the field equations can be demonstrated by counting variables. $R$ and $T$ are
symmetric tensors, so the field equation contains 10 constraints on the metric: 4 from the diagonal
elements and 6 from the off-diagonal ones. 

In addition, local conservation of mass-energy requires the div\-ergence-free\index{stress-energy tensor!divergence-free}
property $\nabla_b T^{ab}=0$. In order to construct an example, we recall that the only
component of $T$ for which we have so far introduced any physical interpretation is $T^{tt}$, which
gives the density of mass-energy.
Suppose we had a stress-energy tensor whose components were all zero,
except for a time-time component varying as $T^{tt}=kt$. This would describe
a region of space in which mass-energy was uniformly appearing or disappearing everywhere at a constant rate.
To forbid such examples, we need the divergence-free property to hold. This is exactly analogous to the
continuity equation in fluid mechanics or
electromagnetism, $\partial \rho/\partial t+\nabla\cdot\vc{J}=0$ (or $\nabla_a J^a=0$),
which states that the quantity of fluid or charge is conserved.

But imposing the divergence-free condition adds
4 more constraints on the metric, for a total of 14. The metric, however, is a symmetric rank-2 tensor itself,
so it only has 10 independent components. This overdetermination of the metric suggests that the proposed field equation
will not in general allow a solution to be evolved forward in time from a set of initial conditions given on
a spacelike surface, and this turns out to be true. It can in fact be shown that the only possible solutions
are those in which the traces $R=R\indices{^a_a}$ and $T=T\indices{^a_a}$ are constant throughout spacetime.

The solution is to replace $R_{ab}$ in the field equations with a different tensor $G_{ab}$, called the
Einstein tensor,\index{Einstein tensor} defined by $G_{ab}=R_{ab}-(1/2)Rg_{ab}$,\index{Einstein field equation}\index{field equation, Einstein}\label{einstein-field-equation}
\begin{equation*}
  G_{ab} = 8\pi T_{ab}\eqquad.
\end{equation*}
The Einstein tensor is constructed exactly so that it is divergence-free, $\nabla_b G^{ab}=0$. (This is not obvious,
but can be proved by direct computation.) Therefore any stress-energy tensor that satisfies the field equation
is automatically divergenceless, and thus no additional constraints need to be applied in order to guarantee
conservation of mass-energy.\label{einstein-tensor-div-free}

Self-check: Does replacing $R_{ab}$ with $G_{ab}$ invalidate the Schwarz\-schild metric?

This procedure of making local conservation of mass-energy ``baked in'' to the field equations is analogous
to the way conservation of charge is treated in electricity and magnetism, where it follows from
Maxwell's equations rather than having to be added as a separate constraint.


<% end_sec %> % The Einstein tensor
<% begin_sec("Interpretation of the stress-energy tensor") %>\label{sec:more-energy-mom-tensor}\index{stress-energy tensor}
The stress-energy tensor was briefly introduced in section \ref{sec:energy-momentum-tensor} on page \pageref{sec:energy-momentum-tensor}.
By applying the Newtonian limit of the field equation to the Schwarzschild metric, we find that
$T^{tt}$ is to be identified as the mass density $\rho$. The Schwarzschild metric describes a spacetime using coordinates
in which the mass is at rest. In the cosmological applications we'll be considering shortly,
it also makes sense to adopt a frame of reference in which the local mass-energy is, on average, at rest,
so we can continue to think of $T^{tt}$ as the (average) mass density. By symmetry, $T$ must be diagonal in
such a frame. For example, if we had $T^{tx}\ne 0$, then the positive $x$ direction would be distinguished from
the negative $x$ direction, but there is nothing that would allow such a distinction.

\begin{eg}{Dust in a different frame}\label{eg:dust-stress-energy}
As discussed in example \ref{eg:dust-and-radiation-cosm} on page \pageref{eg:dust-and-radiation-cosm},
it is convenient in cosmology to distinguish between radiation and ``dust,'' meaning noninteracting, nonrelativistic materials
such as hydrogen gas or galaxies. Here ``nonrelativistic'' means that in the comoving frame, in which the average flow of dust
vanishes, the dust particles all have $|v| \ll 1$. What is the stress-energy tensor associated with dust?

Since the dust is nonrelativistic, we can obtain the Newtonian limit by using units in which $c \ne 1$, and
letting $c$ approach infinity. In Cartesian coordinates, the components of the stress-energy have units that
cause them to scale like
\begin{equation*}
T\indices{^\mu^\nu} \propto \left( \begin{matrix}
                   1 & 1/c & 1/c & 1/c \\
                   1/c & 1/c^2 & 1/c^2 & 1/c^2 \\
                   1/c & 1/c^2 & 1/c^2 & 1/c^2 \\
                   1/c & 1/c^2 & 1/c^2 & 1/c^2
      \end{matrix} \right)\eqquad.
\end{equation*}
In the limit of $c\rightarrow\infty$, we can therefore take the only source of gravitational fields
to be $T\indices{^t^t}$, which in Newtonian gravity must be the mass density $\rho$, so
\begin{equation*}
T\indices{^\mu^\nu} = \left( \begin{matrix}
                   \rho & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0    
      \end{matrix} \right)\eqquad.
\end{equation*}
Under a Lorentz boost by $ v $ in the $x$ direction, the tensor transformation law gives
\begin{equation*}
T\indices{^{\mu'}^{\nu'}} = \left( \begin{matrix}
                   \gamma^2\rho & \gamma^2 v \rho & 0 & 0 \\
                   \gamma^2 v \rho & \gamma^2 v ^2\rho & 0 & 0 \\
                   0 & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0    
      \end{matrix} \right)\eqquad.
\end{equation*}
The over-all factor of $\gamma^2$ arises because of the combination of two effects:
each dust particle's mass-energy is increased by a factor of $\gamma$, and length contraction also
multiplies the density of dust particles by a factor of $\gamma$.
In the limit of small boosts, the stress-energy tensor becomes
\begin{equation*}
T\indices{^{\mu'}^{\nu'}} \approx \left( \begin{matrix}
                   \rho &  v \rho & 0 & 0 \\
                    v \rho & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0 \\
                   0 & 0 & 0 & 0    
      \end{matrix} \right)\eqquad.
\end{equation*}

This motivates the interpretation of the time-space components of $T$ as the flux of mass-energy
along each axis. In the primed frame, mass-energy with density $\rho$ flows in the $x$ direction at velocity
$ v $, so that the rate at which mass-energy passes through a window of area $A$ in the $y-z$ plane is
given by $\rho v  A$.

This is also consistent with our imposition of the div\-ergence-free property,
by which we were essentially stating $T^{tx}$ to be the rate of flow of
$T^{tt}$.
\end{eg}

\begin{eg}{The center of mass-energy}\label{eg:center-of-me}\index{center of mass-energy}
In Newtonian mechanics, for motion in one dimension, the total momentum of a system of particles is
given by $p_{tot}=Mv_{cm}$, where $M$ is the total mass and $v_{cm}$ the velocity of the center of mass.
Is there such a relation in relativity?

Since mass and energy are equivalent, we expect that the relativistic equivalent of the center
of mass would have to be a center of mass-energy.

It should also be clear that a center of mass-energy can only be well defined for a region of spacetime
that is small enough so that effects due to curvature are negligible. For example, we can have cosmological
models in which space is finite, and expands like the surface of a balloon being blown up. If the model
is homogeneous (there are no ``special points'' on the surface of the balloon),
then there is no point in space that could be a center. (A real balloon has a center, but in our metaphor
only the balloon's spherical surface correponds to physical space.) The fundamental issue here is
the same geometrical one that caused us to conclude that there is no global conservation of
mass-energy in general relativity (see section \ref{sec:no-conservation-laws}). In a curved
spacetime, parallel transport is path-dependent, so we can't unambiguously define a way of adding vectors
that occur in different places. The center of mass is defined by a sum of position vectors. From these
considerations we conclude that the center of mass-energy is only well defined in special relativity, not
general relativity.

For simplicity, let's restrict ourselves to 1+1 dimensions, and
adopt a frame of reference in which the center of mass is at rest at $x=0$.

Since $T^{tt}$ is interpreted as the density of mass-energy, the position of the center of mass must
be given by
\begin{equation*}
  0=\int x T^{tt} \der x\eqquad.
\end{equation*}
By analogy with the Newtonian
relation $p_{tot}=Mv_{cm}$, let's see what happens when we differentiate with respect to time.
The velocity of the center of mass is then
$0=\der x_{cm}/\der t=\int \partial_t T^{tt} x \der x$. Applying the
divergence-free property $\partial_t T^{tt}+\partial_x T^{tx}=0$, this becomes 
$0=-\int \partial_x T^{tx} x \der x$. Integration by parts gives us finally
\begin{equation*}
  0 = \int T^{tx} \der x\eqquad.
\end{equation*}
We've already interpreted $T^{tx}$ as the rate of flow of mass-energy,
which is another way of describing momentum. We can therefore interpret
$T^{tx}$ as the density of momentum, and the right-hand side of this equation as the total
momentum. The interpretation is that a system's center of mass-energy
is at rest if and only if it has zero total momentum.

Suppose, for example, that we prepare a uniform metal rod so that one end is hot
and the other cold. We then deposit it in outer space, initially motionless relative to some observer.
Although the rod itself is uniform, its mass-energy is very slightly nonuniform, so its center of mass-energy
must be displaced a tiny bit away from the center, toward the hot end. As the rod approaches thermal
equilibrium, the observer sees it accelerate very slightly and then come to rest again, so that its
center of mass-energy remains fixed! An even stranger case is described in example
\ref{eg:p-in-static-fields} on p.~\pageref{eg:p-in-static-fields}.
\end{eg}

Since the Einstein tensor is symmetric, the Einstein field equation requires that the stress-energy
tensor be symmetric as well. It is reassuring that according to example \ref{eg:dust-stress-energy}
the tensor is symmetric for dust, and that symmetry is
preserved by changes of coordinates and by superpositions of sources.
Besides dust, the other cosmologically significant sources of gravity are electromagnetic radiation and
the cosmological constant, and one can also check that these give 
symmetry.\index{stress-energy tensor!symmetry of} Belinfante noted in 1939 that
symmetry seemed to fail in the case of fields with intrinsic spin, but he found that this problem
could be avoided by modifying the previously assumed way of connecting $T$ to the properties of the field.
This shows that it can be rather subtle to interpret the stress-energy tensor and connect it to experimental
observables. For more on this connection, and the case of electromagnetic fields, see examples
\ref{eg:t-of-em-wave} and \ref{eg:t-of-static-em-fields} on p.~\pageref{eg:t-of-em-wave}.

In example \ref{eg:dust-stress-energy}, we found that $T\indices{^x^t}$ had to be interpreted as the
flux of $T\indices{^t^t}$ (i.e., the flux of mass-energy) across the $x$ axis. Lorentz invariance
requires that we treat $t$, $x$, $y$, and $z$ symmetrically, and this forces us to adopt the
following interpretation: $T\indices{^\mu^\nu}$, where $\mu$ is spacelike, is the flux of the
density of the mass-energy four-vector in the $\mu$ direction. In the comoving frame, in Cartesian
coordinates, this means that $T\indices{^x^x}$, $T\indices{^y^y}$, and $T\indices{^z^z}$ should be
interpreted as pressures. For example, $T\indices{^x^x}$ is the flux in the $x$ direction
of $x$-momentum. This is simply the pressure, $P$, that would be exerted on a surface with its normal
in the $x$ direction, so in the comoving frame we have $T\indices{^\mu^\nu}=\operatorname{diag}(\rho,P,P,P)$.
For a fluid that is not in equilibrium, the pressure need not be isotropic, and the stress
exerted by the fluid need not be perpendicular to the surface on which it acts. The space-space
components of $T$ would then be the classical stress tensor, whose diagonal elements are the anisotropic
pressure, and whose off-diagonal elements are the shear stress. This is the reason for calling $T$
the stress-energy tensor.\index{stress-energy tensor!interpretation of}

The prediction of general relativity is then that pressure acts as a gravitational source with
exactly the same strength as mass-energy density. This has important implications for cosmology,
since the early universe was dominated by radiation, and a photon gas has 
$P=\rho/3$ (example \ref{eg:dust-and-radiation-cosm}, p.~\pageref{eg:dust-and-radiation-cosm}).
<% end_sec %> % Interpretation of the stress-energy tensor
<% begin_sec("Experimental tests") %>
But how do we know that this prediction is even correct? Can it be verified in the laboratory?
The classic laboratory test of the strength of a gravitational source is the 1797 Cavendish experiment,
in which a torsion balance was used to measure the very weak gravitational attractions between
metal spheres. We could test this aspect of general relativity by doing a Cavendish experiment with boxes full of photons,
so that the pressure is of the same order of magnitude as the mass-energy.
This is unfortunately utterly impractical, since both $P$ and $\rho$ for a well-lit box
are ridiculously small compared to $\rho$ for a metal ball.
<% marg(70) %>
<%
  fig(
    'cavendish',
    %q{A Cavendish balance, used to determine the gravitational constant.}
  )
%>
<% end_marg %>

However, the repulsive electromagnetic
pressure inside an atomic nucleus is quite large by ordinary standards --- about $10^{33}\ \zu{Pa}$!\label{nuclear-pressure}
To see how big this is compared to the nuclear mass density of $\rho\sim 10^{18}\ \kgunit/\munit^3$, we need to take
into account the factor of $c^2\ne1$ in SI units, the result being that $P/\rho$ is about $10^{-2}$, which is
not too small. Thus if we measure gravitational interactions of nuclei with different values of $P/\rho$,
we should be able to test this prediction of general relativity. This was done in
a Princeton PhD-thesis experiment by Kreuzer\label{kreuzer}\footnote{Kreuzer, Phys. Rev. 169 (1968) 1007}\index{Kreuzer experiment}
in 1966.

Before we can properly describe and interpret the Kreuzer experiment, we need to distinguish the several
different types of mass that could in principle be different from one another in a theory of gravity.
We've already encountered the distinction between inertial and gravitational mass,\index{mass!inertial}\index{inertial mass}\index{mass!gravitational}\index{gravitational mass}
which  E\"{o}tv\"{o}s experiments (p.~\pageref{sec:eotvos}) show to be equivalent to about one part in $10^{12}$.
But there is also a distinction between an object's \emph{active} gravitational
mass $m_a$, which measures its ability to create gravitational fields, and its \emph{passive} gravitational mass $m_p$,
which measures the force it feels when placed in an externally generated 
field.\index{mass!active gravitational}\index{gravitational mass!active}\index{mass!passive gravitational}\index{gravitational mass!passive}
For experiments using laboratory-scale material objects at nonrelativistic velocities, the Newtonian limit
applies, and we can think of active gravitational mass as a scalar, with a density $T^{tt}=\rho$.

To understand how this relates to pressure as a source of gravitational fields, it is helpful to
consider a case where $P$ is about the same as $\rho$, which occurs for light. Light is inherently
relativistic, so the Newtonian concept of a scalar gravitational mass breaks down, but we can still use ``mass'' in quotes to talk qualitatively
about an electromagnetic wave's active and passive participation in gravitational effects.
Experiments show that general relativity correctly predicts the deflection of light by the sun to about one part
in $10^5$ (p.~\pageref{sec:deflection-of-light}). This is the electromagnetic equivalent of an E\"{o}tv\"{o}s experiment;
it shows that general relativity predicts the right thing about the proportion between a light wave's inertial
and passive gravitational ``masses.'' Now suppose that general relativity was wrong, and pressure was not a source
of gravitational fields. This would cause a drastic decrease in the active gravitational ``mass'' of an
electromagnetic wave.

The Kreuzer experiment actually dealt with static electric fields inside nuclei, not
electromagnetic waves, but it is still clear what we should expect in general: if pressure does not act as
a gravitational source, then the ratio $m_a/m_p$ should be different for different nuclei.
Specifically, it should be lower for a nucleus with a higher atomic number $Z$, in which the electrostatic
pressures are higher.
<% marg(50) %>
<%
  fig(
    'kreuzer-simplified',
    %q{A simplified diagram of Kreuzer's modification. The moving teflon mass is submerged in a liquid with nearly the same density.}
  )
%>
<% end_marg %>

Kreuzer did a Cavendish experiment, figure \figref{kreuzer-simplified}, using masses made of two different substances. The first substance was teflon.
The second substance was
a mixture of the liquids trichloroethylene and dibromoethane, with the proportions chosen so as to give a
density as close as possible to that of teflon.
Teflon is 76\% fluorine by weight, and the liquid is 74\% bromine.
Fluorine has atomic number $Z=9$, bromine $Z=35$, and since the electromagnetic force has a long range, the
pressure within a nucleus scales upward roughly like $Z^{1/3}$ (because any given proton is acted on by $Z-1$
other protons, and the size of a nucleus scales like $Z^{1/3}$, so $P\propto Z/(Z^{1/3})^2$).
The solid mass was immersed in the liquid, and the combined gravitational field of the solid and the liquid
was detected by a Cavendish balance.

Ideally, one would formulate the liquid mixture so that its passive-mass density was exactly equal to that
of teflon, as determined by buoyancy. Any oscillation in the torque measured by the Cavendish balance would
then indicate an inequivalence between active and passive gravitational mass.
<%
  fig(
    'kreuzer',
    %q{%
      The Kreuzer experiment. 1. There are two passive masses, P, and an active mass A consisting of
      a single 23-cm diameter teflon cylinder immersed in a fluid. The teflon cylinder is driven back and forth with a period of 400 s.
      The resulting deflection of the torsion beam is monitored by an optical lever
      and canceled actively by electrostatic forces from capacitor plates (not shown).
      The voltage required for this active cancellation is a measure of the torque exerted by A on the torsion beam.
      2. Active mass as a function of temperature. 3. Passive mass as a function of temperature.
      In both 2 and 3, temperature is measured in units of ohms, i.e., the uncalibrated units of a thermistor that was immersed in the liquid.
    },
    {
      'width'=>'fullpage',
      'sidecaption'=>true,
      'sidepos'=>'b'
    }
  )
%>

In reality, the two substances involved had different coefficients
of thermal expansion, so slight variations in temperature made their passive-mass densities unequal.
Kreuzer therefore measured both the buoyant force and the gravitational torque as functions of temperature.
He determined that these became zero at the same temperature, to within experimental errors, which verified
the equivalence of active and passive gravitational mass to within a certain precision,
\begin{equation*}
  m_p \propto m_a
\end{equation*}
to within $5\times 10^{-5}$.

Kreuzer intended this exeriment only as a test of $m_p\propto m_a$, but it was reinterpreted in 1976
by Will\footnote{Will, ``Active mass in relativistic gravity: Theoretical interpretation of the Kreuzer experiment,'' Ap. J. 204 (1976) 234,
available online at \url{adsabs.harvard.edu}. A broader review of experimental tests of general relativity is given in
Will, ``The Confrontation between General Relativity
and Experiment,'' \url{relativity.livingreviews.org/Articles/lrr-2006-3/}. The Kreuzer experiment is discussed in  section 3.7.3.}
as a test of the coupling of sources to gravitational fields as predicted by general relativity and other theories of gravity.
Crudely, we've already argued that $m_p\propto m_a$ would be substance-dependent if pressure did not couple to gravitational
fields. Will actually carried out a more careful calculation, of which I present a simplified summary.
Suppose that pressure does not contribute as much to gravitational fields as is claimed by general relativity; its
coupling is reduced by a factor $1-x$, where $x=0$ in general relativity.\footnote{In Will's notation, $\zeta_4$ measures
nonstandard coupling to pressure, $\zeta_3$ to internal energy, and $\zeta_1$ to kinetic energy. By requiring that point-particle
models agree with perfect-fluid models, one obtains $(-2/3)\zeta_1=\zeta_3=-\zeta_4=x$.} Will considers a model consisting of
pointlike particles interacting through static electrical forces, and shows that for such a system,
\begin{equation*}
  m_a = m_p + \frac{1}{2}x U_e\eqquad,
\end{equation*}
where $U_e$ is the electrical energy. The Kreuzer experiment then requires $|x| < 6\times 10^{-2}$, meaning that
pressure does contribute to gravitational fields as predicted by general relativity, to within a precision of 6\%.

One of the important ways in which Will's calculation goes beyond my previous crude argument is that it shows that
when $x=0$, as it does for general relativity, the correction term $x U_e/2$ vanishes, and $m_a=m_p$ exactly. This is
interpreted as follows. Let a bromine nucleus be referred to with a capital $M$, fluorine with the lowercase $m$.
Then when a bromine nucleus and a fluorine nucleus interact gravitationally at a distance $r$, the Newtonian
approximation applies, and the total internal force acting on the pair of nuclei taken as a whole equals
$(m_pM_a-M_pm_a)/r^2$ (in units where the Newtonian gravitational constant $G$ equals 1). This vanishes
only if $m_pM_a-M_pm_a=0$, which is equivalent to $m_p/M_p=m_a/M_a$. If this proportionality fails, then
the system violates Newton's third law and conservation of momentum; its center of mass will accelerate
along the line connecting the two nuclei, either in the direction of $M$ or in the direction of $m$, depending
on the sign of $x$. 

Thus the vanishing of the correction term $x U_e/2$ tells us that general relativity
predicts exact conservation of momentum in this interaction. This is comforting, but a little susprising on the face
of it. Newtonian gravity treats active and passive massive perfectly symmetrically, so that there is a perfect
guarantee of conservation of momentum. But relativity
incorporates them in a completely asymmetric manner, so there is no obvious reason that we should have perfect
conservation of momentum. In fact we don't have any general guarantee of conservation of momentum, since,
as discussed in section \ref{sec:no-conservation-laws} on page \pageref{sec:no-conservation-laws}, the language
of general relativity doesn't even give us the symbols we would need in order to state a global conservation law
for a vector. General relativity does, however, allow \emph{local} conservation laws. We will have local
conservation of mass-energy and momentum provided that the stress-energy tensor's divergence $\nabla_b T^{ab}$ vanishes.

Bartlett and van Buren\footnote{Phys. Rev. Lett. 57 (1986) 21. The result is summarized in section 3.7.3 of
the review by Will.} used this connection to conservation of momentum in 1986 to derive a tighter limit on $x$. Since the moon
has an asymmetrical distribution of iron and aluminum, a nonzero $x$ would cause it to have an anomalous acceleration along
a certain line. Because lunar laser ranging gives extremely accurate data on the moon's orbit, the constraint is tightened
to $|x| < 1\times 10^{-8}$.\label{end-kreuzer}
<% marg(40) %>
<%
  fig(
    'mirror-on-moon',
    %q{The Apollo 11 mission left behind this mirror, which in this photo shows the reflection of the black sky. The mirror
       is used for lunar laser ranging measurements, which have an accuracy of about a centimeter.}
  )
%>
<% end_marg %>

These are tests of general relativity's predictions about the gravitational fields generated by the pressure of a static
electric field. In addition, there is indirect confirmation (p.~\pageref{light-gravity-from-bbn}) that general relativity is correct when
it comes to electromagnetic waves.
<% end_sec %> % Experimental tests
<% begin_sec("Energy of gravitational fields not included in the stress-energy tensor") %>\index{energy!of gravitational fields}
Summarizing the story of the Kreuzer and Bartlett-van Buren results, we find that observations verify to high precision
one of the defining properties of general relativity, which is that \emph{all} forms of energy are equivalent to mass. That is,
Einstein's famous $E=mc^2$ can be extended to gravitational effects, with the proviso that the source of gravitational fields
is not really a scalar $m$ but the stress-energy tensor $T$.

But there is an exception to this even-handed treatment of all types of energy, which is that the energy of the gravitational
field itself is not included in $T$, and is not even generally a well-defined concept locally. 
In Newtonian gravity, we can have
conservation of energy if we attribute to the gravitational field a negative potential energy density $-\vc{g}^2/8\pi$.
But the equivalence principle tells us that $\vc{g}$ is not a tensor, for we can always make $\vc{g}$ vanish locally
by going into the frame of a free-falling observer, and yet the tensor transformation laws will never change a nonzero
tensor to a zero tensor under a change of coordinates. Since the gravitational field is not a tensor, there is no way
to add a term for it into the definition of the stress-energy, which is a tensor. The grammar and vocabulary of the tensor notation
are specifically designed to prevent writing down such a thing, so that the language of general relativity is not even capable of expressing
the idea that gravitational fields would themselves contribute to $T$.

Self-check: (1) Convince yourself that the negative sign in the expression
$-\vc{g}^2/8\pi$ makes sense, by considering the case where two equal masses start out
far apart and then fall together and combine to make a single body with twice the mass.
(2) The Newtonian gravitational field is the gradient of the gravitational potential $\phi$,
which corresponds in the Newtonian limit to the time-time component of the metric.
With this motivation, suppose someone proposes generalizing the Newtonian energy density
$-(\vc{\nabla}\phi)^2/8\pi$ to a similar expression such as $-(\nabla_a g\indices{^a_b})(\nabla^c g\indices{_c^b})$,
where $\nabla$ is now the covariant derivative, and $g$ is the metric, not the Newtonian field strength.
What goes wrong?

As a concrete example, we observe that the Hulse-Taylor binary pulsar system (p.~\pageref{hulse-taylor-pulsar})
is gradually losing orbital energy, and that the rate of energy loss exactly matches general relativity's prediction
of the rate of gravitational radiation. There is a net decrease in the forms of energy, such as rest mass and kinetic energy,
that are accounted for in the stress energy tensor $T$. We can account for the missing energy by attributing it to the
outgoing gravitational waves, but that energy is not included in $T$, and we have to develop special techniques for
evaluating that energy. Those techniques only turn out to apply to certain special types of spacetimes, such as
asymptotically flat ones, and they do not allow a uniquely defined energy density to be attributed to a particular
small region of space (for if they did, that would violate the equivalence principle).

\begin{eg}{Gravitational energy is locally unmeasurable.}
When a new form of energy is discovered, the way we establish that it is a form of energy
is that it can be transformed to or from other forms of energy. For example, Becquerel discovered radioactivity
by noticing that photographic plates left in a desk drawer along with radium salts became clouded: some new form
of energy had been converted into previously known forms such as chemical energy. It is only in this limited sense that
energy is ever locally observable, and this limitation prevents us from meaningfully defining certain measures of energy.
For example we can never measure the local electrical potential in the same sense that we can  measure the local barometric
pressure; a potential of 137 volts only has meaning relative to some other region of space taken to be at
ground. Let's use the acronym MELT to refer to measurement of energy by the local transformation of that energy
from one form into another.

The reason MELT works is that energy (or actually the momentum four-vector)
is locally conserved, as expressed by the zero-divergence property of the
stress-energy tensor. Without conservation, there is no notion of transformation.
The Einstein field equations imply this zero-divergence property, and the field equations have been
well verified by a variety of observations, including many observations (such as solar system tests
and observation of the Hulse-Taylor system) that in Newtonian terms would be described as involving
(non-local) transformations between kinetic energy and the energy of the gravitational field.
This agreement with observation is achieved by taking $T=0$ in vacuum, regardless of the gravitational
field. Therefore any local transformation of gravitational field energy into another form of energy
would be inconsistent with previous observation. This implies that MELT is impossible for gravitational field energy.

In particular, suppose that observer A carries out a local MELT of gravitational field energy, and that A
sees this as a process in which the gravitational field is reduced in intensity, causing the release of some
other form of energy such as heat. Now consider the situation as seen by observer B, who is free-falling in the
same local region. B says that there was never any gravitational field in the first place, and therefore sees
heat as violating local conservation of energy. In B's frame, this is a nonzero divergence of
the stress-energy tensor, which falsifies the Einstein field equations.
\end{eg}

<% end_sec %> % Energy of gravitational fields not included in the stress-energy tensor

<% begin_sec("Some examples") %>

We conclude this introduction to the stress-energy tensor with some illustrative examples.

\begin{eg}{A perfect fluid}\label{eg:perfect-fluid-tensorial}
For a perfect fluid, we have
\begin{equation*}
  T_{ab}=(\rho+P)v_a v_b -s Pg_{ab}\eqquad,
\end{equation*}
where $s=1$ for our $+---$ signature or $-1$ for the signature $-+++$, and
$v$ represents the coordinate velocity of the fluid's rest frame.

Suppose that the metric is diagonal, but its components are varying, 
$g_{\alpha\beta}=\operatorname{diag}(A^2,-B^2,\ldots)$. The properly normalized
velocity vector of an observer at (coordinate-)rest is
$v^\alpha=(A^{-1},0,0,0)$. Lowering the index gives $v_\alpha=(sA,0,0,0)$.
The various forms of the stress-energy tensor then look like the following:
\begin{align*}
  T_{00} = A^2\rho   \quad & \quad T_{11} = B^2 P \\
  T\indices{^0_0} = s\rho   \quad & \quad T\indices{^1_1} = - sP \\
  T^{00} = A^{-2}\rho   \quad & \quad T^{11} = B^{-2} P\eqquad.
\end{align*}
\end{eg}

\begin{eg}{A rope dangling in a Schwarzschild spacetime}\label{eg:rope-in-sch}
Suppose we want to lower a bucket on a rope toward the event horizon of a black hole.
We have already made some qualitative remarks about this idea in example \ref{eg:newtonian-black-hole}
on p.~\pageref{eg:newtonian-black-hole}. This seemingly whimsical example turns out to be a good
demonstration of some techniques, and can also be used in thought experiments that illustrate the
definition of mass in general relativity and that probe some ideas about
quantum gravity.\footnote{Brown, ``Tensile Strength and the Mining of Black Holes,'' \url{arxiv.org/abs/1207.3342}}

The Schwarzschild metric (p.~\pageref{exact-schwarzschild-formula}) is
\begin{equation*}
  ds^2 = f^2 \der t^2 - f^{-2}\der r^2 + \ldots\eqquad,
\end{equation*}
where $f=(1-2m/r)^{1/2}$, and $\ldots$ represents angular terms. We will end up needing the following
Christoffel symbols:
\begin{align*}
  \Gamma\indices{^t_t_r} &= f'/f \\
  \Gamma\indices{^\theta_\theta_r} &= \Gamma\indices{^\phi_\phi_r} = r^{-1}
\end{align*}

Since the spacetime has spherical symmetry, it ends up being more convenient to consider a rope whose shape,
rather than being cylindrical, is a cone defined by some set of $(\theta,\phi)$. For convenience we take
this set to cover unit solid angle. The final results obtained in this way can be readily converted into
statements about a cylindrical rope. We let $\mu$ be the mass per unit length of the rope, and $T$ the
tension. Both of these may depend on $r$. The corresponding energy density and tensile stress
are $\rho=\mu/A=\mu/r^2$ and $S=T/A$. To connect this to the stress-energy tensor, we start by comparing
to the case of a perfect fluid from example \ref{eg:perfect-fluid-tensorial}.
Because the rope is made of fibers that have stength only in the radial direction, we will have
$T^{\theta\theta}=T^{\phi\phi}=0$. Furthermore, the stress is tensile rather than compressional,
corresponding to a negative pressure. The Schwarzschild coordinates are orthogonal but not orthonormal,
so the properly normalized velocity of a static observer has a factor of $f$ in it: 
$v^\alpha=(f^{-1},0,0,0)$, or, lowering an index, $v_\alpha=(f,0,0,0)$.
The results of example \ref{eg:perfect-fluid-tensorial} show that the mixed-index form of $T$ will
be the most convenient, since it can be expressed without messy factors of $f$.
We have
\begin{equation*}
  T\indices{^\kappa_\nu} = \operatorname{diag}(\rho,S,0,0) = r^{-2}\operatorname{diag}(\mu,T,0,0)\eqquad.
\end{equation*}
By writing the stress-energy tensor in this form, which is independent of $t$, we have assumed static
equilibrium outside the event horizon. \emph{Inside} the horizon, the $r$ coordinate is the timelike one,
the spacetime itself is not static, and we do not expect to find static solutions, for the reasons
given on p.~\pageref{eg:newtonian-black-hole}.

Conservation of energy is automatically satisfied, since there is no time dependence. Conservation of
radial momentum is expressed by
\begin{equation*}
  \nabla_\kappa T\indices{^\kappa_r} = 0\eqquad,
\end{equation*}
or
\begin{equation*}
  0 = \nabla_r T\indices{^r_r} + \nabla_t T\indices{^t_r} 
          + \nabla_\theta T\indices{^\theta_r} + \nabla_\phi T\indices{^\phi_r}\eqquad.
\end{equation*}
It would be tempting to throw away all but the first term, since $T$ is diagonal, and therefore
$T\indices{^t_r}=T\indices{^\theta_r}=T\indices{^\phi_r}=0$.
However, a covariant derivative can be nonzero even when
the symbol being differentiated vanishes identically. Writing out these four terms, we have
\begin{align*}
  0 = &\partial_r T\indices{^r_r} + \Gamma\indices{^r_r_r}T\indices{^r_r}- \Gamma\indices{^r_r_r}T\indices{^r_r} \\
      +& \Gamma\indices{^t_t_r}T\indices{^r_r}       - \Gamma\indices{^t_t_r}T\indices{^t_t} \\
      +& \Gamma\indices{^\theta_\theta_r}T\indices{^r_r} \\
      +& \Gamma\indices{^\phi_\phi_r}T\indices{^r_r}\eqquad,
\end{align*}
where each line corresponds to one covariant derivative. Evaluating this, we have
\begin{align*}
  0 = T' + \frac{f'}{f}T - \frac{f'}{f}\mu\eqquad,
\end{align*}
where primes denote differentiation with respect to $r$. Note that since no terms of the
form $\partial_rT\indices{^t_t}$ occur, this expression is valid regardless of whether we take $\mu$ to
be constant or varying. Thus we are free to take $\rho\propto r^{-2}$, so that $\mu$ is constant,
and this means that our result is equally applicable to a uniform cylindrical rope. This result is
checked using computer software in example \ref{eg:rope-in-sch-cas}.

This is a differential equation that tells us how the tensile stress in the rope varies along its
length. The coefficient $f'/f=m/r(r-2m)$ blows up at the event horizon, which is as expected,
since we do not expect to be able to lower the rope to or below the horizon.

Let's check the Newtonian limit, where the gravitational field is $g$ and the potential is $\Phi$.
In this limit, we have
$f\approx 1-\Phi$, $f'/f\approx g$ (with $g>0$), and $\mu\gg T$, resulting in
\begin{equation*}
  0 = T' - g\mu\eqquad.
\end{equation*}
which is the expected Newtonian relation.

Returning to the full general-relativistic result, it can be shown that
for a loaded rope with no mass of its own, we have a finite result for $\lim_{r\rightarrow\infty} S$,
even when the bucket is brought arbitrarily close to the horizon. However, this is
misleading without the caveat that for $\mu<T$, the speed of transverse waves in the rope
is greater than $c$, which is not possible for any known form of matter --- it would violate the
null energy condition, discussed in the following section.
\end{eg}

\begin{eg}{The rope, using computer algebra}\label{eg:rope-in-sch-cas}
The result of example \ref{eg:rope-in-sch} can be checked with the following
Maxima code:

\begin{listing}{1}
load(ctensor);
ct_coords:[t,r,theta,phi];
depends(f,r);
depends(ten,r); /* tension depends on r */
depends(mu,r); /* mass/length depends on r */
lg:matrix([f^2,0,0,0],
          [0,-f^-2,0,0],
          [0,0,-r^2,0],
          [0,0,0,-r^2*sin(theta)^2]);
cmetric();
christof(mcs);
/* stress-energy tensor, T^mu_nu */
t:r^-2*matrix(
  [mu,0,0,0],
  [0,ten,0,0],
  [0,0,0,0],
  [0,0,0,0]
);
/*
  Compute covariant derivative of the stress-energy
  tensor with respect to its first index. The 
  function checkdiv is defined so that the first 
  index has to be covariant (lower); the T I'm 
  putting in is T^mu_nu, and since it's symmetric,
  that's the same as T_mu^nu.
*/
checkdiv(t);
\end{listing}
\end{eg}


<% end_sec %> % Some examples



<% end_sec %> % The Einstein field equation

<% begin_sec("Energy conditions") %>\index{energy conditions}
Physical theories are supposed to answer questions. For example:\label{energy-condition-questions}
\begin{enumerate}
\item Does a small enough physical object always have a world-line that is approximately a geodesic?
\item Do massive stars collapse to form black-hole singularities?
\item Did our universe originate in a Big Bang singularity?
\item If our universe doesn't currently have violations of causality
such as the closed timelike curves exhibited by the Petrov metric (p.~\pageref{eq:petrov-metric}),
can we be assured that it will never develop causality violation in the future?
\end{enumerate}
We would like to ``prove'' whether the answers to questions like these are yes or no, but
physical theories are not formal mathematical systems in which results can be ``proved'' absolutely.
For example, the basic structure of general relativity isn't a set of axioms but a
list of ingredients like the
equivalence principle, which has evaded formal definition.\footnote{
``Theory of gravitation theories: a no-progress report,''
Sotiriou,  Faraoni, and Liberati, \url{http://arxiv.org/abs/0707.2748}}\index{equivalence principle!not mathematically well defined}

Even the Einstein field equations, which appear to be completely well defined, are not
mathematically formal predictions of the behavior of a physical system.
The field equations are agnostic on the question of what kinds of matter fields contribute to the stress-energy tensor.
In fact, any spacetime at all is a solution to the Einstein field equations, provided we're willing to admit the
corresponding stress-energy tensor. We can never answer questions like the ones above without assuming something about the stress-energy tensor.

In example \ref{eg:dust-and-radiation-cosm} on page \pageref{eg:dust-and-radiation-cosm}, we saw that
radiation has $P=\rho/3$ and dust has $P=0$. Both have $\rho\ge0$. If the universe is made out of nothing but dust and radiation, then
we can obtain the following four constraints on the energy-momentum 
tensor:\index{trace energy condition}\index{strong energy condition}\index{dominant energy condition}\index{weak energy condition}\index{null energy condition}

\begin{tabular}{ll}
trace energy condition   &  $\rho-3P \ge 0$ \\
strong energy condition  &  $\rho+3P \ge 0$ and $\rho+P \ge 0$ \\
dominant energy condition & $\rho \ge 0$ and $|P| \le \rho$ \\
weak energy condition    &  $\rho \ge 0$ and $\rho+P \ge 0$ \\
null energy condition    &  $\rho+P \ge 0$ \\
\end{tabular}\label{table-of-energy-conditions}

\noindent These are arranged roughly in order from strongest to weakest.
They all have to do with the idea that negative mass-energy doesn't seem to exist in our universe, i.e., that
gravity is always attractive rather than repulsive. With this motivation, it would seem that there should only
be one way to state an energy condition: $\rho > 0$. But the symbols $\rho$ and $P$ refer to the form of the
stress-energy tensor in a special frame of reference, interpreted as the one that is at rest relative to the
average motion of the ambient matter. (Such a frame is not even guaranteed to exist unless the matter acts
as a perfect fluid.) In this frame, the tensor is diagonal. Switching to some other frame
of reference, the $\rho$ and $P$ parts of the tensor would mix, and it might be possible to end up with
a negative energy density. The weak energy condition is the constraint we need in order to make sure that
the energy density is never negative in any frame.

The dominant energy condition is like the weak energy condition, but it also guarantees that no observer
will see a flux of energy flowing at speeds greater than $c$.

The strong energy condition essentially states that gravity is never repulsive; it is violated
by the cosmological constant (see p.~\pageref{lambda-and-energy-conditions}).

\begin{eg}{An electromagnetic wave}\label{eg:t-of-em-wave}\index{stress-energy tensor!of an electromagnetic wave}
In example \ref{eg:dust-stress-energy} on p.~\pageref{eg:dust-stress-energy}, we saw that dust boosted
along the $x$ axis gave a stress-energy tensor
\begin{equation*}
T\indices{_{\mu}_{\nu}} = \gamma^2\rho \left( \begin{matrix}
                    1 &  v  \\
                    v  &  v^2 
      \end{matrix} \right)\eqquad,
\end{equation*}
where we now suppress the $y$ and $z$ parts, which vanish. For $v\rightarrow 1$, this becomes
\begin{equation*}
T\indices{_{\mu}_{\nu}} = \rho' \left( \begin{matrix}
                   1 & 1 \\
                   1 & 1
      \end{matrix} \right)\eqquad,
\end{equation*}
where $\rho'$ is the energy density as measured in the new frame.
As a source of gravitational fields, this ultrarelativistic dust is indistinguishable from any other form
of matter with $v=1$ along the $x$ axis, so this is also the stress-energy tensor of an electromagnetic
wave with local energy-density $\rho'$, propagating along the $x$ axis.
(For the full expression for the stress-energy tensor of an
arbitrary electromagnetic field, see the Wikipedia article ``Electromagnetic stress-energy tensor.'')

This is a stress-energy tensor that represents a flux of energy at a speed equal to $c$, so we expect it to
lie at exactly the limit imposed by the dominant energy condition (DEC). Our statement of the DEC, however,
was made for a diagonal stress-energy tensor, which is what is seen by an observer at rest relative to the
matter. But we know that it's impossible to have an observer who, as the teenage Einstein imagined, rides
alongside an electromagnetic wave on a motorcycle. One way to handle this is to generalize our definition
of the energy condition. For the DEC, it turns out that this can be done by requiring that the matrix $T$, when multiplied
by a vector on or inside the future light-cone, gives another vector on or inside the cone.

A less elegant but more concrete workaround is as follows. Returning to the original expression for the
$T$ of boosted dust at velocity $v$, we let $v=1+\epsilon$, where $|\epsilon|\ll 1$. This gives
a stress-energy tensor that (ignoring multiplicative constants) looks like:
\begin{equation*}
\left( \begin{matrix}
                    1 &  1+\epsilon  \\
                    1+\epsilon  &  1+2\epsilon
      \end{matrix} \right)\eqquad.
\end{equation*}

If $\epsilon$ is negative, we have ultrarelativistic dust, and we can verify that it satisfies the DEC
by un-boosting back to the rest frame. To do this explicitly, we can find the matrix's
eigenvectors, which (ignoring terms of order $\epsilon^2$) 
are $(1,1+\epsilon)$ and $(1,1-\epsilon)$, with eigenvalues $2+2\epsilon$ and $0$, respectively.
For $\epsilon<0$, the first of these is timelike, the second
spacelike. We interpret them simply as the $t$ and $x$ basis vectors of the rest frame in which we originally
described the dust. Using them as a basis, the stress-energy tensor takes on the form $\operatorname{diag}(2+2\epsilon,0)$.
Except for a constant factor that we didn't bother to keep track of, this is the original form of the $T$
in the dust's rest frame, and it clearly satisfies the DEC, since $P=0$.

For $\epsilon>0$, $v=1+\epsilon$ is a velocity greater than the speed of light, and there is no way
to construct a boost corresponding to $-v$. We can nevertheless
find a frame of reference in which the stress-energy tensor is diagonal, allowing us to check the DEC.
The expressions found above for the eigenvectors and eigenvalues are still valid, but now the
timelike and spacelike characters of the two basis vectors have been interchanged.
The stress-energy tensor has the form $\operatorname{diag}(0,2+2\epsilon)$, with $\rho=0$ and $P>0$, which
violates the DEC. As in this example, any flux of mass-energy at speeds greater than $c$ will violate
the DEC.

The DEC is obeyed for $\epsilon<0$ and violated for $\epsilon>0$, and since $\epsilon=0$ gives a stress-energy
tensor equal to that of an electromagnetic wave, we can tell
that light is exactly on the border between forms of matter that fulfill the DEC and those
that don't. Since the DEC is formulated as a non-strict inequality, it follows that light obeys the DEC.
\end{eg}

\begin{eg}{No ``speed of flux''}\label{eg:t-of-static-em-fields}
The foregoing discussion may have encouraged the reader to believe that it is possible in general
to read off a ``speed of energy flux'' from the value of $T$ at a point. This is not true.

The difficulty lies in the distinction between flow with and without accumulation, which is sometimes
valid and sometimes not. In springtime in the Sierra Nevada, snowmelt adds water to alpine lakes
more rapidly than it can flow out, and the water level rises. This is flow with accumulation. In the
fall, the reverse happens, and we have flow with depletion (negative accumulation).

Figure \subfigref{static-poynting-vector}{1} shows a second example in which the distinction seems valid.
Charge is flowing through the lightbulb, but because there is no accumulation of charge in the DC circuit,
we can't detect the flow by an electrostatic measurement; the wire does not attract the tiny bits
of paper below it on the table.

But we know that with different measurements, we could detect the flow of charge in \subfigref{static-poynting-vector}{1}.
For example, the magnetic field from the wire would deflect a nearby magnetic compass. This shows that the distinction
between flow with and without accumulation may be sometimes valid and sometimes invalid. Flow without accumulation
may or may not be detectable; it depends on the physical context.
<% marg(50) %>
<%
  fig(
    'static-poynting-vector',
    %q{Example \ref{eg:t-of-static-em-fields}.}
  )
%>
<% end_marg %>

In figure \subfigref{static-poynting-vector}{2}, an electric charge and a magnetic dipole are superimposed at a point.
The Poynting vector\index{Poynting vector} $\vc{P}$ defined as $\vc{E}\times\vc{B}$ is used in electromagnetism as a measure of the
flux of energy, and it tells the truth, for example, when the sun warms your sun on a 
hot day. In \subfigref{static-poynting-vector}{2},
however, all the fields are static. It seems as though there can be no flux of energy. But that doesn't
mean that the Poynting vector is lying to us. It tells us that there is a pattern of flow, but it's flow
without accumulation; the Poynting vector forms circular loops that close upon themselves, and electromagnetic
energy is transported in and out of any volume at the same rate. We would perhaps prefer to have a mathematical
rule that gave zero for the flux in this situation, but it's acceptable that our rule 
$\vc{P}=\vc{E}\times\vc{B}$ gives a nonzero result, since it doesn't incorrectly predict an accumulation, which
is what would be detectable.

Now suppose we're presented with this stress-energy tensor, measured at a single point and expressed in some units:
\begin{equation*}
T\indices{^{\mu}^{\nu}} = \left( \begin{matrix}
                   4.037\pm0.002 & 4.038\pm0.002 \\
                   4.036\pm0.002 & 4.036\pm0.002
      \end{matrix} \right)\eqquad.
\end{equation*}
To within the experimental error bars, it has the right form to be many different things: 
(1) We could have a universe filled with perfectly uniform dust, moving along the $x$ axis at some ultrarelativistic
speed $v$ so great that the $\epsilon$ in $v=1-\epsilon$, as in example
\ref{eg:t-of-em-wave}, is not detectably different from zero. (2) This could be a point sampled
from an electromagnetic wave traveling
along the $x$ axis. (3) It could be a point taken from figure \subfigref{static-poynting-vector}{2}.
(In cases 2 and 3, the off-diagonal elements are simply the Poynting vector.)

In cases 1 and 2, we would be inclined to interpret this stress-energy tensor by saying that its
off-diagonal part measures the flux of mass-energy along the $x$ axis, while in case 3 we would reject
such an interpretation. The trouble here is not so much in our interpretation of $T$ as in
our Newtonian expectations about what is or isn't observable about fluxes that flow without accumulation.
In Newtonian mechanics, a flow of mass is observable, regardless of whether there is accumulation, because
it carries momentum with it; a flow of energy, however, is undetectable if there is no accumulation.
The trouble here is that relativistically, we can't maintain this distinction between mass and energy.
The Einstein field equations tell us that a flow of either will contribute equally to the stress-energy,
and therefore to the surrounding gravitational field.

The flow of energy in \subfigref{static-poynting-vector}{2} contributes to the gravitational field, and its
contribution is changed, for example, if the magnetic field is reversed. The figure is in fact not
a bad qualitative representation of the spacetime around a rotating, charged black hole. At large
distances, however, the gravitational effect of the off-diagonal terms in $T$ becomes small, because they average
to nearly zero over a sufficiently large spherical region. The distant gravitational field approaches that
of a point mass with the same mass-energy.
\end{eg}

\begin{eg}{Momentum in static fields}\label{eg:p-in-static-fields}\index{center of mass-energy}
Continuing the train of thought described in example \ref{eg:t-of-static-em-fields}, we can come up with situations that
seem even more paradoxical. In figure \subfigref{static-poynting-vector}{2}, the total momentum of the fields
vanishes by symmetry. This symmetry can, however, be broken by displacing the electric charge by $\Delta\vc{R}$
perpendicular to the magnetic dipole vector $\vc{D}$. The total momentum no longer vanishes, and now lies
in the direction of $\vc{D}\times\Delta\vc{R}$. But we have proved in example \ref{eg:center-of-me}
on p.~\pageref{eg:center-of-me} that a system's center of mass-energy is at rest if and only if its total momentum
is zero. Since this system's center of mass-energy is certainly at rest, where is the other momentum that
cancels that of the electric and magnetic fields?

Suppose, for example, that the magnetic dipole consists of a loop of copper
wire with a current running around it. If we open a switch and
extinguish the dipole, it appears that the system must recoil! This seems impossible, since the fields
are static, and an electric charge does not interact with a magnetic dipole.

Babson et al.\footnote{Am. J. Phys. 77 (2009) 826} have analyzed a number of examples of this type.
In the present one, the mysterious ``other momentum'' can be attributed to a relativistic imbalance
between the momenta of the electrons in the different parts of the wire. A subtle point about these
examples is that even in the case of an idealized dipole of vanishingly small size, it makes a difference
what structure we assume for the dipole. In particular, the field's momentum is nonzero for a dipole
made from a current loop of infinitesimal size, but zero for a dipole made out of 
two magnetic monopoles.\footnote{Milton and Meille, \url{arxiv.org/abs/1208.4826}}
\end{eg}

<% begin_sec("Geodesic motion of test particles") %>\label{geodesic-motion-details}
Question 1 on p.~\pageref{energy-condition-questions} was: ``Does a small enough physical object always have a world-line that is approximately a geodesic?''
In other words, do E\"{o}tv\"{o}s experiments give null results when carried out in laboratories using real-world apparatus of small enough size?
We would like something of this type to be true, since general relativity is based on the equivalence principle, and the equivalence principle is
motivated by the null results of E\"{o}tv\"{o}s experiments.
Nevertheless, it is fairly easy to show that the answer to the
question is no, unless we make some more specific assumption, such as an energy condition, about the system being modeled.

Before we worry about energy conditions, let's consider why the small size of the apparatus is relevant. Essentially this is
because of gravitational radiation. In a gravitationally radiating system such as the Hulse-Taylor binary pulsar (p.~\pageref{hulse-taylor-pulsar}),
the material bodies lose energy, and as with any radiation process, the radiated power depends on the square of the strength
of the source. The world-line of a such a body therefore depends on its mass, and this shows that its world-line
cannot be an exact geodesic, since the initially tangent world-lines of two different masses diverge from one another,
and these two world-lines can't both be geodesics.

Let's proceed to give a rough argument for geodesic motion and then try to poke holes in it.
When we test geodesic motion, we do an E\"{o}tv\"{o}s experiment that is restricted to a certain small region of spacetime S.
Our test-body's world-line enters S with a certain energy-momentum vector $p$ and exits with $p'$.
If spacetime was flat, then Gauss's theorem would hold exactly, and the vanishing divergence  $\nabla_b T^{ab}$ of
the stress-energy tensor would require that the incoming flux represented by $p$ be exactly canceled by the
outgoing flux due to $p'$. In reality spacetime isn't flat, and it isn't even possible to compare $p$ and $p'$ except
by parallel-transporting one into the same
location as the other. Parallel transport is path-dependent, but if we make the reasonable restriction to
paths that stay within S, we expect the ambiguity due to path-dependence to be proportional to the area
enclosed by any two paths, so that if S is small enough, the ambiguity can be made small.
Ignoring this small ambiguity, we can see that one way for the fluxes to cancel would be for the
particle to travel along a geodesic, since both  $p$ and $p'$ are tangent to the test-body's world-line, and a geodesic is
a curve that parallel-transports its own tangent vector. Geodesic motion is therefore one solution, and we
expect the solution to be nearly unique when S is small.

Although this argument is almost right, it has some problems.
First we have to ask whether ``geodesic'' means a geodesic of the full spacetime including the object's own fields, or
of the background spacetime B that would have existed without the object. The latter is the more sensible interpretation,
since the question is basically asking whether a spacetime can really be defined geometrically, as the equivalence principle claims,
based on the motion of test particles inserted into it. We also have to define words like ``small enough'' and ``approximately;''
to do this, we imagine a sequence of objects $\zu{O}_n$ that get smaller and smaller as $n$ increases.
We then form the following conjecture, which is meant to formulate
question 1 more exactly: Given a vacuum background spacetime B, and a timelike world-line $\ell$ in B,
consider a sequence of spacetimes $\zu{S}_n$, formed by inserting the $\zu{O}_n$ into B, such that:
(i) the metric of $\zu{S}_n$ is defined on the same points as the metric of B;
(ii) $\zu{O}_n$ moves along $\ell$, and for any $r>0$, there exists some $n$ such that for $m\ge n$, $\zu{O}_m$ is smaller than $r$;\footnote{i.e., at any point P on $\ell$,
an observer moving along $\ell$ at P defines a surface of simultaneity K passing through P, and
sees the stress-energy tensor of  $\zu{O}_n$ as vanishing outside of a three-sphere of radius $r$ within K  and centered on P}
(iii) the metric of $\zu{S}_n$ approaches the metric of B as $n\rightarrow\infty$. 
Then $\ell$ is a geodesic of B.

This is almost right but not quite, as shown by the following counterexample.
Papapetrou\footnote{Proc.~Royal Soc.~London A 209 (1951) 248. The relevant 
result is summarized in Misner, Thorne, and Wheeler, \emph{Gravitation}, p.~1121.}
has shown that a spinning body in a curved background spacetime deviates from a geodesic with an acceleration that
is proportional to $LR$, where $L$ is its angular momentum and $R$ is the Riemann curvature.
Let all the $\zu{O}_n$ have a fixed value of $L$, but let the spinning mass be concentrated into
a smaller and smaller region as $n$ increases, so as to satisfy (ii). As the radius $r$ decreases,
the motion of the particles composing an $\zu{O}_n$ eventually has to become ultrarelativistic, so that the main contribution to the gravitational
field is from the particles' kinetic energy rather than their rest mass. We then have $L\sim pr\sim Er$, so that in order to keep
$L$ constant, we must have $E\propto 1/r$. This causes two problems. First, it makes the gravitational field
blow up at small distances, violating (iii). Also, we expect that for any known form of matter, there will come a point
(probably the Tolman-Oppenheimer-Volkoff limit)
at which we get a black hole; the singularity is then not part of the spacetime $\zu{S}_n$, violating (i). But our failed
counterexample can be patched up. We obtain a supply of exotic matter, whose gravitational mass is negative, and we
mix enough of this mysterious stuff into each $\zu{O}_n$ so that the gravitational field shrinks rather than growing
as $n$ increases, and no black hole is ever formed.

Ehlers and Geroch\footnote{\url{arxiv.org/abs/gr-qc/0309074v1}} have proved that it suffices to require an additional
condition: (iv) The $\zu{O}_n$ satisfy
the dominant energy condition. This rules out our counterexample.
<% end_sec %> % Geodesic motion of test particles
<% marg(40) %>
<%
  fig(
    'negative-mass',
    %q{Negative mass.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'pushme-pullyou',
    %q{The black sphere is made of ordinary matter. The crosshatched sphere
       has positive gravitational mass and negative inertial mass. If the two
       of them are placed side by side in empty space, they will both accelerate
       steadily to the right, gradually approaching the speed of light. Conservation
       of momentum is preserved, because the exotic sphere has leftward momentum
       when it moves to the right, so the total momentum is always zero.}
  )
%>
<% end_marg %>

<% begin_sec("The Newtonian limit") %>
In units with $c \ne 1$, a quantity like $\rho+P$ is expressed as $\rho+P/c^2$. The Newtonian limit is
recovered as $c\rightarrow\infty$, which makes the pressure term negligible, so that all the energy conditions
reduce to $\rho\ge 0$. What would it mean if this was violated? Would $\rho<0$ describe an object with negative
inertial mass, which would accelerate east when you pushed it to the west? Or would it describe something with
negative gravitational mass, which would repel ordinary matter? We can imagine various possiblities, as shown in
figure \figref{negative-mass}. Anything that didn't lie on the main diagonal would violate the equivalence principle,
and would therefore be impossible to accomodate within general relativity's geometrical description of gravity.
If we had ``upsidasium'' matter such as that described by the second quadrant of the figure (example \ref{eg:upsidasium}, p.~\pageref{eg:upsidasium}), gravity would be like electricity,
except that like masses would attract and opposites repel; we could have gravitational dielectrics and gravitational
Faraday cages. The fourth quadrant leads to amusing possibilities like figure \figref{pushme-pullyou}.



\begin{eg}{No gravitational shielding}\label{eg:shielding}\index{antigravity}\index{shielding!gravitational}\index{gravitational shielding}
Electric fields can be completely excluded from a Faraday cage, and magnetic fields can be very strongly blocked
with high-perme\-ability materials such as mu-metal.
It would be fun if we could do the same with gravitational fields, so that we could have zero-gravity or near-zero-gravity
parties in a specially shielded room. It would be a form of antigravity, but a different one than the ``upsidasium''
type. Unfortunately this is difficult to do, and the reason it's difficult
turns out to be related to the unavailability of materials that violate energy conditions.

First we need to define what we mean by shielding. We restrict ourselves to the Newtonian limit, and to one dimension, so that 
a gravitational field is specified by a function of one variable $g(x)$. The best kind of shielding would be some substance
that we could cut with shears and form into a box, and that would exclude gravitational fields from the
interior of the box. This would be analogous to a Faraday cage; no matter what
external field it was embedded in, it would spontaneously adjust itself so that the internal field
was canceled out. A less desirable
kind of shielding would be one that we could set up on an \emph{ad hoc} basis to null out a specific,
given, externally imposed field. Once we know what the external field is, we try to choose some
arrangement of masses such that the field is nulled out.
We will show that even this kind of shielding is unachievable,
if nulling out the field is interpreted to mean this: at some point, which for convenience we take to
be the origin, we wish to have a gravitational field such that $g(0)=0$, $\der g/\der x(0)=0$, \ldots
$\der^n g/\der x^n(0)=0$, where $n$ is arbitrarily specified. For comparison, \emph{magnetic}
fields \emph{can} be nulled out according to this definition by building an appropriately chosen configuration of coils
such as a Helmholtz coil.
<% marg(120) %>
<%
  fig(
    'nulling-gravity',
    %q{Nulling out a gravitational field is impossible in one dimension without exotic matter. 1. The planet imposes a nonvanishing gravitational
    field with a nonvanishing gradient. 2. We can null the field at one point in space, by placing a sphere of very dense, but
    otherwise normal, matter overhead. The stick figure still experiences a tidal force, $g'\ne 0$. 3. To change the field's
    derivative without changing the field, we can place two additional masses above and below the given point. But to change
    its derivative in the desired direction --- toward zero --- we would have to make these masses negative.}
  )
%>
<% end_marg %>

Since we're only doing the Newtonian limit, the gravitational field is the sum of the fields
made by all the sources, and we can take this as a sum over point sources. For a point source $m$
placed at $x_\zu{o}$, the field $g(x)$ is odd under reflection about $x_\zu{o}$. The derivative
of the field $g'(x)$ is even. Since $g'$ is even, we can't control its sign at $x=0$ by choosing
$x_\zu{o}>0$ or $x_\zu{o}<0$. The only way to control the sign of $g'$ is by choosing the sign of $m$.
Therefore if the sign of the externally imposed field's derivative is wrong, we can never null
it out. Figure \figref{nulling-gravity} shows a special case of this theorem.

The theorem does not apply to three dimensions, and it does not prove that all fields are impossible
to null out, only that some are. For example, the field inside a hemispherical shell can be nulled by
adding another hemispherical shell to complete the sphere. I thank P. Allen for helpful discussion
of this topic.
\end{eg}
<% end_sec %> % The Newtonian limit
<% begin_sec("Singularity theorems") %>\label{singularity-theorems}\index{singularity theorems}\index{Penrose-Hawking singularity theorems}
An important example of the use of the energy conditions is that Hawking and Ellis
have proved that under the assumption of the strong energy condition, 
any body that becomes sufficiently compact will end up forming a singularity.
We might imagine that the formation of a black hole would be a delicate thing, requiring perfectly
symmetric initial conditions in order to end up with the perfectly symmetric Schwarzschild metric.
Many early relativists thought so, for good reasons. If we look around the universe at various scales, we find that collisions
between astronomical bodies are extremely rare. This is partly because the distances are vast compared to the sizes
of the objects, but also because conservation of angular momentum has a tendency to make objects swing past one another
rather than colliding head-on. Starting with a cloud of objects, e.g., a globular cluster,
Newton's laws make it extremely difficult, regardless of the attractive nature of gravity, to pick initial conditions that will make them all collide the future.
For one thing, they would have to have exactly zero total angular momentum.

Most relativists now believe that this is not the case. General relativity describes gravity in terms of the tipping of light cones. When the field
is strong enough, there is a tendency for the light cones to tip over so far that the entire future light-cone
points at the source of the field. If this occurs on an entire surface surrounding the source, it is referred to
as a trapped
surface.\index{trapped surface}\index{volume expansion}\index{expansion scalar}\index{foliation}\index{congruence}  

To make this notion of light cones ``pointing at the source'' more rigorous, we need to define the volume expansion $\Theta$.
Let the set of all points in a spacetime (or some open subset of it) be expressed as the union of geodesics. This is referred to as a foliation
in geodesics, or a congruence. Let the velocity vector tangent to such a curve be $u^a$. Then we define $\Theta=\nabla_a u^a$. This is exactly
analogous to the classical notion of the divergence of the velocity field of a fluid, which is a measure of compression or expansion.
Since $\Theta$ is a scalar,\label{volume-expansion}
it is coordinate-independent. Negative values of $\Theta$ indicate that the geodesics are converging, so that volumes of space shrink. A trapped
surface is one on which $\Theta$ is negative when we foliate with lightlike geodesics oriented outward along normals to the surface.

When a trapped surface forms, any lumpiness or rotation in the initial conditions becomes irrelevant, because every particle's
entire future world-line lies inward rather than outward. A possible loophole in this argument is the question of whether the light cones
will really tip over far enough. We could imagine that under extreme
conditions of high density and temperature, matter might demonstrate unusual behavior, perhaps
including a negative energy density, which would then give rise to a gravitational repulsion.
Gravitational repulsion would tend to make the light cones tip outward rather than inward, possibly preventing the collapse to a singularity.
We can close this loophole by assuming an appropriate energy condition.
Penrose and Hawking have formalized the above argument in the form of a pair of
theorems, known as the singularity theorems.
One of these applies to the formation of black holes, and another one to cosmological singularities such as the
Big Bang.

In a cosmological model, it is natural to foliate using world-lines that are at rest relative to the Hubble flow (or, equivalently, the world-lines
of observers who see a vanishing dipole moment in the cosmic microwave background). The $\Theta$ we then obtain is positive,
because the universe is expanding. The volume expansion is $\Theta=3H_\zu{o}$, where $H_\zu{o}\approx 2.3\times10^{-18}\ \sunit^{-1}$ is the
Hubble constant (the fractional rate of change of the scale factor of cosmological distances).\index{Hubble constant}
The factor of three occurs because volume is proportional to the cube of the linear dimensions.
<% end_sec %> % Singularity theorems
<% begin_sec("Current status") %>
The current status of the energy conditions is shaky.
Although it is clear that all of them hold in a variety of situations, there are strong reasons to believe that
they are violated at both microscopic and cosmological scales, for reasons both classical
and quantum-mechanical.\footnote{Barcelo and Visser, ``Twilight for the energy conditions?,'' \url{http://arxiv.org/abs/gr-qc/0205066v1}.}
We will see such a violation in the following section.
<% end_sec %> % Current status
<% end_sec %> % Energy conditions

<% begin_sec("The cosmological constant") %>\label{sec:cosmological-constant}% Is a sub-sub-section, so only refer to it by page, not by section number.

Having included the source term in the Einstein field equations, our most important application will be to cosmology.
Some of the relevant ideas originate long before Einstein.
Once Newton had formulated a theory of gravity as a universal attractive force, he realized that there would be a tendency for the universe
to collapse. He resolved this difficulty by assuming that the universe was infinite in spatial extent, so that it would have
no center of symmetry, and therefore no preferred point to collapse toward. The trouble with this argument is that the
equilibrium it describes is unstable. Any perturbation of the uniform density of matter breaks the symmetry, leading to
the collapse of some pocket of the universe. If the radius of such a collapsing region is $r$, then its gravitational is
proportional to $r^3$, and its gravitational field is proportional to $r^3/r^2=r$. Since its acceleration is proportional
to its own size, the time it takes to collapse is independent of its size. The prediction is that the universe will
have a self-similar structure, in which the clumping on small scales behaves in the same way as clumping on large scales;
zooming in or out in such a picture gives a landscape that appears the same. With modern hindsight, this is actually not
in bad agreement with reality. We observe that the universe has a hierarchical structure consisting of solar systems,
galaxies, clusters of galaxies, superclusters, and so on. Once such a structure starts to condense, the collapse tends
to stop at some point because of conservation of angular momentum. This is what happened, for example, when our own
solar system formed out of a cloud of gas and dust.

Einstein confronted similar issues, but in a more acute form. Newton's symmetry argument, which failed only because of
its instability, fails even more badly in relativity: the entire spacetime can simply contract uniformly over time,
without singling out any particular point as a center. Furthermore, it is not obvious that angular momentum
prevents total collapse in relativity in the same way that it does classically, and even if it did, how would
that apply to the universe as a whole? Einstein's Machian orientation would have led him to reject the idea that
the universe as a whole could be in a state of rotation, and in any case it was sensible to start the study of
relativistic cosmology with the simplest and most symmetric possible models, which would have no preferred axis
of rotation.

Because of these issues, Einstein decided to try to patch up his field equation so that it would allow a static
universe.
Looking back over the considerations that led us to this form of the equation, we see that it is
very nearly uniquely determined by the following criteria:
\begin{enumerate}
\item It should be consistent with experimental evidence for local conservation of energy-momentum.
\item It should satisfy the equivalence principle.
\item It should be coordinate-independent.
\item It should be equivalent to Newtonian gravity or ``plain'' general relativity in the appropriate limit.
\item It should not be overdetermined.
\end{enumerate}
This is not meant to be a rigorous proof, just a general observation that it's not easy to tinker with the
theory without breaking it.

\begin{eg}{A failed attempt at tinkering}\label{eg:failed-scalar-tinkering}
As an example of the lack of ``wiggle room'' in the structure of the field equations, suppose we
construct the scalar $T\indices{^a_a}$, the trace of the stress-energy tensor, and try to insert
it into the field equations as a further source term. The first problem is that the field equation
involves rank-2 tensors, so we can't just add a scalar. To get around this, suppose we multiply by the
metric. We then have something like $G_{ab} = c_1 T_{ab}+c_2 g_{ab} T\indices{^c_c}$, where
the two constants $c_1$ and $c_2$ would be constrained by the requirement that the theory agree
with Newtonian gravity in the classical limit.

To see why this attempt fails, note that the stress-energy tensor of an electromagnetic field
is traceless, $T\indices{^c_c}=0$.
Therefore the beam of light's coupling to gravity in the $c_2$ term is zero. As discussed on
pp.~\pageref{kreuzer}-\pageref{end-kreuzer}, empirical tests of conservation of momentum would therefore
constrain $c_2$ to be $\lesssim 10^{-8}$.
\end{eg}

One way in which we \emph{can} change the field equation without violating any of these 
requirements is to add a term $\Lambda g_{ab}$, giving
\begin{equation*}
  G_{ab} = 8\pi T_{ab} + \Lambda g_{ab}\eqquad,
\end{equation*}
which is what we will refer to as the Einstein field
equation.\footnote{In books that use a $-+++$ metric rather then our
$+---$, the sign of the cosmological constant term is reversed
relative to ours.}
As we'll see in example \ref{eg:lambda-must-be-constant} on p.~\pageref{eg:lambda-must-be-constant},
this is consistent with conservation of energy-momentum (requirement 1 above) if and only if $\Lambda$ is constant.
In example \ref{eg:lambda-is-cosmological} we find that its effects are only significant on the largest scales,
which makes it undetectable, for example, in solar-system tests (criterion 4).
For these reasons  $\Lambda$ is referred to as the
cosmological constant.\index{cosmological constant}\index{Einstein
field equation} As we'll see below, Einstein introduced it in order to make a certain type of cosmology work.

We could also choose to absorb
the $\Lambda g_{ab}$ term in the field equations into the $8\pi
T_{ab}$, as if the cosmological constant term were due to some form of matter.
It would then be a perfect fluid (example \ref{eg:perfect-fluid-tensorial}, p.~\pageref{eg:perfect-fluid-tensorial})
with a negative pressure, and it would violate the strong energy condition
(example \ref{eg:lambda-energy-conditions}, p.~\pageref{eg:lambda-energy-conditions}).
When we think of it this way, it's common these days to refer to it
as \emph{dark energy}.
But even if we think of it as analogous to a matter field, its constancy means that it has none of its
own independent degrees of freedom. It can't vibrate, rotate, flow, be compressed or rarefied, heated or cooled.
It acts like a kind of energy that is automatically built in to every cubic centimeter of space.
This is closely related to the fact that its contribution to the stress-energy tensor is proportional
to the metric. One way of stating the equivalence principle (requirement 2 above)
is that space itself does not come equipped with any other tensor besides the metric.

Einstein originally introduced a positive cosmological
constant because he wanted relativity to be able to describe a static
universe.  To see why it would have this effect, compare its behavior
with that of an ordinary fluid.  When an ordinary fluid, such as the
exploding air-gas mixture in a car's cylinder, expands, it does work
on its environment, and therefore by conservation of energy its own
internal energy is reduced. A positive cosmological constant, however,
acts like a certain amount of mass-energy built into every cubic meter
of vacuum. Thus when it expands, it \emph{releases} energy. Its
pressure is negative. 

Now consider the following semi-relativistic argument.
Although we've already seen (page \pageref{no-separate-k-and-u}) that there is no useful way to separate the
roles of kinetic and potential energy in general relativity, suppose that there are some quantities analogous
to them in the description of the universe as a whole. (We'll see below that the universe's contraction and
expansion is indeed described by a set of differential equations that can be interpreted in essentially this way.)
If the universe contracts, a cubic meter of space becomes less than a cubic meter.
The cosmological-constant energy associated with that volume is reduced, so some energy has been consumed.
The kinetic energy of the collapsing matter goes down, and the collapse is decelerated.

\begin{eg}{Cosmological constant must be constant}\label{eg:lambda-must-be-constant}
If $\Lambda$ is thought of as a form of matter, then it becomes 
natural to ask whether\label{varying-lambda}\index{cosmological constant!no variation of}
it's spread more thickly in some places than others: is the cosmological ``constant'' really constant?
The following argument shows that it cannot vary.
The field equations are $G_{ab}=8\pi T_{ab}+\Lambda g_{ab}$.
Taking the divergence of both sides, we have $\nabla^a G_{ab}=8\pi \nabla^a T_{ab}+\nabla^a (\Lambda g_{ab})$.
The left-hand side vanishes (see p.~\pageref{einstein-tensor-div-free}). Since laboratory experiments have
verified conservation of mass-energy to high precision for all the forms of matter represented by $T$,
we have $\nabla^a T_{ab}=0$ as well. Applying the product rule to the term $\nabla^a (\Lambda g_{ab})$,
we get $g_{ab}\nabla^a\Lambda+\Lambda\nabla^a g_{ab}$. But the covariant derivative of the metric vanishes,
so the result is simply $\nabla_b\Lambda$. Thus any variation in the cosmological constant over space or time
violates the field equations, and the violation is equivalent to the violation we would get from a form of matter
than didn't conserve mass-energy locally.
\end{eg}

\begin{eg}{Cosmological constant is cosmological}\label{eg:lambda-is-cosmological}
The addition of the $\Lambda$ term constitutes a change to the \emph{vacuum} field equations, and the good agreement
between theory and experiment in the case of, e.g., Mercury's orbit puts an upper limit on $\Lambda$ then implies
that $\Lambda$ must be small. For an
order-of-magnitude estimate, consider that $\Lambda$ has units of mass density, and the only parameters with units
that appear in the description of Mercury's orbit are the mass of the sun, $m$, and the radius of Mercury's orbit, $r$.
The relativistic corrections to Mercury's orbit are on the order of $v^2$, or about $10^{-8}$, and they come out right.
Therefore we can estimate  that the cosmological constant could not have been greater than about $(10^{-8})m/r^3 \sim 10^{-10}\ \kgunit/\munit^3$,
or it would have caused noticeable discrepancies. This is a very poor bound; if $\Lambda$ was this big, we might even be
able to detect its effects in laboratory experiments.
Looking at the role played by $r$ in the estimate, we see that the
upper bound could have been made tighter by increasing $r$. Observations on galactic scales, for example, constrain it much
more tightly. This justifies the description of $\Lambda$ as cosmological:
the larger the scale, the more significant the effect of a nonzero $\Lambda$ would be.
\end{eg}

\begin{eg}{Energy conditions}\label{eg:lambda-energy-conditions}
Since the right-hand side of the field equation is $8\pi T_{ab} + \Lambda g_{ab}$, it is possible to consider
the cosmological constant as a type of matter contributing to the stress-energy tensor.\index{energy conditions!violated by cosmological constant}
We then have $\rho=-P=\Lambda/8\pi$.\label{lambda-and-energy-conditions}
As described in more detail in section \ref{sec:cosmology-observation} on p.~\pageref{sec:cosmology-observation},
we now know that $\Lambda$ is positive.
With $\Lambda>0$, the weak and dominant energy conditions are both satisfied, so that in every frame of reference,
$\rho$ is positive and there is no flux of energy flowing at speeds greater than $c$.
The negative pressure does violate the strong energy condition, meaning that the constant acts as a form
of gravitational repulsion. If the cosmological constant is a product of the
quantum-mechanical structure of the vacuum, then this violation 
is not too surprising, because quantum fields are known to violate
various energy conditions.
For example, the energy density between two parallel conducting plates
is negative due to the Casimir effect.\index{Casimir effect}
\end{eg}


<% end_sec %> % The cosmological constant
<% end_sec %> % Sources in general relativity

<% begin_sec("Cosmological solutions") %>\label{sec:cosmological-solutions}
We are thus led to pose two interrelated questions. First, what can empirical observations about the universe tell us about
the laws of physics, such as the zero or nonzero value of the cosmological constant? Second, what can the laws of physics, combined with observation,
tell us about the large-scale structure of the universe, its origin, and its fate?

<% begin_sec("Evidence for the finite age of the universe") %>
We have a variety of evidence that the universe's existence does not stretch for an unlimited time into the past.

When astronomers view light
from the deep sky that has been traveling through space for billions of years, they observe a universe that looks different from
today's. For example, quasars were common in the early universe but are uncommon today. 

In the present-day universe, stars
use up deuterium nuclei,\index{deuterium!evidence for finite age of universe}
but there are no known processes that could replenish their supply. We therefore expect that
the abundance of deuterium in the universe should decrease over time. If the universe had existed for an infinite
time, we would expect that all its deuterium would have been lost, and yet we observe that deuterium does exist
in stars and in the interstellar medium.

The second law of thermodynamics predicts that any system should approach a state of thermodynamic equilibrium, and
yet our universe is very far from thermal equilibrium, as evidenced by the fact that our sun is hotter than
interstellar space, or by the existence of functioning heat engines such as your body or an automobile engine.

With hindsight, these observations suggest that we should not look for cosmological models that persist for
an infinite time into the past.
<% end_sec %> % 

<% begin_sec("Evidence for expansion of the universe") %>\label{evidence-for-expansion}
We don't only see time-variation in locally observable quantities such as quasar abundance, deuterium abundance,
and entropy. In addition, we find empirical evidence for global changes in the universe.
By 1929, Edwin Hubble\index{Hubble, Edwin} at Mount Wilson had determined that the universe was expanding, and historically
this was the first convincing evidence that
Einstein's original goal of modeling a static cosmology had been a mistake.
Einstein later referred
to the cosmological constant as the ``greatest blunder of my life,'' and for the next 70 years it was
commonly assumed that $\Lambda$ was exactly zero. 

Since we observe that the universe is expanding, the laws of thermodynamics require that it also be cooling, just
as the exploding air-gas mixture in a car engine's cylinder cools as it expands.
If the universe is currently expanding and cooling, it is natural to imagine that in the past it might have been
very dense and very hot.
This is confirmed directly by looking up in the sky and seeing radiation from the hot early universe.\index{cosmic microwave background!discovery of}
In 1964, Penzias and Wilson\index{Penzias, Arno}\index{Wilson, Robert} at Bell Laboratories in New Jersey detected a mysterious background of microwave radiation
using a directional horn antenna. As with many accidental
discoveries in science, the important thing was to pay attention to the surprising observation
rather than giving up and moving on when it confounded attempts to understand it. They pointed
the antenna at New York City, but the signal didn't increase. The radiation didn't
show a 24-hour periodicity, so it couldn't be from a source in a certain direction in the sky.
They even went so far as to sweep out the pigeon droppings inside.
It was eventually established that the radiation was
coming uniformly from all directions in the
sky and had a black-body spectrum with a temperature of about 3 K.
<% marg(60) %>
<%
  fig(
    'penzias-wilson-antenna',
    %q{The horn antenna used by Penzias and Wilson.}
  )
%>
<% end_marg %>

This is now interpreted as follows. At one time, the universe was hot enough to
ionize matter. An ionized gas is opaque to light, since the oscillating fields of an electromagnetic
wave accelerate the charged particles, depositing kinetic energy into them. Once the universe
became cool enough, however, matter became electrically neutral, and the universe became
transparent. Light from this time is the most long-traveling light that we can detect now.
The latest data show that transparency set in
when the temperature was about 3000 K. The surface we see, dating back to this time, is known
as the surface of last scattering.\index{surface of last scattering} Since then, the universe has expanded by about a factor
of 1000, causing the wavelengths of photons to be stretched by the same amount due to the
expansion of the underlying space. This is equivalent to a Doppler shift due to the source's
motion away from us; the two explanations are equivalent. We therefore see the 3000 K optical
black-body radiation red-shifted to 3 K, in the microwave region.

It is logically possible to have a universe that is expanding but
whose local properties are nevertheless static, as in the steady-state
model of Fred Hoyle,\index{Hoyle, Fred}\index{steady-state cosmology}
in which some novel physical process spontaneously creates new
hydrogen atoms, preventing the infinite dilution of matter over the
universe's history, which in this model extends infinitely far into
the past. But we have already seen strong empirical evidence that the
universe's local properties (quasar abundance, etc.) \emph{are}
changing over time.  The CMB is an even more extreme and direct
example of this; the universe full of hot, dense gas that emitted the
CMB is clearly nothing like today's universe.
A brief discussion of the steady-state model is given in section \ref{sec:steady-state}, 
p.~\pageref{sec:steady-state}.

<% end_sec %> % evidence for expansion of the universe

<% begin_sec("Evidence for homogeneity and isotropy") %>\label{sec:evidence-for-homogeneity}
These observations demonstrate that the universe is not homogeneous in time, i.e., that one can observe the present conditions of the
universe (such as its temperature and density), and infer what epoch of the universe's evolution we inhabit.
A different question is the Copernican one of whether the universe is homogeneous in space.
Surveys of distant quasars show that the universe has very little structure at scales greater than a few times $10^{25}$ m.
(This can be seen on a remarkable logarithmic map  constructed by Gott et al., \url{astro.princeton.edu/universe}.)
This suggests that we can, to a good approximation, model the universe as being 
isotropic (the same in all spatial directions) and homogeneous (the same at all
locations in space).

Further evidence comes from the extreme uniformity of the cosmic microwave background radiation,\index{cosmic microwave background!isotropy of}
once one subtracts out the dipole anisotropy due to the Doppler shift arising from our galaxy's motion relative to the CMB.
When the CMB was first discovered, there was doubt about whether it was cosmological in origin (rather than, say,
being associated with our galaxy), and it was expected that its isotropy would be as large as 10\%.
As physicists began to be convinced that it really was a relic of the early universe, interest focused
on measuring this anisotropy, and a series of measurements put tighter and tighter upper bounds on it.

Other than the dipole term, there are two ways in which one might naturally expect anisotropy to
occur. There might have been some lumpiness in the early universe, which might have served as seeds for
the condensation of galaxy clusters out of the cosmic medium. Furthermore, we might wonder whether the
universe as a whole is rotating. The general-relativistic notion of rotation is very different from the
Newtonian one, and in particular, it is possible to have a cosmology that is rotating without having any
center of rotation (see problem \ref{hw:rotation-with-no-center}, p.~\pageref{hw:rotation-with-no-center}).
In fact one of the first exact solutions discovered for the Einstein field equations was the
G\"{o}del metric,\index{G\"{o}del metric} which described a bizarre rotating universe with closed timelike curves,
i.e., one in which causality was violated. In a rotating universe, one expects that radiation received from
great cosmological distances will have a transverse Doppler shift, i.e., a shift originating from the time dilation
due to the motion of the distant matter across the sky. This shift would be greatest for sources lying in
the plane of rotation relative to us, and would vanish for sources lying along the axis of rotation.
The CMB would therefore show variation with the form of a quadrupole term, $3\cos^2\theta-1$.
In 1977 a U-2 spyplane (the same type involved in the 1960 U.S.-Soviet incident) was used by
Smoot et al.\footnote{G. F. Smoot, M. V. Gorenstein, and R. A. Muller, ``Detection of Anisotropy in the Cosmic Blackbody Radiation,'' Phys. Rev. Lett. 39 (1977) 898.
The interpretation of the CMB measurements is somewhat model-dependent; in the early years of observational cosmology,
it was not even universally accepted that the CMB had a cosmological origin. The best model-independent
limit on the rotation of the universe comes from observations of the solar system,
Clemence, ``Astronomical Time,'' Rev. Mod. Phys. 29 (1957) 2.}
to search for anisotropies in the CMB. This experiment was the first to definitively succeed in detecting
the dipole anisotropy. After subtraction of the dipole component, the CMB was found to be uniform at the
level of $\sim 3\times 10^{-4}$. This provided strong support for homogeneous cosmological models, and ruled
out rotation of the universe with $\omega \gtrsim 10^{-22}\ \zu{Hz}$.
<% end_sec %>

<% begin_sec("The FRW cosmologies") %>
<% begin_sec("The FRW metric and the standard coordinates") %>
Motivated by Hubble's observation that the universe is expanding, we hypothesize the existence of solutions of the
field equation in which the properties of space are homogeneous and isotropic,
but the over-all scale of space is increasing as described by some scale function $a(t)$. Because of
coordinate invariance, the metric can still be written in a variety of forms. One such form is
\begin{equation*}
  \der s^2 = \der t^2 - a(t)^2\der\ell^2\eqquad,
\end{equation*}
where the spatial part is
\begin{equation*}
  \der\ell^2 = f(r)\der r^2 + r^2 \der\theta^2 + r^2 \sin^2\theta \der\phi^2\eqquad.
\end{equation*}

To interpret the coordinates, we note that if an observer is able to determine the functions $a$ and $f$ for her universe,
then she can always measure some scalar curvature such as the Ricci scalar or the Kretchmann invariant, and since these
are proportional to $a$ raised to some power, she can determine $a$ and $t$. This shows that $t$ is a ``look-out-the-window'' time,
i.e., a time coordinate that we can determine by looking out the window and observing the present conditions in the universe. Because the quantity being
measured directly is a scalar, the result is independent of the observer's state of motion. (In practice, these scalar curvatures
are difficult to measure directly, so we measure something else, like the sky-wide average temperature of the cosmic microwave background.)
Simultaneity is supposed to be ill-defined in relativity, but the look-out-the-window time defines a notion of simultaneity that
is the most naturally interesting one in this spacetime. With this particular definition of simultaneity, we can also define a
preferred state of rest at any location in spacetime, which is the one in which $t$ changes as slowly as possible relative to one's own clock. This local rest frame,
which is more easily determined in practice as the one in which the microwave background is most uniform across the sky, can
also be interpreted as the one that is moving along with the Hubble flow, i.e., the average motion of the galaxies, photons, or
whatever else inhabits the spacetime. The time $t$ is interpreted as the proper time of a particle that has always been locally at rest.
The spatial distance measured by $L=\int a\der\ell$ is called the proper distance.\index{proper distance}\label{proper-distance-cosmologically}
It is the distance that would be measured by a chain of rulers, each of them
``at rest'' in the above sense.

These coordinates are referred as the ``standard'' cosmological coordinates; one will also encounter
other choices, such as the comoving and conformal coordinates, which are more convenient for certain purposes.
Historically, the solution for the functions $a$ and $f$ was found by de 
Sitter in 1917.\index{de Sitter, Willem}\index{standard cosmological coordinates}\index{comoving cosmological coordinates}\index{conformal cosmological coordinates}
\index{cosmological coordinates!standard}\index{cosmological coordinates!comoving}\index{cosmological coordinates!conformal}\label{standard-cosm-coords}
<% end_sec %> % The FRW metric and the standard coordinates

<% begin_sec("The spatial metric") %>
The unknown function $f(r)$ has to give a 3-space metric $\der\ell^2$ with a constant Einstein curvature tensor.
The following Maxima program computes the curvature.
\begin{listing}{1}
load(ctensor);
dim:3;
ct_coords:[r,theta,phi];
depends(f,t);
lg:matrix([f,0,0],
          [0,r^2,0],
          [0,0,r^2*sin(theta)^2]);
cmetric();
einstein(true);
\end{listing}
% constant_curvature.mac
Line 2 tells Maxima that we're working in a space with three dimensions rather than its default of four.
Line 4 tells it that $f$ is a function of time.  Line 9 uses its built-in function for computing the Einstein tensor $G\indices{^a_b}$. The result has only one nonvanishing
component, $G\indices{^t_t}=(1-1/f)/r^2$. This has to be constant, and since scaling can be absorbed in the factor $a(t)$ in the
3+1-dimensional metric, we can just set the value of $G_{tt}$ more or less arbitrarily, except for its sign.
The result is $f=1/(1-kr^2)$, where $k=-1$, 0, or 1.

The resulting metric, called the Robertson-Walker metric, is
\begin{equation*}
  \der s^2 = \der t^2 - a^2\left(\frac{\der r^2}{1-kr^2} + r^2 \der\theta^2 + r^2 \sin^2\theta \der\phi^2\right)\eqquad.
\end{equation*}

The form of $\der\ell^2$ shows us that $k$ can be interpreted
in terms of the sign of the spatial curvature. We recognize the $k=0$ metric as a flat spacetime described in spherical coordinates.
To interpret the $k\ne 0$ cases, we note that a circle at coordinate $r$ has proper circumference $C=2\pi a r$
and proper radius $R=a\int_0^r \sqrt{f(r')}\der r'$. For $k<0$, we have $f<1$ and $C>2\pi R$, indicating
negative spatial curvature. For $k>0$ there is positive curvature.

Let's examine the positive-curvature case more closely. Suppose we select a particular plane of simultaneity defined by
$t=\text{constant}$ and $\phi=\pi/2$, and we start doing geometry in this plane.
In two spatial dimensions, the Riemann tensor only has a single independent component, which can be identified
with the Gaussian curvature (sec.~\ref{sec:curvature-tensors}, p.~\pageref{sec:curvature-tensors}), and
when this Gaussian curvature is positive and constant, it can be interpreted as the angular defect of a triangle per unit area
(sec.~\ref{sec:curvature-2-d}, p.~\pageref{sec:curvature-2-d}). Since the sum of the interior angles of a triangle
can never be greater than $3\pi$, we have an upper limit on the area of any triangle. This happens because
the positive-curvature Robertson-Walker metric represents a cosmology that is spatially finite. At a given $t$, it
is the three-dimensional analogue of a two-sphere. On a two-sphere, if we set up polar coordinates with a given
point arbitrarily chosen as the origin, then we know that the $r$ coordinate must ``wrap around'' when we get to
the antipodes. That is, there is a coordinate singularity there. (We know it can only be a coordinate singularity,
because if it wasn't, then the antipodes would have special physical characteristics, but the FRW model was
constructed to be spatially homogeneous.) This ``wrap-around'' behavior is described by saying that the model
is \emph{closed}.\index{closed cosmology}
<% marg(70) %>
<%
  fig(
    'triangles',
    %q{1. In the Euclidean plane, this triangle can be scaled by any factor while remaining similar to itself.
       2. In a plane with positive curvature, geometrical figures have a maximum area and maximum linear dimensions. This triangle has almost the maximum area, because the sum
          of its angles is nearly $3\pi$.
       3. In a plane with negative curvature, figures have a maximum area but no maximum linear dimensions. This triangle has almost the maximum area, because the sum
          of its angles is nearly zero. Its vertices, however, can still be separated from one another without limit.}
  )
%>
<% end_marg %>

In the negative-curvature case, there is no limit on distances, \subfigref{triangles}{3}. Such a universe is called \emph{open}.\index{open cosmology}
In the case of an open universe, it is particularly easy to demonstrate a fact that bothers many students, which is that
proper distances can grow at rates exceeding $c$. Let particles A and B both be at rest relative to the Hubble flow.
The proper distance between them is then given by $L=a\ell$, where $\ell=\int_A^B\der\ell$ is constant. Then differentiating
$L$ with respect to the look-out-the-window time $t$ gives $\der L/\der t=\dot{a}\ell$. In an open universe,
there is no limit on the size of $\ell$, so at any given time, we can make $\der L/\der t$ as large as we like.
This does not violate special relativity, since it is only locally that special relativity is a valid approximation to general
relativity. Because GR only supplies us with frames of reference that are local, the velocity of two objects relative to one another is
not even uniquely defined; our choice of $\der L/\der t$ was just one of infinitely many possible definitions.

The distinction between closed and open universes is not just a matter of geometry, it's a matter of topology as well.
Just as a two-sphere cannot be made into a Euclidean plane without cutting or tearing, a closed universe is not topologically
equivalent to an open one. The correlation between local properties (curvature) and global ones (topology) is a general
theme in differential geometry. A universe that is open is open forever,
and similarly for a closed one. 
<% end_sec %> % The spatial metric

<% begin_sec("The Friedmann equations") %>
Having fixed $f(r)$, we can now see what the field equation tells us about $a(t)$.
The next program computes the Einstein tensor for the full four-dimensional spacetime:
\begin{listing}{1}
load(ctensor);
ct_coords:[t,r,theta,phi];
depends(a,t);
lg:matrix([1,0,0,0],
          [0,-a^2/(1-k*r^2),0,0],
          [0,0,-a^2*r^2,0],
          [0,0,0,-a^2*r^2*sin(theta)^2]);
cmetric();
einstein(true);
\end{listing}
% cosmology.mac
The result is
\begin{align*}
  G\indices{^t_t} &= 3\left(\frac{\dot{a}}{a}\right)^2 + 3ka^{-2} \\
  G\indices{^r_r} = G\indices{^\theta_\theta} = G\indices{^\phi_\phi} &= 2\frac{\ddot{a}}{a} + \left(\frac{\dot{a}}{a}\right)^2 + ka^{-2}\eqquad,
\end{align*}
where dots indicate differentiation with respect to time.

Since we have $G\indices{^a_b}$ with mixed upper and lower indices, we either have to convert it into $G_{ab}$, or write out the
field equations in this mixed form. The latter turns out to be simpler. In terms of mixed indices, $g\indices{^a_b}$ is always simply
$\operatorname{diag}(1,1,1,1)$. Arbitrarily singling out $r=0$ for simplicity, we have $g=\operatorname{diag}(1,-a^2,0,0)$. The stress-energy tensor is
$T\indices{^\mu_\nu}=\operatorname{diag}(\rho,-P,-P,-P)$. (See
example \ref{eg:perfect-fluid-tensorial} on p.~\pageref{eg:perfect-fluid-tensorial} for the signs.)
Substituting into $G\indices{^a_b}=8\pi T\indices{^a_b}+\Lambda g\indices{^a_b}$, we find
\begin{align*}
  3\left(\frac{\dot{a}}{a}\right)^2 + 3ka^{-2} - \Lambda                     &= 8\pi\rho \\
  2\frac{\ddot{a}}{a} + \left(\frac{\dot{a}}{a}\right)^2 + ka^{-2} - \Lambda &= -8\pi P\eqquad.
\end{align*}
Rearranging a little, we have a set of differential equations known as the Friedmann equations,\index{Friedmann equations}
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &= \frac{1}{3}\Lambda - \frac{4\pi}{3}(\rho+3P) \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{1}{3}\Lambda + \frac{8\pi}{3}\rho-k a^{-2}\eqquad.
\end{align*}\label{friedmann-equations}

% This now checks out against Carroll, \url{http://nedwww.ipac.caltech.edu/level5/March01/Carroll3/Carroll8.html }, except for the
%       lambda terms, which he doesn't include.

The cosmology that results from a solution of these differential equations is known as the
Friedmann-Robertson-Walker (FRW) or Friedmann-Lema\^{i}tre-Robertson-Walker (FLRW) cosmology.\index{Friedmann-Robertson-Walker cosmology}

The first Friedmann equation describes the rate at which cosmological expansion accelerates or decelerates.
Let's refer to it as the acceleration equation. It expresses the basic idea of the field equations, which is
that non-tidal curvature (left-hand side) is caused by the matter that is present locally (right-hand side).
Example \ref{eg:cosmo-hole} illustrates this in a simple case.
<% marg(20) %>
<%
  fig(
    'friedmann-portrait',
    %q{Alexander Friedmann (1888-1925).}
  )
%>
<% end_marg %>

The second Friedmann equation tells us the magnitude of the
rate of expansion or contraction. Call it the velocity equation. The quantity $\dot{a}/a$, evaluated at the present
cosmological time, is the Hubble constant $H_\zu{o}$ (which is constant only in the sense that at a fixed time, it
is a constant of proportionality between distance and recession velocity).\index{Hubble constant}

To the practiced eye, it seems odd to have two dynamical laws, one predicting velocity and one acceleration.
The analogous laws in freshman mechanics would be Newton's second law, which predicts acceleration, and conservation
of energy, which predicts velocity. Newton's laws and conservation of energy are not independent, and for mechanical
systems either can be derived from the other. 
The Friedmann equations, however, are not overdetermined or redundant. They are underdetermined, because
we want to predict \emph{three} unknown functions of time: $a$, $\rho$, and $P$.
Since there are only two equations, they are not sufficient
to uniquely determine a solution for all three functions. The third constraint comes in the form of
some type of equation of state for the matter described by $\rho$ and $P$, which in simple models can often
be written in the form $P=w\rho$. For example, dust has $w=0$.

Unlike $a$, $\rho$, and $P$, the cosmological constant $\Lambda$
is not free to vary with time; if it did, then the stress-energy tensor would have a nonvanishing divergence,
which is not consistent with the Einstein field equations (see p.~\pageref{varying-lambda}).

Although general relativity does not provide any scalar, globally conserved measure of mass-energy that is
conserved in all spacetimes, the Friedmann velocity equation can be loosely interpreted as a statement of
conservation of mass-energy in an FRW spacetime. The left-hand side acts like kinetic energy. In a
cosmology that expands and then recontracts in a Big Crunch, the turn-around point is defined by the time
at which the right-hand side equals zero. The origin of the velocity equation is in fact the time-time
part of the field equations, whose source term is the mass-energy component of the stress-energy tensor.
<% marg(-10) %>
<%
  fig(
    'cosmo-hole',
    %q{Example \ref{eg:cosmo-hole}.}
  )
%>
<% end_marg %>

\begin{eg}{Scooping out a hole}\label{eg:cosmo-hole} 
This example illustrates the connection between cosmological acceleration and local density of matter
given by the Friedmann acceleration equation.
Consider two cosmologies, each with $\Lambda=0$. Cosmology 1 is 
an FRW spacetime in which all matter is in the form of nonrelativistic particles such as atoms or galaxies.
2 is identical to 1, except that all the matter has been scooped out of a small spherical region S, leaving
a vacuum. (``Small'' means small compared to the Hubble scale $1/H_\zu{o}$.)
Within S, we introduce test particles A and B. Because an FRW spacetime is homogeneous
and isotropic, cosmology 2 retains spherical symmetry about the center of S.
Since $\Lambda=0$, Birkhoff's theorem applies to 2, so 2 is flat inside S.
Therefore in 2, the relative acceleration $\vc{a}$ of the test particles equals zero.

Because S is small compared to cosmological distances, and because the dust is nonrelativistic,
local observers can accurately attibute the difference
in behavior between 1 and 2 to the Newtonian gravitational force from the dust that was present
in 1 but not in 2. 
For convenience, let A and B both be initially at rest relative to the local dust
(i.e., having $\dot{\theta}=\dot{\phi}=0$). 
By the definition of the scale factor (i.e., by inspection of the FRW metric), the distance between them 
varies as $\text{const}\times a(t)$. If one of these particles is an observer, she sees a ``force''
acting on the other particle that causes an acceleration $(\ddot{a}/a)\vc{r}$, where
$\vc{r}$ is the displacement between the particles.

Since $\vc{a}=0$ in 2, it follows that the acceleration in 1 can be calculated
accurately by finding the Newtonian gravitational force due to the added dust. This results in a connection
between $\ddot{a}/a$, on the left-hand side of the Friedmann acceleration equation, and $\rho$, on the right side.

For consistency, we can verify that the Newtonian gravitational force exerted by a uniform sphere, at a point
on its interior, is proportional to $\vc{r}$. This is a classic result that is easily derived from
Newton's shell theorem.
\end{eg}
<% end_sec %> % The Friedmann equations
<% end_sec %> % The FRW cosmologies

<% begin_sec("A singularity at the Big Bang") %>
The Friedmann equations only
allow a constant $a$ in the case where $\Lambda$ is perfectly tuned relative to the other parameters, and even this artificially
fine-tuned equilibrium turns out to be unstable. These considerations make a static cosmology implausible on theoretical grounds, and
they are also consistent with the observed Hubble expansion (p.~\pageref{evidence-for-expansion}).

Since the universe is not static, what happens if we use general relativity to extrapolate farther and farther back in time?
<% marg(40) %>
<%
  fig(
    'lemaitre',
    %q{Georges Lema\^{i}tre (1894-1966) proposed in 1927 that our universe be modeled in general relativity as a spacetime
    in which space expanded over time. Lema\^{i}tre's ideas
    were initially treated skeptically by Eddington and Einstein, who told him,
    ``Your calculations are correct, but your physics is abominable.''
    Later, as Hubble's observational evidence for cosmological expansion became widely accepted, both Einstein and Eddington
    became converts, helping to bring Lema\^{i}tre's ideas to the attention of the community. In 1931, an emboldened Lema\^{i}tre
    described the idea that the universe began from a ``Primeval Atom'' or ``Cosmic Egg.'' The name that eventually stuck was ``Big Bang,''
    coined by Fred Hoyle as a derisive term.}
  )
%>
<% end_marg %>

If we extrapolate the Friedmann equations
backward in time, we find that they always have $a=0$ at some point in the past, and this
occurs regardless of the details of what we assume about the matter and radiation that
fills the universe. To see this, note that, as discussed in example \ref{eg:dust-and-radiation-cosm}
on page \pageref{eg:dust-and-radiation-cosm}, radiation is expected to dominate the early universe, for
generic reasons that are not sensitive to the (substantial) observational uncertainties about the universe's present-day mixture of ingredients.
Under radiation-dominated conditions, we can approximate
$\Lambda=0$ and $P=\rho/3$ (example \ref{eg:dust-and-radiation-cosm},
p.~\pageref{eg:dust-and-radiation-cosm})
in the first Friedmann equation,
finding
\begin{equation*}
  \frac{\ddot{a}}{a} = - \frac{8\pi}{3}\rho 
\end{equation*}
where $\rho$ is the density of mass-energy due to radiation. Since $\ddot{a}/a$ is always negative,
the graph of $a(t)$ is always concave down, and since $a$ is currently increasing, there must be some
time in the past when $a=0$. One can readily verify that this is not just a coordinate singularity;
the Ricci scalar curvature $R\indices{^a_a}$ diverges, and the singularity occurs at a finite proper time in the past.

In section \ref{sec:black-hole-singularity}, we saw that a black hole contains a singularity,
but it appears that such singularities are always hidden behind event horizons, so that
we can never observe them from the outside. The FRW singularity, however, is not
hidden behind an event horizon. It lies in our past light-cone, and our own world-lines
emerged from it.
The universe, it seems,
originated in a Big Bang,\index{Big Bang} a concept that originated with the Belgian Roman Catholic priest
Georges Le\-ma\^{i}tre.\index{Lema\^{i}tre, Georges}

Self-check: Why is it not correct to think of the Big Bang as an explosion that occurred at a specific
point in space?

Does the FRW singularity represent something real about our universe?

One thing to worry about is the accuracy of our physical modeling of the radiation-dominated universe. The presence
of an initial singularity in the FRW solutions does not depend sensitively on on assumptions like $P=\rho/3$, but
it is still disquieting that no laboratory
experiment has ever come close to attaining the conditions under which we could test whether a gas of photons
produces gravitational fields as predicted by general relativity. We saw on p.~\pageref{kreuzer} that static
electric fields do produce gravitational fields as predicted, but this is not the same as an empirical confirmation
that electromagnetic waves also act as gravitational sources in exactly the manner that general relativity claims.
We do, however, have a consistency check in the form of the abundances of nuclei. Calculations of nuclear reactions
in the early, radiation-dominated universe predict certain abundances 
of hydrogen, helium, and deuterium.\index{deuterium!test of cosmological models}
In particular,
the relative abundance of helium and deuterium is a sensitive test of the relationships among $a$, $\dot{a}$, and $\ddot{a}$
predicted by the FRW equations, and they confirm these relationships to a precision of about $5\pm 4$\%.\footnote{Steigman,
Ann. Rev. Nucl. Part. Sci. 57 (2007) 463. These tests are stated in terms of the Hubble ``constant'' $H=\dot{a}/a$,
which is actually varying over cosmological time-scales.
The nuclear helium-deuterium ratio is sensitive to $\dot{H}/H$.}\label{light-gravity-from-bbn}

An additional concern is whether the
Big Bang singularity is just a product of the unrealistic assumption of perfect symmetry  that went into the
FRW cosmology.
One of the Penrose-Hawking singularity theorems\index{Penrose-Hawking singularity theorems} proves that it
is not.\footnote{Hawking and Ellis, ``The Cosmic Black-Body Radiation and the Existence of Singularities in Our Universe,''
Astrophysical Journal, 152 (1968) 25. Available online at \url{articles.adsabs.harvard.edu}.}
This particular singularity theorem requires three conditions: (1) the strong energy condition holds; (2) there are no closed timelike curves; and
(3) a trapped surface exists in the past timelike geodesics originating at some point.
The requirement of a trapped surface can fail if the universe is inhomogeneous to $\gtrsim 10^{-4}$, but observations
of the cosmic microwave background rule out any inhomogeneity this large (see p.~\pageref{sec:evidence-for-homogeneity}). The other possible failure of the assumptions is that if the cosmological constant is large enough, it violates the strong
energy equation, and we can have a Big Bounce rather than a Big Bang (see 
p.~\pageref{bb-singularity-with-lambda}).\label{bb-singularity-without-lambda}\label{big-bang-singularity-inevitable}

<% begin_sec("An exceptional case: the Milne universe") %>\index{Milne universe}\label{milne}
There is still a third loophole in our conclusion that the Big Bang singularity must have existed. Consider the special
case of the FRW analysis, found by Milne in 1932 (long before FRW), in which the universe is completely empty, with $\rho=0$ and $\Lambda=0$. This is of course not
consistent with the fact that the universe contains stars and galaxies, but we might wonder whether it could tell us
anything interesting as a simplified approximation to a very dilute universe. The result is that the scale factor $a$
varies linearly with time (problem \ref{hw:verify-milne}, p.~\pageref{hw:verify-milne}). If $a$ is not constant, then
there exists a time at which $a=0$, but this doesn't turn out to be a real singularity (which isn't surprising, since there
is no matter to create gravitational fields). Let this universe have a scattering of test particles whose masses are too
small to invalidate the approximation of $\rho=0$, and let the test particles be at rest in the $(r,\theta,\phi)$ coordinates.
The linear dependence of $a$ on $t$ means that these particles simply move inertially and without any gravitational
interactions, spreading apart from one another
at a constant rate like the raisins in a rising loaf of raisin bread. The Friedmann equations require $k=-1$, so the spatial
geometry is one of constant negative curvature.

The Milne universe is in fact flat spacetime described in tricky coordinates. The connection can be made as follows.
Let a spherically symmetric cloud of test particles be emitted by an explosion that occurs at some arbitrarily
chosen event in flat spacetime. Make the cloud's density be nonuniform in a certain specific way, so that
every observer moving along with a test particle (called a comoving observer) sees the same local conditions in his own frame;
due to Lorentz contraction by a factor $\gamma$, this requires that the density be proportional to $\gamma$
as described by the observer O who remained at the origin. This scenario turns out to be identical to the Milne
universe under the change of coordinates from spatially flat coordinates
$(T,R)$ to FRW coordinates $(t,r)$, where $t=T/\gamma$ is the proper time
and $r=v\gamma$. (Cf.~problem \ref{hw:nontrivial-curvature-in-one-plus-one}, p.~\pageref{hw:nontrivial-curvature-in-one-plus-one}.)

The Milne universe may be useful as an innoculation against the common misconception that the Big Bang was
an explosion of matter spreading out into a preexisting vacuum. Such a description seems obviously incompatible
with homogeneity, since, for example, an observer at the edge of the cloud sees the cloud filling only half of the sky.
But isn't this a logical contradiction, since the Milne universe \emph{does} have an explosion into vacuum,
and yet it was derived as a special case of the FRW analysis, which explicitly assumed homogeneity?
It is not a contradiction, because a comoving observer never actually sees an edge. In the limit as we approach
the edge, the density of the cloud (as seen by the observer who stayed at the origin) approaches infinity, and
the Lorentz contraction also approaches infinity, so that O considers them to be like Hamlet saying, 
``I could be bounded in a nutshell, and count myself a king of infinite space.'' This logic \emph{only} works
in the case of the Milne universe. The explosion-into-preexisting-vacuum interpretation fails in Big Bang cosmologies
with $\rho \ne 0$.
<% end_sec %>

<% end_sec %> % A singularity at the Big Bang

<% begin_sec("Observability of expansion") %>\label{sec:observability-of-expansion}
<% begin_sec("Brooklyn is not expanding!") %>
The proper interpretation of the expansion of the universe, as described by the Friedmann equations,
can be tricky. The example of the Milne universe encourages us to imagine that the expansion would be undetectable,
since the Milne universe can be described as either expanding or not expanding, depending on the choice of
coordinates. A more general consequence of
coordinate-independence is that relativity does not pick out any preferred distance scale.
That is, if all our meter-sticks expand, and the rest of the universe expands as well, we would
have no way to detect the expansion. The flaw in this reasoning is that the Friedmann equations
only describe the average behavior of spacetime. As dramatized in the classic Woody Allen movie
``Annie Hall:'' ``Well, the universe is everything, and if it's expanding, someday it will break apart and that would be the end of everything!''
``What has the universe got to do with it? You're here in Brooklyn! Brooklyn is not expanding!''

To organize our thoughts, let's consider the following hypotheses:
\begin{enumerate}
\item The distance between one galaxy and another increases at the rate given by $a(t)$ (assuming the galaxies are sufficiently
distant from one another that they are not gravitationally bound within the same galactic cluster, supercluster, etc.).
\item The wavelength of a photon increases according to $a(t)$ as it travels cosmological distances.
\item The size of the solar system increases at this rate as well (i.e., gravitationally bound systems get bigger, including
the earth and the Milky Way).
\item The size of Brooklyn increases at this rate (i.e., electromagnetically bound systems get bigger).
\item The size of a helium nucleus increases at this rate (i.e., systems bound by the strong nuclear force get bigger).
\end{enumerate}

We can imagine that:
\begin{itemize}
\item All the above hypotheses are true.
\item All the above hypotheses are false, and in fact none of these sizes increases at all.
\item Some are true and some false.
\end{itemize}

If all five hypotheses were true, the expansion would be undetectable, because all available meter-sticks would be
expanding together. Likewise if no sizes were increasing, there would be nothing to detect. These two possibilities
are really the same cosmology, described in two different coordinate systems.
But the Ricci and
Einstein tensors were carefully constructed so as to be intrinsic. The fact that the expansion
affects the Einstein tensor shows that it cannot interpreted as a mere coordinate expansion.
Specifically, suppose someone tells you that the FRW metric can be made into a flat metric
by a change of coordinates. (I have come across this claim on internet forums.)
The linear structure of the tensor transformation equations guarantees that a nonzero tensor
can never be made into a zero tensor by a change of coordinates. Since the Einstein tensor
is nonzero for an FRW metric, and zero for a flat metric, the claim is false.

Self-check: The reasoning above implicitly assumed a non-empty universe. Convince yourself that it
fails in the special case of the Milne universe.

We can now see some of the limitations of a common metaphor used to explain cosmic
expansion, in which the universe is visualized as the surface of an expanding balloon. The metaphor
correctly gets across several ideas: that the Big Bang is not an explosion that occurred at a preexisting
point in empty space; that hypothesis 1 above holds; and that the rate of recession of one galaxy
relative to another is proportional to the distance between them. Nevertheless the metaphor may be misleading,
because if we take a laundry marker and draw any structure on the balloon, that structure will expand
at the same rate. But this implies that hypotheses 1-5 all hold, which cannot be true.

Since some of the five hypotheses must be true and some false, and we would like to sort out which are which.
It should also be clear by now that these are not five independent hypotheses. For example, we can
test empirically whether the ratio of Brooklyn's size to the distances between galaxies changes like $a(t)$,
remains constant, or changes with some other time dependence, but it is only the ratio that is
actually observable.

Empirically, we find that hypotheses 1 and 2 are true (i.e., the photon's wavelength maintains a constant
ratio with the intergalactic distance scale), while 3, 4, and 5 are false. For example, the orbits of
the planets in our solar system have been measured extremely accurately by radar reflection and by
signal propagation times to space probes, and no expanding trend is detected.
<% end_sec %> % Brooklyn is not expanding!
<% begin_sec("General-relativistic predictions") %>
Does general relativity correctly reproduce these observations? General relativity is mainly a theory of
gravity, so it should be well within its domain to explain why the solar system does not expand detectably while
intergalactic distances do. It is impractical to solve the Einstein field equations exactly so as to describe the
internal structure of all the bodies that occupy the universe: galaxies, superclusters, etc.
We can, however, handle simple cases, as in example
\ref{eg:sch-de-sitter} on page \pageref{eg:sch-de-sitter}, where we display an exact solution
for the case of a universe containing only two things: an isolated black hole, and an energy density described
by a cosmological constant. We find that
the characteristic scale of the black hole, i.e., the radius of its event horizon, does not increase with time.
A fuller treatment of these issues is given on p.~\pageref{local-expansion}, after some facts about realistic
cosmologies have been established. The result is that although bound systems like the solar system are 
in some cases predicted to
expand, the expansion is absurdly small, too small to measure, and much smaller than the rate of expansion
of the universe in general as represented by the scale factor $a(t)$. This agrees with observation.

It is easy to show that atoms and nuclei do not \emph{steadily} expand over time.
because such an expansion would
violate either the equivalence principle or the basic properties of quantum mechanics. One way of stating
the equivalence principle is that the local geometry of spacetime is always approximately Lorentzian, so that
the laws of physics do not depend on one's position or state of motion. Among these laws of physics are
the principles of quantum mechanics, which imply that an atom or a nucleus has a well-defined ground state,
with a certain size that depends only on fundamental constants such as Planck's constant and the masses
of the particles involved. Atoms and nuclei do experience deformation due to gravitational strains
(examples \ref{eg:leg-bone-expansion}-\ref{eg:nucleus-expansion},
p.~\pageref{eg:leg-bone-expansion}), but these deformations do not increase with time, and would only be
detectable if cosmological expansion were to accelerate radically
(example \ref{eg:big-rip}, p.~\pageref{eg:big-rip}).\label{no-secular-trend-for-atoms}

This is different from the case of a photon traveling across the
universe. The argument given above fails, because the photon does not have a ground state.
The photon \emph{does} expand, and this is required
by the correspondence principle. If the photon did not expand, then its wavelength would
remain constant, and this would be inconsistent with the classical theory of
electromagnetism, which predicts a Doppler shift due to the relative motion of the
source and the observer. One can choose to describe cosmological redshifts either as
Doppler shifts or as expansions of wavelength due to cosmological expansion.

A nice way of discussing atoms, nuclei, photons, and solar systems all on the same footing is to note that
in geometrized units, the units of mass and length are the same. Therefore the existence of
any fundamental massive particle sets a universal length scale, one that will be known to any intelligent species
anywhere in the universe. Since photons are massless, they can't be used to set a universal scale in this
way; a photon has a certain mass-energy, but that mass-energy can take on any value. Similarly, a solar system
sets a length scale, but not a universal one; the radius of a planet's orbit can take on any value.
A universe without massive fundamental particles would be a universe without length measurement.
It would obey the laws of conformal geometry,\index{conformal geometry} in which angles and light-cones were the
only measures. This is the reason that atoms and nuclei, which are made of massive fundamental
particles, do not expand.
<% end_sec %> % General-relativistic predictions
<% begin_sec("More than one dimension required") %>
Another good way of understanding why a photon expands, while an atom does not, is to recall that
a one-dimensional space can never have any intrinsic curvature. If the expansion of atoms were to be
detectable, we would need to detect it by comparing against some other meter-stick. Let's suppose
that a hydrogen atom expands more, while a more tightly bound uranium atom expands less, so that
over time, we can detect a change in the ratio of the two atoms' sizes. The world-lines of the
two atoms are one-dimensional curves in spacetime. They are housed in a laboratory, and although
the laboratory does have some spatial extent, the equivalence principle guarantees that to a good
approximation, this small spatial extent doesn't matter. This implies an intrinsic curvature in a
one-dimensional space, which is mathematically impossible, so we have a proof by contradiction that
atoms do not expand streadily.

Now why does this one-dimensionality argument fail for photons and galaxies? For a pair of galaxies, it
fails because the galaxies are not sufficiently close together to allow them both to be covered
by a single Lorentz frame, and therefore the set of world-lines comprising the observation cannot
be approximated well as lying within a one-dimensional space. Similar reasoning applies for
cosmological redshifts of photons received from distant galaxies. One could instead propose
flying along in a spaceship next to an electromagnetic wave, and monitoring the change in its
wavelength while it is in flight. All the world-lines involved in such an experiment would indeed be
confined to a one-dimensional space. The experiment is impossible, however, because the measuring apparatus
cannot be accelerated to the speed of light. In reality, the speed of the light wave relative to the
measuring apparatus will always equal $c$, so the two world-lines involved in the experiment
will diverge, and will not be confined to a one-dimensional region of spacetime.

\begin{eg}{A cosmic girdle}\label{eg:girdle}
Since cosmic expansion has no significant effect on Brooklyn, nuclei, and solar systems, we might be
tempted to infer that its effect on any solid body would also be negligible. To see that this
is not true, imagine that we live in a closed universe, and the universe has a leather belt wrapping
around it on a closed spacelike geodesic. All parts of the belt are initially at rest relative to the
local galaxies, and the tension is initially zero everywhere. The belt must stretch and eventually
break: for if not, then it could not remain everywhere at rest with respect to the local galaxies, and
this would violate the symmetry of the initial conditions, since there would be no way to pick the
direction in which a certain part of the belt should begin accelerating.
\end{eg}

\begin{eg}{\O{}stvang's quasi-metric relativity}
Over the years, a variety of theories of gravity have been proposed as alternatives to general
relativity. Some of these, such as the Brans-Dicke theory, remain viable, i.e., they are consistent
with all the available experimental data that have been used to test general relativity.
One of the most important reasons for trying to construct such theories is that it can
be impossible to interpret tests of general relativity's predictions unless one also possesses
a theory that predicts something different. This issue, for example, has made it impossible
to test Einstein's century-old prediction that gravitational effects propagate at $c$, since
there is no viable  theory available that predicts any other speed for them (see section \ref{sec:speed-of-gravity}).

\O{}stvang (\url{arxiv.org/abs/gr-qc/0112025v6}) has proposed an alternative theory of
gravity, called quasi-metric relativity, which, unlike general relativity,
predicts a significant cosmological expansion of the solar system,
and which is claimed to be able to explain the observation of small, unexplained accelerations of the Pioneer space
probes that remain after all accelerations due to known effects have been subtracted (the ``Pioneer anomaly'').\index{Pioneer anomaly}
We've seen above that there are a variety of arguments against such an expansion of the solar system,
and that many of these arguments do not require detailed technical calculations but only knowledge of
certain fundamental principles, such as the structure of differential geometry (no intrinsic curvature
in one dimension), the equivalence principle, and the existence of ground states in quantum mechanics.
We therefore expect that \O{}stvang's theory, if it is logically self-consistent, will probably violate
these assumptions, but that the violations must be relatively small if the theory is claimed to be
consistent with existing observations. This is in fact the case. The theory violates the strictest
form of the equivalence principle.

Over the years, a variety of explanations have been proposed for
the Pioneer anomaly, including both glamorous ones (a modification of the $1/r^2$ law of gravitational
forces) and others more pedestrian (effects due to outgassing of fuel, radiation pressure from sunlight, or infrared radiation originating from the
spacecrafts radioisotope thermoelectric generator).
Calculations by Iorio\footnote{\url{http://arxiv.org/abs/0912.2947v1}} in 2006-2009
show that if the force law for gravity is modified in order to explain the Pioneer anomalies, and if
gravity obeys the equivalence principle, then the results are inconsistent with the observed orbital motion
of the satellites of Neptune. This makes gravitational explanations unlikely, but does not obviously rule
out \O{}stvang's theory, since the theory is not supposed to obey the equivalence principle.
\O{}stvang says\footnote{private communication, Jan.~4, 2010} that his theory predicts an expansion of
$\sim 1\munit/\zu{yr}$ in the orbit of Triton's moon Nereid, which is consistent with observation.

In December 2010, the original discoverers of the effect made a statement in the popular press that they
had a new analysis, which they were preparing to publish in a scientific paper, in which the size of
the anomaly would be drastically revised downward, with a far greater proportion of the acceleration
being accounted for by thermal effects. In my opinion this revision, combined with the putative effect's
violation of the equivalence principle, make it clear that the anomaly is not gravitational.
\end{eg}

<% end_sec %> % More than one dimension required
<% begin_sec("Does space expand?") %>
Finally, the balloon metaphor encourages us to interpret cosmological expansion as a phenomenon in which
space itself expands, or perhaps one in which new space is produced. Does space really expand?
Without posing the question in terms of more rigorously defined, empirically observable quantities,
we can't say yes or no. It is merely a matter of which definitions one chooses and which conceptual framework
one finds easier and more natural to work within. Bunn and Hogg have stated the minority view
against expansion of space\footnote{\url{http://arxiv.org/abs/0808.1081v2}}, while the opposite
opinion is given by Francis et al.\footnote{\url{http://arxiv.org/abs/0707.0380v1}}

As an example of a self-consistent set of definitions that lead to the conclusion that space
does expand, Francis et al.~give the following. Define eight observers positioned at the corners
of a cube, at cosmological distances from one another. Let each observer be at rest relative to the
local matter and radiation that were used as ingredients in the FRW cosmology. (For example, we know that our own
solar system is \emph{not} at rest in this sense, because we observe that the cosmic microwave
background radiation is slightly Doppler shifted in our frame of reference.) Then these eight observers
will observe that, over time, the volume of the cube grows as expected according to the cube of the function $a(t)$ in the FRW model.

This establishes that expansion of space is a plausible interpretation. To see that it is not the only\label{split-kin-grav}
possible interpretation, consider the following example. A photon is observed after having traveled
to earth from a distant galaxy G, and is found to be red-shifted. Alice, who likes expansion, will
explain this by saying that while the photon was in flight, the space it occupied expanded, lengthening its
wavelength. Betty, who dislikes expansion, wants to interpret it as a kinematic red shift, arising from
the motion of galaxy G relative to the Milky Way Malaxy, M. If Alice and Betty's disagreement
is to be decided as a matter of absolute truth, then we need some objective method for resolving an
observed redshift into two terms, one kinematic and one gravitational. But we've seen in section \ref{sec:static-and-stationary}
on page \pageref{sec:static-and-stationary} that this is only possible for a stationary spacetime, and
cosmological spacetimes are not stationary: regardless of an observer's state of motion, he sees a change
over time in observables such as density of matter and curvature of spacetime. As an extreme example,
suppose that Betty, in galaxy M, receives a photon without realizing that she lives in a closed universe,
and the photon has made a circuit of the cosmos, having been emitted from her own galaxy in the distant past.
If she insists on interpreting this as a kinematic red shift, the she must conclude that her galaxy M is moving
at some extremely high velocity relative to itself. This is in fact not an impossible interpretation, if we
say that M's high velocity is relative to itself \emph{in the past}. An observer who sets up a frame of reference
with its origin fixed at galaxy G will happily confirm that M has been accelerating over the eons. What this
demonstrates is that we can split up a cosmological red shift into kinematic and gravitational parts in any way
we like, depending on our choice of coordinate system (see also p.~\pageref{potential-cosm}).\index{red-shift!cosmological!kinematic versus gravitational}

\begin{eg}{A cosmic whip}\label{eg:whip}
The cosmic girdle of example \ref{eg:girdle} on p.~\pageref{eg:girdle} does not transmit any information from one part of the universe to another, for
its state is the same everywhere by symmetry, and therefore an observer near one part of the belt
gets no information that is any different from what would be available to an observer anywhere else.

Now suppose that the universe is open rather than closed, but we have a rope that, just like the belt, stretches out over
cosmic distances along a spacelike geodesic. If the rope is initially at rest with respect to
a particular galaxy G (or, more strictly speaking, with respect to the locally averaged cosmic medium),
then by symmetry the rope will always remain at rest with respect to G,
since there is no way for the laws of physics to pick a direction in which it should accelerate.
Now the residents of G cut the rope, release half of it, and tie the other half securely
to one of G's spiral arms using a square knot. If they do this smoothly, without
varying the rope's tension, then no vibrations will propagate, and everything will
be as it was before on that half of the rope. (We assume that G is so massive relative
to the rope that the rope does not cause it to accelerate significantly.)

Can observers at distant points observe the tail of the rope whipping by at a certain speed, and thereby
infer the velocity of G relative to them? This would produce all kinds of strange conclusions.
For one thing, the Hubble law says that this velocity is directly proportional to the length
of the rope, so by making the rope long enough we could make this velocity exceed the speed of light.
We've also convinced ourselves that the relative velocity of cosmologically distant objects is not even
well defined in general relativity, so it clearly can't make sense to interpret the rope-end's
velocity in that way.

The way out of the paradox is to recognize that disturbances can only propagate along the rope at a certain
speed $v$. Let's say that the information is transmitted in the form of longitudinal vibrations, in which case
it propagates at the speed of sound. For a rope made out of any known material, this is far less than
the speed of light, and we've also seen in example \ref{eg:newtonian-black-hole} on page \pageref{eg:newtonian-black-hole}
and in problem \ref{hw:speed-of-sound} on page \pageref{hw:speed-of-sound} that relativity places fundamental limits
on the properties of all possible materials, guaranteeing $v<c$. We can now see that all we've accomplished with the rope is to
recapitulate using slower sound waves the discussion that was carried out on page \pageref{split-kin-grav}
using light waves. The sound waves may perhaps preserve some information about the state of motion
of galaxy G long ago, but all the same ambiguities apply to its interpretation as in the case of
light waves --- and in addition, we suspect that the rope has long since parted somewhere along its length.
\end{eg}
<% end_sec %> % Does space expand?
<% end_sec %> % Observability of expansion

<% begin_sec("The vacuum-dominated solution") %>\label{sec:vacuum-dominated}
For 70 years after Hubble's discovery of cosmological expansion, the standard picture was one in
which the universe expanded, but the expansion must be decelerating. The deceleration is predicted
by the special cases of the FRW cosmology that were believed to be applicable, and even if we didn't
know anything about general relativity, it would be reasonable to expect a deceleration due to the
mutual Newtonian gravitational attraction of all the mass in the universe.

But observations of distant supernovae starting around 1998 introduced a further twist in the plot.
In a binary star system consisting of a white dwarf and a non-degenerate star, as the non-degenerate
star evolves into a red giant, its size increases, and it can begin dumping mass onto the white dwarf.
This can cause the white dwarf to exceed the Chandrasekhar limit (page \pageref{chandrasekhar-limit}),
resulting in an explosion known as a type Ia supernova. Because the Chandrasekhar limit provides a uniform
set of initial conditions, the behavior of type Ia supernovae is fairly predictable, and in particular
their luminosities are approximately equal. They therefore provide a kind of standard candle: since
the intrinsic brightness is known, the distance can be inferred from the apparent brightness. Given the
distance, we can infer the time that was spent in transit by the light on its way to us, i.e. the
look-back time. From measurements of Doppler shifts of spectral lines, we can also find the velocity
at which the supernova was receding from us. The result is that we can measure the universe's
rate of expansion as a function of time. Observations show that this rate of expansion has been
accelerating. The Friedmann equations show that this can only occur for $\Lambda \gtrsim 4\rho$.
This picture has been independently verified by measurements of the cosmic microwave background (CMB)
radiation.\index{cosmic microwave background} A more detailed discussion of the supernova and
CMB data is given in section \ref{sec:cosmology-observation} on page \pageref{sec:cosmology-observation}.

With hindsight, we can see that
in a quantum-mechanical context, it is natural to expect that fluctuations of the vacuum, required by the Heisenberg
uncertainty principle, would contribute to the cosmological constant, and in fact models tend to overpredict $\Lambda$
by a factor of about $10^{120}$! From this point of view, the mystery is why these effects cancel out so precisely.
A correct understanding of the cosmological constant presumably requires a full theory
of quantum gravity, which is presently far out of our reach.

The latest data show that our universe, in the present epoch, is dominated by the cosmological constant,
so as an approximation we can write the Friedmann equations as
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &= \frac{1}{3}\Lambda  \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{1}{3}\Lambda\eqquad.
\end{align*}
This is referred to as a vacuum-dominated universe.
The solution is
\begin{equation*}
  a = \exp\left[\sqrt{\frac{\Lambda}{3}} \: t\right]\eqquad,
\end{equation*}
where observations show that $\Lambda\sim 10^{-26}\ \kgunit/\munit^3$, giving $\sqrt{3/\Lambda}\sim 10^{11}$ years.

The implications for the fate of the universe are depressing. All parts of the universe will accelerate
away from one another faster and faster as time goes on. The relative separation between two objects, say galaxy A
and galaxy B, will eventually be increasing faster than the speed of light. (The Lorentzian character of spacetime
is local, so relative motion faster than $c$ is only forbidden between objects that are passing right by one another.)
At this point, an observer in either galaxy will say that the other one has passed behind an event horizon.
If intelligent observers do actually exist in the far future, they may have no way to tell that the cosmos even exists.
They will perceive themselves as living in island universes, such as we believed our own galaxy to be a hundred years ago.

When I introduced the standard cosmological coordinates on page \pageref{standard-cosm-coords}, I described them
as coordinates in which events that are simultaneous according to this $t$ are events at which the local properties of the universe are the same.
In the case of a perfectly vacuum-dominated universe, however, this notion loses its meaning. The only observable
local property of such a universe is the vacuum energy described by the cosmological constant, and its density is
always the same, because it is built into the structure of the vacuum. Thus the vacuum-dominated cosmology is
a special one that maximally symmetric, in the sense that it has not only the symmetries of homogeneity and
isotropy that we've been assuming all along, but also a symmetry with respect to time: it is a cosmology without
history, in which all times appear identical to a local observer. In the special case of this cosmology, the
time variation of the scaling factor $a(t)$ is unobservable, and may be thought of as the unfortunate result
of choosing an inappropriate set of coordinates, which obscure the underlying symmetry. When I argued in section
\ref{sec:observability-of-expansion} for the observability of the universe's expansion, note that all my
arguments assumed the presence of matter or radiation. These are completely absent in a perfectly
vacuum-dominated cosmology.

For these reasons de Sitter originally proposed this solution as a static universe in 1927. But by 1920
it was realized that this was an oversimplification. The argument above only shows that the time variation
of $a(t)$ does not allow us to distinguish one epoch of the universe from another. That is, we can't
look out the window and infer the date (e.g., from the temperature of the cosmic microwave background
radiation). It does not, however, imply that the universe is static in the
sense that had been assumed until Hubble's observations.
The $r$-$t$ part of the metric is
\begin{equation*}
  \der s^2 = \der t^2 - a^2 \der r^2\eqquad,
\end{equation*}
where $a$ blows up exponentially with time, and the $k$-dependence has been
neglected, as it was in the approximation to the Friedmann equations
used to derive $a(t)$.\footnote{A computation of the Einstein tensor with $\der s^2 = \der t^2 - a^2(1-kr^2)^{-1} \der r^2$ shows
that $k$ enters only via a factor the form $(\ldots)e^{(\ldots)t}+(\ldots)k$. For large $t$, the $k$ term
becomes negligible, and the Einstein tensor becomes $G\indices{^a_b}=g\indices{^a_b}\Lambda$,
This is consistent with the approximation we used in deriving the solution, which was to
ignore both the source terms and the $k$ term in the Friedmann equations. The exact solutions with $\Lambda>0$ and
$k=-1$, 0, and 1 turn out in fact to be equivalent except for a change of coordinates.}
Let a test particle travel in the radial direction, starting at event $\zu{A}=(0,0)$ and ending at
$\zu{B}=(t',r')$. In flat space, a world-line of the linear form $r=vt$ would be a geodesic connecting
A and B; it would maximize the particle's proper time.
But in the this metric, it cannot be a geodesic. The curvature of geodesics relative to a line on
an $r$-$t$ plot is most easily understood in the limit where $t'$ is fairly long compared to the
time-scale $T=\sqrt{3/\Lambda}$ of the exponential, so that $a(t')$ is huge. The particle's best strategy for maximizing
its proper time is to make sure that its $\der r$ is extremely small when $a$ is extremely large.
The geodesic must therefore have nearly constant $r$ at the end. This makes it sound as though the
particle was decelerating, but in fact the opposite is true. If $r$ is constant, then the particle's
spacelike distance from the origin is just $r a(t)$, which blows up exponentially. The near-constancy of
the coordinate $r$ at large $t$ actually means that the particle's motion at large $t$ isn't really
due to the particle's inertial memory of its original motion, as in Newton's first law. What happens
instead is that the particle's initial motion allows it to move some distance away from the origin during a
time on the order of $T$, but after that, the expansion of the universe has become so rapid that the
particle's motion simply streams outward because of the expansion of space itself. Its initial motion only
mattered because it determined how far out the particle got before being swept away by the exponential expansion.

\begin{eg}{Geodesics in a vacuum-dominated universe}\label{eg:geodesic-vacuum-dominated}
In this example we confirm the above interpretation in the special case where the particle, rather
than being released in motion at the origin, is released at some nonzero radius $r$, with $\der r/\der t=0$ initially.
First we recall the geodesic equation
\begin{equation*}
  \frac{\der^2 x^i}{\der\lambda^2} = \Gamma\indices{^i_{jk}} \frac{\der x^j}{\der\lambda} \frac{\der x^k}{\der\lambda}\eqquad.
\end{equation*}
from page \pageref{geodesic-diffeq}. The nonvanishing Christoffel symbols for the 1+1-dimensional metric 
$\der s^2 = \der t^2 - a^2 \der r^2$ are $\Gamma\indices{^r_{tr}}=\dot{a}/a$ and $\Gamma\indices{^t_{rr}}=\dot{a}a$.
Setting $T=1$ for convenience, we have $\Gamma\indices{^r_{tr}}=1$ and $\Gamma\indices{^t_{rr}}=e^{-2t}$.
% geodesic_vacuum_dominated.mac

We conjecture that the particle remains at the same value of $r$. Given this conjecture, the particle's
proper time $\int \der s$ is simply the same as its time coordinate $t$, and we can therefore use $t$ as
an affine coordinate.
Letting $\lambda=t$, we have
\begin{align*}
  & \frac{\der^2 t}{\der t^2}-\Gamma\indices{^t_{rr}}\left(\frac{\der r}{\der t}\right)^2 = 0\\
  &0-\Gamma\indices{^t_{rr}}\dot{r}^2 = 0 \\
  & \dot{r} = 0 \\
  & r = \text{constant}
\end{align*}
This confirms the self-consistency of the conjecture that $r=\text{constant}$ is a geodesic.

Note that we never actually had to use the actual expressions for the Christoffel symbols; we only needed to know
which of them vanished and which didn't. The conclusion
depended only on the fact that the metric had the form $\der s^2 = \der t^2 - a^2 \der r^2$ for some function
$a(t)$. This provides a rigorous justification for the interpretation of the cosmological scale factor $a$
as giving a universal time-variation on all distance scales.

The calculation also confirms that there is nothing special about $r=0$. A particle released with $r=0$ and $\dot{r}=0$
initially stays at $r=0$, but a particle released at any other value of $r$ also stays at that $r$.
This cosmology is homogeneous, so any point could have been
chosen as $r=0$. If we sprinkle test particles, all at rest, across the surface of a sphere centered on this arbitrarily
chosen point, then they will all accelerate outward \emph{relative to one another}, and the volume of the sphere will increase. This is exactly what
we expect. The Ricci curvature is interpreted as the second derivative of the volume of a region of space defined by
test particles in this way. The fact that the second derivative is positive rather than negative tells us that we
are observing the kind of repulsion provided by the cosmological constant, not the attraction that results from the
existence of material sources.
\end{eg}


\begin{eg}{Schwarzschild-de Sitter space}\label{eg:sch-de-sitter}

% http://en.wikipedia.org/wiki/De_Sitter%E2%80%93Schwarzschild

The metric
\begin{equation*}
  \der s^2 = \left(1-\frac{2m}{r}-\frac{1}{3}\Lambda r^2\right)\der t^2 - \frac{\der r^2}{1-\frac{2m}{r}-\frac{1}{3}\Lambda r^2} - r^2\der\theta^2-r^2\sin^2\theta\der\phi^2
\end{equation*}
is an exact solution to the Einstein field equations with cosmological constant $\Lambda$, and can be interpreted as a universe in which the
only mass is a black hole of mass $m$ located at $r=0$. Near the black hole, the $\Lambda$ terms become negligible, and this is simply
the Schwarzschild metric.
As argued in section \ref{sec:observability-of-expansion}, page \pageref{sec:observability-of-expansion},
this is a simple example of how cosmological expansion does not cause all structures in the universe to grow at the same rate.
% sch_de_sitter.mac
\end{eg}

\begin{eg}{Conservation of energy-momentum}\label{eg:de-sitter-cons}
Suppose that we assume the de Sitter geometry, and ask what type of matter fields are necessary to create it.
We know that a cosmological constant will do the job, but could we have some other matter field that
would also work? Suppose that the matter field is constrained to be a perfect fluid.
The total stress-energy is then of the form $T^\mu_\nu=\operatorname{diag}(\rho,-P,-P,-P)$
in Cartesian coordinates. (See
example \ref{eg:perfect-fluid-tensorial} on p.~\pageref{eg:perfect-fluid-tensorial}
for the signs,
some of which depend on our use of the $+---$ signature.)
The divergence
$\nabla_\mu T^\mu_t$ measures the rate at which an observer says energy is being created, and we need this to be zero.
This expression is one of those tricky examples where the covariant derivative can be nonzero even when the
thing being differentiated vanishes identically.
The divergence is
$\nabla_tT\indices{^t_t}+\nabla_xT\indices{^x_t}$, and the term that doesn't vanish is the
\emph{second} one, even though $T\indices{^x_t}=0$.
Using the nonvanishing Christoffel symbols this becomes
$\Gamma^x_{xt}T\indices{^t_t}-\Gamma^x_{tx}T\indices{^x_x}=\frac{\dot{a}}{a}(\rho+P)$,
so that $\rho+P=0$. This condition is satisfied by a cosmological constant. Our result is that
the only way to get a de Sitter geometry is with matter fields that exactly mimic a cosmological
constant. This is of some historical interest in the context of the steady-state cosmologies,
section \ref{sec:steady-state}, p.~\pageref{sec:steady-state}. It may seem mysterious that
we have obtained this result by requiring conservation of energy-momentum, but we could also
have done it using the Einstein field equations. In fact these are not two separate requirements,
since the field equations require conservation of energy-momentum in order to be consistent.
\end{eg}

<% begin_sec("The Big Bang singularity in a universe with a cosmological constant") %>\label{bb-singularity-with-lambda}
On page \pageref{bb-singularity-without-lambda} we discussed the possibility that the Big Bang singularity was an artifact of the
unrealistically perfect symmetry assumed by our cosmological models, and we found that this was not the case: the Penrose-Hawking singularity theorems
demonstrate that the singularity is real, provided that the cosmological constant is zero. The cosmological constant is \emph{not}
zero, however. Models with a very large positive cosmological constant can also display a Big Bounce rather than a Big Bang. If we imagine using the Friedmann
equations to evolve the universe backward in time from its present state, the scaling arguments of  example \ref{eg:dust-and-radiation-cosm}
on page \pageref{eg:dust-and-radiation-cosm}
suggest that at early enough times, radiation and matter should dominate over the cosmological constant.
For a large enough value of the cosmological constant, however, it can happen that this switch-over never happens.
In such a model, the universe is and always has been dominated by the cosmological constant, and we get
a Big Bounce in the past because of the cosmological constant's repulsion. In this book I will only develop simple
cosmological models in which the universe is dominated by a single component; for a discussion of bouncing models
with both matter and a cosmological constant, see Carroll, ``The Cosmological Constant,'' \url{http://www.livingreviews.org/lrr-2001-1}.
By 2008, a variety of observational data had pinned down the cosmological constant well enough to rule out the
possibility of a bounce caused by a very strong cosmological constant.
<% end_sec %>

<% end_sec %> % The vacuum-dominated solution

<% begin_sec("The matter-dominated solution") %>\label{sec:matter-dominated}
Our universe is not perfectly vacuum-dominated, and in the past it was even less so. Let us consider the matter-dominated
epoch, in which the cosmological constant was negligible compared to the material sources. 
The equation of state
for nonrelativistic matter (p. \pageref{eg:dust-and-radiation-cosm}) is
\begin{equation*}
 P=0\eqquad.
\end{equation*}
The dilution of the dust with cosmological expansion gives
\begin{equation*}
 \rho \propto a^{-3}
\end{equation*}
(see example \ref{eg:dust-dilution}). The Friedmann equations become
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &=  - \frac{4\pi}{3}\rho \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{8\pi}{3}\rho-k a^{-2}\eqquad,
\end{align*}
where for compactness $\rho$'s dependence on $a$, with some constant of proportionality, is not shown explicitly.
A static solution, with constant $a$, is impossible, and $\ddot{a}$ is negative, which we can interpret in Newtonian
terms as the deceleration of the matter in the universe due to gravitational attraction. There are three cases
to consider, according to the value of $k$.

<% begin_sec("The closed universe") %>
We've seen that $k=+1$ describes a universe in which the spatial curvature is positive, i.e., the circumference
of a circle is less than its Euclidean value. By analogy with a sphere, which is the two-dimensional surface of constant positive curvature,
we expect that the total volume of this universe is finite.

The second Friedmann equation also shows us that at some value of
$a$, we will have $\dot{a}=0$. The universe will expand, stop, and then recollapse, eventually coming back together
in a ``Big Crunch'' which is the time-reversed version of the Big Bang. 

Suppose we were to describe an initial-value problem in this cosmology, in which the initial conditions are
given for all points in the universe on some spacelike surface, say $t=\text{constant}$. Since the universe
is assumed to be homogeneous at all times, there are really only three numbers to specify, $a$, $\dot{a}$, and $\rho$:
how big is the universe, how fast is it expanding, and how much matter is in it?
But these three pieces of data may or may not be consistent with the second Friedmann equation. That is, the
problem is overdetermined. In particular, we can see that for small enough values of $\rho$, we do not have
a valid solution, since the square of $\dot{a}/a$ would have to be negative. Thus a closed universe requires
a certain amount of matter in it. The present observational evidence (from supernovae and the cosmic microwave
background, as described above) is sufficient to show that our universe does not contain this much matter.
<% end_sec %> % The closed universe

<% begin_sec("The flat universe") %>\label{flat-dust}
The case of $k=0$ describes a universe that is spatially flat. It represents a knife-edge case lying between the
closed and open universes. In a Newtonian analogy, it represents the case in which the universe is moving
exactly at escape velocity; as $t$ approaches infinity, we have $a \rightarrow\infty$, $\rho\rightarrow 0$, and
$\dot{a}\rightarrow 0$. This case, unlike the others, allows an easy closed-form solution to the motion.
Let the constant of proportionality in the equation of state $\rho \propto a^{-3}$ be fixed by setting
$-4\pi\rho/3=-ca^{-3}$. The Friedmann equations are
\begin{align*}
  \ddot{a} &= -ca^{-2} \\
  \dot{a} &= \sqrt{2c} a^{-1/2}\eqquad.
\end{align*}
Looking for a solution of the form $a\propto t^p$, we find that by choosing $p=2/3$ we can simultaneously
satisfy both equations. The constant $c$ is also fixed, and we can investigate this most transparently
by recognizing that $\dot{a}/a$ is interpreted as the Hubble constant, $H$,\index{Hubble constant}
which is the constant of proportionality relating a far-off galaxy's velocity to its distance.
Note that $H$ is a ``constant'' in the sense that it is the same for all galaxies, in this particular model
with a vanishing cosmological constant; it does not stay
constant with the passage of cosmological time.
Plugging back into the original form of the Friedmann equations, we find that the flat universe can
only exist if the density of matter satisfies $\rho=\rho_{crit}=3H^2/8\pi=3H^2/8\pi G$.
The observed value of the Hubble constant is about $1/(14\times10^9\ \text{years})$, which is roughly
interpreted as the age of the universe, i.e., the proper time experienced by a test particle since
the Big Bang. This gives $\rho_{crit}\sim 10^{-26}\ \kgunit/\munit^3$.

% G=6.7 10^-11 m3kg-1s-2
% t=14 10^9 * 365.25 *24 * (3600 s)
% H=1/t
%  2H^2/(8piG)
%    6.08487033002423*10^-27 kg/m3

As discussed in subsection \ref{sec:cosmology-observation}, our universe turns out to be almost exactly
spatially flat. Although it is presently vacuum-dominated, the flat and matter-dominated FRW cosmology
is a useful description of its matter-dominated era.



<% end_sec %> % The flat universe


<% begin_sec("The open universe") %>
The $k=-1$ case represents a universe that has negative spatial curvature, is spatially infinite, and is also infinite in time,
i.e., even if the cosmological constant had been zero, the expansion of the universe would have had too little matter in it
to cause it to recontract and end in a Big Crunch.

The time-reversal symmetry of general relativity was discussed on p.~\pageref{sec:sch-t-reversal} in connection with the
Schwarzschild metric.\footnote{Problem \ref{hw:frw-time-reversal} on p.~\pageref{hw:frw-time-reversal} shows that this symmetry is also exhibited by the
Friedmann equations.} Because of this symmetry, we expect that solutions to the field equations will be symmetric under
time reversal (unless asymmetric boundary conditions were imposed). The closed universe has exactly this type of
time-reversal symmetry. But the open universe clearly breaks this symmetry, and this is why we speak of the Big Bang
as lying in the past, not in the future. This is an example of 
spontaneous symmetry breaking.\label{spontaneous-symm-breaking}\index{spontaneous symmetry breaking}\index{symmetry breaking!spontaneous}
Spontaneous symmetry breaking happens when we try to balance a pencil on its tip, and it is also an important phenomenon in particle physics.
The time-reversed version of the open universe is an equally valid solution of the field equations. Another example of spontaneous symmetry breaking
in cosmological solutions is that the solutions have a preferred frame of reference, which is the one at rest relative to the cosmic microwave
background and the average motion of the galaxies. This is referred to as the Hubble flow.\index{Hubble flow}


<% end_sec %> % The open universe

\begin{eg}{Size and age of the observable universe}\label{eg:observable-universe}\index{universe!observable}\index{universe!observable!size and age}
\index{observable universe}\index{observable universe!size and age}
The observable universe is defined by the region from which light has had time to reach us since the Big Bang.
Many people are inclined to assume that its radius in units of light-years must therefore be equal to the
age of the universe expressed in years. This is not true. Cosmological distances like these are not even
uniquely defined, because general relativity only has local frames of reference, not global ones.

Suppose we adopt the proper distance $L$ defined on p.~\pageref{proper-distance-cosmologically} as our measure
of radius. By this measure, realistic cosmological models  say that
our 14-billion-year-old universe has a radius of 46 billion light years.

For a flat universe, $f=1$, so by inspecting the FRW metric we find that a photon moving radially with $\der s=0$ has
$|\der r/\der t|=a^{-1}$, giving $r=\pm\int_{t_1}^{t_2} \der t/a$.
Suppressing signs, the proper distance the photon traverses starting
soon after the Big Bang is $L=a(t_2)\int d\ell=a(t_2)\int \der r=a(t_2)r=a(t_2)\int_{t_1}^{t_2} \der t/a$.

In the matter-dominated case, $a \propto t^{2/3}$, so this results in
$L=3t_2$ in the limit where $t_1$ is small. Our universe
has spent most of its history being matter-dominated, so it's
encouraging that the matter-dominated calculation seems to do a
pretty good job of reproducing the actual ratio of 46/14=3.3 between
$L$ and $t_2$.

While we're at it, we can see what happens in the purely vacuum-dominated case, which has
$a\propto e^{t/T}$, where $T=\sqrt{3/\Lambda}$. This cosmology doesn't have a Big Bang, but we can think of it as
an approximation to the more recent history of the universe, glued on to an earlier matter-dominated
solution. Here we find $L=\left[e^{(t_2-t_1)/T}-1\right]T$, where $t_1$ is the time when the switch
to vacuum-domination happened. This function grows more quickly with $t_2$ than the one obtained in the
matter-dominated case, so it makes sense that the real-world ratio of $L/t_2$ is somewhat greater than
the matter-dominated value of 3.

The radiation-dominated version is handled in problem
\ref{hw:observable-universe-perfect-fluid} on p.~\pageref{hw:observable-universe-perfect-fluid}.
\end{eg}

\begin{eg}{Local conservation of mass-energy}\label{eg:dust-dilution}
Any solution to the Friedmann equations is a solution of the field equations, and therefore locally
conserves mass-energy. We saved work above by applying this condition in advance in the form $\rho\propto a^{-3}$
to make the dust dilute itself properly with cosmological expansion. In this example we prove the same proportionality
by explicit calculation.

Local conservation of mass-energy is expressed by the zero divergence of the stress-energy tensor,
$\nabla_jT^{jb}=0$. The definition of the covariant derivative gives
\begin{equation*}
  \nabla_j T^{bc} = \partial_j T^{bc}+\Gamma^b_{jd}T^{dc}+\Gamma^c_{jd}T^{bd}\eqquad.
\end{equation*}
For convenience, we carry out the calculation at $r=0$; if conservation holds here, then it holds everywhere
by homogeneity.

In a local Cartesian frame $(t',x',y',z')$ at rest relative to the dust, the stress-energy tensor is diagonal
with $T^{t't'}=\rho$. At $r=0$, the transformation from FLRW coordinates into these coordinates doesn't
mix $t$ or $t'$ with the other coordinates, so by the tensor transformation law we still have $T^{tt}=\rho$. 

There are a number of Christoffel symbols involved, but the only three of relevance that don't vanish at
$r=0$ turn out to be $\Gamma^r_{rt}=\Gamma^\theta_{\theta t}=\Gamma^\phi_{\phi t}=\dot{a}/a$. The result is
\begin{equation*}
  \nabla_\mu T^{t\mu} = \partial_tT^{tt}+3\frac{\dot{a}}{a} T^{tt}\eqquad,
\end{equation*}
or $\dot{\rho}/\rho=-3\dot{a}/a$, which can be rewritten as
\begin{equation*}
  \frac{\der}{\der t} \ln\rho = -3 \frac{\der}{\der t} \ln a\eqquad,
\end{equation*}
producing the proportionality originally claimed.
\end{eg}

<% end_sec %> % The matter-dominated solution

<% begin_sec("The radiation-dominated solution") %>
For the reasons discussed in example \ref{eg:dust-and-radiation-cosm} on page \pageref{eg:dust-and-radiation-cosm},
the early universe was dominated by radiation. The solution of the Friedmann equations for this case is taken up
in problem \ref{hw:flat-perfect-fluid-cosmology} on page  \pageref{hw:flat-perfect-fluid-cosmology}.
<% end_sec %> % The radiation-dominated solution

<% begin_sec("Local effects of expansion") %>\label{local-expansion}

In this section we discuss the predictions of general relativity concerning the effect of cosmological
expansion on small, gravitationally bound systems such as the solar system or clusters of galaxies.
The short answer is that in most realistic cosmologies (but not necessarily in
``Big Rip'' scenarios, p.~\pageref{eg:big-rip})\index{Big Rip} the effect of expansion
is not zero, but is many orders of magnitude too small
to measure. Many readers will probably be willing to accept these assertions while skipping the following demonstrations.

To begin with, we observe that there are two qualitatively distinct types of effects that could exist.
Suppose that a loaf of raisin bread is rising. Let's say that the loaf's scale factor $a$ doubles
by the time the yeast's efforts are spent.
By definition, this means that the raisins (galaxies, test particles) get farther apart by a factor of 2.
We could imagine that in addition: (1) the strain of expansion could cause each raisin to puff
up by, say, 1\%,
and to maintain this increased size
over the entire course of expansion; or that (2) expansion could could cause each raisin to expand
gradually, to 0.2\% more than its original size, then 0.4\% more than its original size, and so on,
until, at the end of the process, each had grown beyond its original size by some amount such as 3.8\%,
which, while less than the 100\%
growth of the inter-raisin distances, was nevertheless nonzero. Astronomers refer to the second possibility
as a ``secular'' trend. For example, simulations of solar systems often show that over billions of years,
planets gradually migrate either inward or outward,
under the influence of their gravitational interactions with other planets. As an example of an expansion without
a secular trend, asteroids may experience a nonnegligible $1/r^2$ force due to radiation pressure from the sun.
The effect is exactly as if the sun's mass or the gravitational constant had been slightly reduced.
Kepler's elliptical orbit law holds, the law of periods is slightly off, and the orbital radius
shows zero trend over time.

If either type of effect exists, an observer in some local inertial frame will interpret it as
a ``force.'' (The scare quotes are a reminder that general relativity doesn't describe gravity as
Newton-style linearly additive, instantaneous action at a distance.) 
Such a force, if it exists, cannot simply be proportional
to the rate of expansion $\dot{a}/a$. As a counterexample, the Milne universe
is just flat spacetime described in silly coordinates, and it has $\dot{a}\ne 0$.

It would make more sense for the force to depend on the second derivative of the scale factor.
To justify this more precisely, imagine releasing two test particles, initially separated by
some distance that is much less than the Hubble scale. They are initially at rest relative to
the Hubble flow, and no locally gravitating bodies are present.
As discussed in example \ref{eg:cosmo-hole} on p.~\pageref{eg:cosmo-hole},
the acceleration of one test particle relative to
the other is given by $(\ddot{a}/a)\vc{r}$, where $\vc{r}$ is their relative displacement.

Thus if we are to observe any nonzero effects of expansion on a local system, they are not
really effects of expansion at all, but effects of the \emph{acceleration} of expansion.
The factor $\ddot{a}/a$ is on the order of the inverse square of the age
of the universe, i.e., $H_\zu{o}^2\sim 10^{-35}\ \sunit^{-2}$. The smallness of this factor is what
makes the effect on a system such as the solar system so absurdly tiny. 


\begin{eg}{A human body}\label{eg:leg-bone-expansion}
Let's estimate the effect of cosmological expansion on the length $L$ of your thigh bone.
The body is made of atoms, and for the reasons given on p.~\pageref{no-secular-trend-for-atoms},
there can be no steady trend in the sizes of these atoms or the lengths of the chemical bonds
between them. The bone experiences a stress due to cosmological expansion, but it is in equilibrium,
and the strain will disappear if the gravitational stress is removed (e.g., if other gravitational stresses are
superimposed on top of the cosmological one in order to cancel it).
The anomalous acceleration between the ends of the bone is $(\ddot{a}/a)L$, which is observed as an anomalous stress.
Taking $\ddot{a}/a\sim H_\zu{o}^2$, the anomalous acceleration
of one end of the bone relative to the other is $\sim LH^2$. The corresponding compression or
tension is $\sim mLH^2$, where $m$ is your body mass. The resulting strain is
$\epsilon \sim mLH^2/AE$,
where $E$ is the Young's modulus of bone (about $10^{10}$ Pa) and $A$ is the bone's cross-sectional area.

Putting in numbers, the result for the strain is about $10^{-40}$, which is much too small to be
measurable by any imaginable technique, and would in reality be swamped by other effects. 
Since the sign of $\ddot{a}$ is currently positive, this strain is tensile, not compressive.
In the earlier, matter-dominated era of the universe, it would have been compressive.

There is no ``secular trend,'' i.e., your leg bone is not expanding over time. It's in equilibrium, and
is simply elongated imperceptibly compared to the length if would have had without the
effect of cosmological expanson. 
\end{eg}

\begin{eg}{Strain on an atomic nucleus}\label{eg:nucleus-expansion}
The estimate in example \ref{eg:leg-bone-expansion} can also be applied to an atomic nucleus, which has
a nuclear ``Young's modulus'' on the order of $1\ \textup{MeV}/\zu{fm}^3\sim 10^{32}\ \zu{Pa}$.
The result is a strain $\epsilon\sim 10^{-52}$.
\end{eg}

\begin{eg}{A Big Rip}\label{eg:big-rip}\index{Big Rip}
Known forms of matter are believed to have equations of state $P=w\rho$ with $w \ge -1$.
The value for a vacuum-dominated universe would be $w=-1$.
Cosmological observations\footnote{Carnero et al., \url{arxiv.org/abs/1104.5426}}
show that empirically the present-day universe behaves as if it is made out of stuff with $w=-1.03\pm.16$,
and this leaves open the possibility of $w<-1$. In this case, the solution to the Friedmann equations
gives a scale factor $a(t)$ that blows up to infinity at some finite $t$. In such a scenario,
known as a ``Big Rip,''
$(d/dt)(\ddot{a}/a)$ diverges, and any system, no matter how tightly bound, is 
ripped apart.\footnote{Caldwell et al., \url{arxiv.org/abs/astro-ph/0302506}}
\end{eg}

\enlargethispage{-\baselineskip}

Examples \ref{eg:leg-bone-expansion}-\ref{eg:big-rip} show that except under hypothetical extreme cosmological
conditions, there is no hope of detecting any effect of cosmological expansion on systems made of
condensed matter. We need to look at much larger systems to see any effect, and such systems are
held together by gravity.
For concreteness, let's keep talking about the earth-sun system.
Not only is the anomalous force on the earth small, it is not guaranteed to produce any secular trend,
which is what would be most likely to be detectable.
The direction of the anomalous force on the earth
is outward for an accelerating cosmological expansion, as we now know is the case for the present epoch.
As an example in which no secular trend occurs, a vacuum-dominated cosmology gives
a constant value for $\ddot{a}/a$, so the outward force is constant. As with the effect of radiation
pressure, the existence of this constant, outward force
is very nearly equivalent to rescaling the sun's gravitational force by a tiny amount, so the motion is still very nearly
Keplerian, but with a slightly ``wrong'' constant of proportionality in Kepler's law of periods.
The rate of change $\dot{r}$ in the radius of the circular orbit is therefore zero in this case.

But in most cosmologies $\ddot{a}/a$ is not exactly constant, and the anomalous force on the earth varies.
In a matter-dominated cosmology with $\Lambda=0$, in its expanding phase, the force is inward but decreasing over
time, so the orbit expands over time. What really matters then, is $(\der/\der t)(\ddot{a}/a)$.
If we were free to pick any function for $a(t)$, we could make up examples
in which $\dot{a}>0$ but $(\der/\der t)(\ddot{a}/a)<0$, so that the solar system would respond to cosmological
expansion by shrinking!

The function $a(t)$, however, has to satisfy the Friedmann equations, one of which is (in units with $G \ne 1$)
\begin{equation*}
  \frac{\ddot{a}}{a} = G\left[\frac{1}{3}\Lambda - \frac{4\pi}{3}(\rho+3P)\right]\eqquad.
\end{equation*}
The present epoch of the universe seems to be well modeled by dark energy described by a constant $\Lambda$ plus
dust with $P\ll \rho$. Differentiating both sides with respect to time gives
\begin{equation*}
 \frac{\der}{\der t}\left(\frac{\ddot{a}}{a}\right) \propto \dot{\rho}\eqquad,
\end{equation*}
with a negative constant of proportionality.
This ensures that the sign of the effect is always as expected from the naive Manichean image of
binding forces struggling against cosmological expansion (or perhaps cooperating during the contracting
phase of a Big Crunch cosmology). 

One way of understanding why this reduces so nicely to a dependence on $\dot{\rho}$
is the reasoning given in example \ref{eg:cosmo-hole} on p.~\pageref{eg:cosmo-hole}, in which we found that
the relative acceleration of two test particles A and B in a matter-dominated FRW cosmology could be calculated accurately
by pretending that it was due to the presence of the dust in any given sphere S surrounding the two particles.
We now let A be the sun, B the earth, and S a sphere centered on the sun whose radius equals the
radius of the earth's circular orbit. Due to cosmological expansion, the dust inside S
thins out with time, reducing its density $\rho$.  Applying Newton's laws to the orbit
of the earth gives $\omega^2 r = GM/r^2$, and
conservation of angular momentum results in $\omega r^2=\text{const}$. A calculation gives
$  r/r_\zu{o} = [M+(4\pi/3) \rho_\zu{o} r_\zu{o}^3]/[M+(4\pi/3) \rho r^3] $, which results
in $ \dot{r}/r_o \approx -(4\pi/3) G \omega_o^{-2}\dot{\rho} $. Application of the Friedmann
equations yields
\begin{equation*}
  \dot{r}/r_\zu{o} =  \omega_o^{-2} (d/dt)(\ddot{a}/a)\eqquad,
\end{equation*}
which is valid generally, not just for $P=0$. The $\omega_o^{-2}$ factor shows that the effect is smaller
for more tightly bound systems.

We know that the universe in the present era has $(d/dt)(\ddot{a}/a)>0$ because $\dot{\rho}<0$, and
for purposes of an order-of-magnitude estimate we can take $(d/dt)(\ddot{a}/a)\sim H_\zu{o}^3$.
Plugging in numbers for the earth-sun system,
we find that since the age of the dinosaurs, the radius of the earth's orbit has grown by less than
the diameter of an atomic nucleus.\footnote{The picturesque image comes from Cooperstock et al.,
\url{http://arxiv.org/abs/astro-ph/9803097v1}, who give a different calculation
leading to a result for $\dot{r}$ exactly equivalent to the one derived here.}


<% end_sec %> % Local effects of expansion

<% begin_sec("Observation") %>\label{sec:cosmology-observation}
Historically, it was believed that the cosmological constant was zero, that nearly all matter in the universe was
in the form of atoms, and that there was therefore only one interesting cosmological parameter to measure, which was
the average density of matter. This density was very difficult
to determine, even to within an order of magnitude, because most of the matter in the universe
probably doesn't emit light, making it difficult to detect. Astronomical distance scales were also very poorly calibrated
against absolute units such as the SI. Starting around 1995, however, a new set of techniques led to an era of
high-precision cosmology.
<% marg(-10) %>
<%
  fig(
    'cmb-geometry',
    %q{The angular scale of fluctuations in the cosmic microwave background can be used to infer the curvature of the universe.}
  )
%>
<% end_marg %>

<% begin_sec("Spatial curvature from CMB fluctuations") %>\index{cosmological constant!observation}
A strong constraint on the models comes from accurate measurements of the cosmic microwave
background, especially by the 1989-1993 COBE probe, and its 2001-2009 successor, the Wilkinson Microwave Anisotropy Probe,
positioned at the L2 Lagrange point of the earth-sun system, beyond the Earth on the line
connecting sun and earth.\footnote{Komatsu et al., 2010, \url{arxiv.org/abs/1001.4538}}
The temperature of the cosmic microwave background radiation is not the same in all directions, and
it can be measured at different angles.
In a universe with negative spatial
curvature, the sum of the interior angles of a triangle is less than the Euclidean value of 180 degrees.
Therefore if we observe a variation in the CMB over some angle, the distance between two points on
the surface of last scattering is actually greater than would have been inferred from Euclidean geometry.
The distance scale of such variations is limited by the speed of sound in the early universe, so
one can work backward and infer the universe's spatial curvature based on the angular scale of the
anisotropies. The measurements of spatial curvature are usually stated in terms of the parameter
$\Omega$, defined as the total average density of all source terms in the Einstein field equations,
divided by the critical density that results in a flat universe. $\Omega$ includes contributions
from matter, $\Omega_M$, the cosmological constant, $\Omega_\Lambda$, and radiation (negligible in the
present-day unverse). The results from WMAP, combined with other data from other methods, gives
$\Omega=1.005\pm .006$. In other words, the universe is very nearly spatially flat.
<% end_sec %> % Spatial curvature from CMB fluctuations

<% begin_sec("Accelerating expansion from supernova data") %>
The supernova data described on page \pageref{sec:vacuum-dominated} complement the CMB data because they are
mainly sensitive to the difference $\Omega_\Lambda-\Omega_M$, rather than their sum $\Omega=\Omega_\Lambda+\Omega_M$.
This is because these data measure the acceleration or deceleration of the universe's expansion. Matter produces
deceleration, while the cosmological constant gives acceleration. Figure \figref{supernova-graph} shows some recent
supernova data.\footnote{Riess et al., 2007, \url{arxiv.org/abs/astro-ph/0611572}. A larger data set is analyzed
in Kowalski et al., 2008, \url{arxiv.org/abs/0804.4142}.} The horizontal axis gives the redshift
factor $z=(\lambda'-\lambda)/\lambda$, where $\lambda'$ is the wavelength observed on earth and $\lambda$ the wavelength
originally emitted. It measures how fast the supernova's galaxy is receding from us. The vertical axis is
$\Delta(m-M)=(m-M)-(m-M)_{empty}$, where $m$ is the apparent magnitude, $M$ is the absolute magnitude, and
$(m-M)_{empty}$ is the value expected in a model of an empty universe, with $\Omega=0$. The difference $m-M$ is
a measure of distance, so essentially this is a graph of distance versus recessional velocity, of the same general
type used by Hubble in his original discovery of the expansion of the universe. Subtracting $(m-M)_{empty}$ on the
vertical axis makes it easier to see small differences. Since the WMAP data require $\Omega=1$, we need to fit the
supernova data with values of $\Omega_M$ and $\Omega_\Lambda$ that add up to one. Attempting to do so with
$\Omega_M=1$ and $\Omega_\Lambda=0$ is clearly inconsistent with the data, so we can conclude that the cosmological
constant is definitely positive.
<% end_sec %> 
<% marg(100) %>
<%
  fig(
    'supernova-graph',
    %q{A Hubble plot for distant supernovae. Each data point represents an average over several different supernovae with
          nearly the same $z$.}
  )
%>
<% end_marg %>

<% begin_sec("Density of matter from baryonic acoustic oscillations") %>\index{baryon acoustic oscillations}
Efforts such as the Sloan Digital Sky Survey have made three-dimensional maps of the density of luminous
matter in the universe.\footnote{Sanchez et al., 2012, \url{arxiv.org/abs/1203.6616}} The distribution is
clumpy. Measuring the average correlation $\xi$ between the density at points separated by some distance $s$ (measured
in the comoving frame),
one would expect that the function $\xi(s)$  would be largest when $s$ was small and would simply taper off with
increasing $s$. By analogy, we don't usually find a Manhattan-style landscape of skyscrapers side by side
with an uninhabited mountainous wilderness. On the other hand, imagine constructing such a correlation function for
houses in a subdivision in which the roads do not form any regular grid, but zoning regulations prohibit construction
of houses on lots of less than a certain size. In this situation, there would be zero probability of finding
houses separated by very small distances, and $\xi(s)$ would exhibit a peak at some larger scale set by the legal code.
The actual results of the sky surveys do show such a peak, which is due to well known physics
referred to as baryon acoustic oscillations 
(BAO).\footnote{Bassett and Hlozek, 2009, \url{arxiv.org/abs/0910.5224}}
In the early universe, any region of overdensity would tend to create a radiating
sound wave like the bang of
a firecracker. Such waves propagated at a known speed (about half the speed of light) for a known time
(about 400,000 years, until matter became deionized and transparent to radiation, making it immune to the
photon pressure that drove the oscillations). This leads to a known distance $s$, which forms a standard
ruler at which the peak in $\xi(s)$ occurs. In cosmological models, these results strongly constrain
$\Omega_M$, while being relatively insensitive to $\Omega_\Lambda$, and they are therefore complementary
to both the supernova data and the CMB results.
<% end_sec %> % Density of matter from baryonic acoustic oscillations
<% marg(300) %>
<%
  fig(
    'eisenstein-bao',
    %q{The acoustic peak in the BAO correlation function. Redrawn from Eisenstein, New Astronomy Reviews. 49 (2005) 360,
       as reproduced in Bassett and Hlozek, 2009, \url{arxiv.org/abs/0910.5224}.}
  )
%>
<% end_marg %>

<% begin_sec("Conclusions about cosmology") %>
Figure \figref{cosmology-parameters} summarizes what we can conclude about our universe, parametrized in terms of a
model with both $\Omega_M$ and $\Omega_\Lambda$ nonzero.\footnote{See Carroll, ``The Cosmological Constant,'' \url{http://www.livingreviews.org/lrr-2001-1}
for a full mathematical treatment of such models.} We can tell that it
originated in a Big Bang singularity, that it will go on expanding forever, and that it is very nearly flat.
Note that in a cosmology with nonzero values for both $\Omega_M$ and $\Omega_\Lambda$, there is no strict
linkage between the spatial curvature and the question of recollapse, as there is in a model with only
matter and no cosmological constant; therefore even though we know that the universe will not recollapse,
we do not know whether its spatial curvature is slightly positive (closed) or negative (open).
<% marg(45) %>
<%
  fig(
    'cosmology-parameters',
    %q{The cosmological parameters of our universe, after Perlmutter, 1998, \url{arxiv.org/abs/astro-ph/9812133}
       and Kowalski, 2008, \url{arxiv.org/abs/0804.4142}. The three shaded regions represent the 95\%
       confidence regions for the three types of observations.}
  )
%>
<% end_marg %>
<% end_sec %> % Conclusions about cosmology

<% begin_sec("Consistency checks") %>

Astrophysical considerations provide further constraints and consistency checks. In the era before the advent
of high-precision cosmology, estimates of the age of the universe ranged from 10 billion to 20 billion years, and the
low end was inconsistent with the age of the oldest globular clusters. This was believed to be a problem either for
observational cosmology or for the astrophysical models used to estimate the age of the clusters: ``You can't be
older than your ma.'' Current data have shown that the low estimates of the age were incorrect, so consistency is
restored.

That only a small fraction of the universe's matter was luminous had been suspected by astronomers such as
Zwicky as early as 1933, based on the inability to reconcile the observed kinematics with Newton's laws if
all matter was assumed to be luminous.
<% end_sec %> % Consistency checks

<% begin_sec("Dark matter") %>         
Another constraint comes from models of nucleosynthesis during the era shortly after the Big Bang (before the
formation of the first stars). The observed relative abundances of hydrogen, helium, and deuterium cannot be
reconciled with the density of ``dust'' (i.e., nonrelativistic matter) inferred from the observational
data. If the inferred mass density were entirely due to normal ``baryonic'' matter (i.e., matter whose mass
consisted mostly of protons and neutrons), then nuclear reactions in the dense early universe should have
proceeded relatively efficiently, leading to a much higher ratio of helium to hydrogen, and a much lower
abundance of deuterium. The conclusion is that most of the matter in the universe must be made of
an unknown type of exotic non-baryonic matter, known generically as ``dark matter.''\index{dark matter}

 The existence of nonbaryonic matter is also required in order to
reconcile the observed density of galaxies with the observed strength of the CMB fluctuations, and in merging
galaxy clusters it has been observed that the gravitational potential is offset from the radiating plasma.
A 2012 review paper on dark matter is Roos, \url{arxiv.org/abs/1208.3662}.

A number of experiments are under way to detect dark matter directly. As of 2013,
the most sensitive experiment has given null results: \url{arxiv.org/abs/1310.8214}.

At one time it was widely expected that dark matter would consist of the lightest supersymmetric particle, which
might for example be the neutralino. However, results from the LHC seem to make it unlikely that our universe
exhibits supersymmetry, assuming that the energy scale is the electroweak scale, which is the only scale that
has strong motivation. It now appears more likely that dark matter consists of some other particle such
as sterile neutrinos or axions.

Even with the inclusion of dark matter, there is a problem with the abundance of lithium-7 relative to
hydrogen, which models greatly overpredict.\footnote{\url{arxiv.org/abs/0808.2818}, \url{arxiv.org/abs/1107.1117}}
<% end_sec %> % Dark matter
<% end_sec %> % observation

<% end_sec %> % Cosmological solutions

<% begin_sec("Mach's principle revisited") %>\label{sec:brans-dicke}\index{Mach's principle}
<% begin_sec("The Brans-Dicke theory") %>
Mach himself never succeeded in stating his ideas in the form of a precisely testable
physical theory, and we've seen that to the extent that Einstein's hopes and intuition had been formed by Mach's
ideas, he often felt that his own theory of gravity came up short. The reader has so far encountered
Mach's principle in the context of certain thought experiments that are obviously impossible to realize,
involving a hypothetical universe that is empty except for certain apparatus (e.g., section \ref{einstein-two-planets}, p.~\pageref{einstein-two-planets}).
It would be easy, then, to get an impression of Mach's principle as one of those theories that is ``not even
wrong,'' i.e., so ill-defined that it cannot even be falsified by experiment, any more than Christianity can be.

But in 1961, Robert Dicke and his student Carl Brans came up with a theory of gravity that made testable predictions, and
that was specifically designed to be more Machian than general relativity.\index{Brans-Dicke theory} Their paper\footnote{C. Brans and R. H. Dicke, ``Mach's
Principle and a Relativistic Theory of Gravitation,'' Physical Review 124 (1961) 925} is extremely readable, even for
the non-specialist. In this theory, the seemingly foolproof operational definition of a Lorentz frame given on p.~\pageref{operational-lorentz}
fails. On the first page, Brans and Dicke propose one of those seemingly foolish
thought experiments about a nearly empty universe:

\begin{quotation}
The imperfect expression of [Mach's ideas] in general relativity can be seen by considering the case of a space
empty except for a lone experimenter in his laboratory. [...] The observer would, according to general relativity,
observe normal behavior of his apparatus in accordance with the usual laws of physics. However, also according
to general relativity, the experimenter could set his laboratory rotating by leaning out a window and firing his
22-caliber rifle tangentially. Thereafter the delicate gyroscope in the laboratory would continue to point in
a direction nearly fixed relative to the direction of motion of the rapidly receding bullet. The gyroscope would
rotate relative to the walls of the laboratory. Thus, from the point of view of Mach, the tiny, almost massless,
very distant bullet seems to be more important than the massive, nearby walls of the laboratory in determining
inertial coordinate frames and the orientation of the gyroscope.
\end{quotation}

They then proceed to construct a mathematical and more Mach\-ian theory of gravity. From the Machian point of view,
the correct local definition of an inertial frame must be determined relative to the bulk of the matter in the
universe. We want to retain the Lorentzian local character of spacetime, so this influence can't be transmitted
via instantaneous action at a distance.
It must propagate via some physical field, at a speed less than or equal to $c$. It is implausible that this
field would be the gravitational field as described by general relativity. Suppose we divide the cosmos up
into a series of concentric spherical shells centered on our galaxy. In Newtonian mechanics, the gravitational
field obeys Gauss's law, so the field of such a shell vanishes identically on the interior. In relativity,
the corresponding statement is Birkhoff's theorem,\index{Birkhoff's theorem} which states that the Schwarzschild
metric is the unique spherically symmetric solution to the vacuum field equations. Given this solution in the exterior universe,
we can set a boundary condition at the outside surface of the shell, use the Einstein field equations to extend the solution through it,
and find a unique solution on the interior, which is simply a flat space.

Since the Machian effect can't be carried by the gravitational field, Brans and Dicke
took up an idea earlier proposed by Pascual Jordan\footnote{Jordan was a member of the Nazi \emph{Sturmabteilung} or
``brown shirts'' who nevertheless ran afoul of the Nazis for his close professional relationships with Jews.} of hypothesizing
an auxiliary field $\phi$. The fact that such a field has never been detected directly suggests that it has no mass or charge.
If it is massless, it must propagate at exactly $c$, and this also makes sense because if it were to propagate at speeds less than
$c$, there would be no obvious physical parameter that would determine that speed. How many tensor indices
should it have? Since Mach's principle tries to give an account of inertia, and
inertial mass is a scalar,\footnote{A limit of $5\times 10^{-23}$
has been placed on the anisotropy of the inertial mass of the proton: R.W.P. Drever, ``A search for anisotropy of inertial mass using a free
precession technique,'' Philosophical Magazine, 6:687 (1961) 683.} $\phi$ should presumably be a scalar (quantized by a spin-zero particle). Theories of this type
are called tensor-scalar theories, because they use a scalar field in addition to the metric tensor.

The wave equation for a massless scalar field, in the absence of sources, is simply $\nabla_i \nabla^i \phi=0$. The solutions of this
wave equation fall off as $\phi\sim 1/r$. This is gentler than the $1/r^2$ variation of the gravitational field, so
results like Newton's shell theorem and Birkhoff's theorem no longer apply. If a spherical shell of mass acts as a source
of $\phi$, then $\phi$ can be nonzero and varying inside the shell. The $\phi$ that you experience
right now as you read this book should be a sum of wavelets originating from all the masses whose world-lines
intersected the surface of your past light-cone. In a static universe, this sum would diverge linearly, so a self-consistency
requirement for Brans-Dicke gravity is that it should produce cosmological solutions that avoid such a divergence, e.g.,
ones that begin with Big Bangs.

Masses are the sources of the field $\phi$. How should they couple to it? Since $\phi$ is a scalar, we need to
construct a scalar as its source, and the only reasonable scalar that can play this role is the trace of the
stress-energy tensor, $T\indices{^i_i}$. As discussed in example \ref{eg:failed-scalar-tinkering} on page
\pageref{eg:failed-scalar-tinkering}, this vanishes for light, so the
only sources of $\phi$ are material particles.\footnote{This leads to an
exception to the statement above that all Brans-Dicke spacetimes are expected
to look like Big Bang cosmologies. Any solution of the GR field equations
containing nothing but vacuum and
electromagnetic fields (known as an ``elevtrovac'' solution) is also a
valid Brans-Dicke spacetime. In such a spacetime, a constant $\phi$ can be
set arbitrarily. Such a spacetime is in some sense not generic for Brans-Dicke
gravity.}
Even so, the Brans-Dicke theory retains a form of the equivalence principle. As discussed on pp.~\pageref{sec:chiao-paradox} and \pageref{eg:chiao-energy},
the equivalence principle is a statement about the results of local experiments, and $\phi$ at any given location in the
universe is dominated by contributions from matter lying at cosmological distances.
Objects of different composition will have differing fractions of their mass that arise from internal electromagnetic
fields. Two such objects will still follow identical geodesics, since their own effect on the local value of $\phi$
is negligible. This is unlike the behavior of electrically charged objects, which experience significant back-reaction
effects in curved space (p.~\pageref{sec:chiao-paradox}). However, the strongest form of the equivalence principle requires that all experiments in free-falling laboratories produce
identical results, no matter where and when they are carried out. Brans-Dicke gravity violates this, because such experiments
could detect differences between the value of $\phi$ at different locations --- but of course this is part and parcel of
the purpose of the theory.

We now need to see how to connect $\phi$ to the local notion of inertia so as to produce an effect of the kind
that would tend to fulfill Mach's principle. In Mach's original formulation, this would entail some kind of
local rescaling of all inertial masses, but Brans and Dicke point out that in a theory of gravity, this is
equivalent to scaling the Newtonian gravitational constant $G$ \emph{down} by the same factor. The latter turns out
to be a better approach.
For one thing, it has a natural interpretation in terms of units.
Since $\phi$'s amplitude falls off as $1/r$, we can write $\phi\sim \Sigma m_i/r$, where the sum is over
the past light cone. If we then make the identification of $\phi$ with $1/G$ (or $c^2/G$ in a system wher $c \ne 1$), the
units work out properly, and the coupling constant between matter and
$\phi$ can be unitless. If this coupling constant, notated $1/\omega$, were not unitless, then the theory's predictive value would be weakened, because
there would be no way to know what value to pick for it. For a unitless constant, however, there is a reasonable
way to guess what it should be: ``in any sensible theory,'' Brans and Dicke write, ``$\omega$ must be of the general order of magnitude of unity.''
This is, of course, assuming that the Brans-Dicke theory was correct. In general, there are other reasonable values to
pick for a unitless number, including zero and infinity.
The limit of $\omega\rightarrow\infty$ recovers the special case of general relativity. Thus Mach's principle, which
once seemed too vague to be empirically falsifiable, comes down to measuring a specific number, $\omega$, which
quantifies how non-Machian our universe is.\footnote{
Another good technical reasons for 
thinking of $\phi$ as relating to the gravitational constant is that general relativity has a standard prescription for
describing fields on a background of curved spacetime. The vacuum field equations of general relativity can be derived
from the principle of least action, and although the details are beyond the scope of this book (see, e.g.,
Wald, \emph{General Relativity}, appendix E), the general idea is that we define a Lagrangian density
$\mathcal{L}_G$ that depends on the Ricci scalar curvature, and then extremize its integral over all possible
histories of the evolution of the gravitational field. If we want to describe some other field, such as matter,
light, or $\phi$, we simply take the special-relativistic Lagrangian $\mathcal{L}_M$ for that field, change
all the derivatives to covariant derivatives, and form the sum $(1/G)\mathcal{L}_G+\mathcal{L}_M$. In the Brans-Dicke
theory, we have three pieces, $(1/G)\mathcal{L}_G+\mathcal{L}_M+\mathcal{L}_\phi$,
where $\mathcal{L}_M$ is for matter and $\mathcal{L}_\phi$ for $\phi$. If we were to
interpret $\phi$ as a rescaling of inertia, then we would have to have $\phi$ appearing as a fudge factor
modifying all the inner workings of $\mathcal{L}_M$. If, on the other hand, we think of $\phi$ as changing
the value of the gravitational constant $G$, then the necessary modification is extremely simple. Brans and
Dicke introduce one further modification to $\mathcal{L}_\phi$ so that the coupling constant $\omega$
between matter and $\phi$ can be unitless. This modification has no effect on the wave equation of $\phi$
in flat spacetime.}
<% end_sec %> % The Brans-Dicke theory
<% begin_sec("Predictions of the Brans-Dicke theory") %>

Returning to the example of the spherical shell of mass, we can see based on considerations of units
that the value of $\phi$ inside should be $\sim m/r$, where $m$ is the total mass of the shell and $r$ is its
radius. There may be a unitless factor out in front, which will depend on $\omega$, but for $\omega\sim 1$ we
expect this constant to be of order 1. Solving the nasty set of field equations that result from their
Lagrangian, Brans and Dicke indeed found $\phi\approx[2/(3+2\omega)](m/r)$, where the constant in square brackets is of order unity if $\omega$ is
of order unity. In the limit of $\omega\rightarrow\infty$, $\phi=0$, and the shell has no physical effect
on its interior, as predicted by general relativity.

Brans and Dicke were also able to calculate cosmological models, and in a typical model with a nearly spatially flat
universe, they found $\phi$ would vary according to
\begin{equation*}
  \phi = 8\pi \frac{4+3\omega}{6+4\omega} \rho_o t_o^2 \left(\frac{t}{t_o}\right)^{2/(4+3\omega)}\eqquad,
\end{equation*}
where $\rho_o$ is the density of matter in the universe at time $t=t_o$.
When the density of matter is small, $G$ is large, which has the same observational consequences as
the disappearance of inertia; this is exactly what one expects according to Mach's principle.
For $\omega\rightarrow\infty$, the gravitational ``constant'' $G= 1/\phi$ really is constant.

Returning to the thought experiment involving the 22-caliber rifle fired out the window, we find
that in this imaginary universe, with a very small density of matter, $G$ should be very large. This causes
a frame-dragging effect from the laboratory on the gyroscope, one much stronger than we would
see in our universe. Brans and Dicke calculated this effect for a laboratory consisting of a spherical
shell, and although technical difficulties
prevented the reliable extrapolation of their result to $\rho_o\rightarrow 0$, the trend was that
as $\rho_o$ became small, the frame-dragging effect would get stronger and stronger, presumably
eventually forcing the gyroscope to precess in lock-step with the laboratory. There would thus be
no way to determine, once the bullet was far away, that the laboratory was rotating at all --- in
perfect agreement with Mach's principle.
<% end_sec %> % Predictions of the Brans-Dicke theory
<% begin_sec("Hints of empirical support") %>
Only six years after the publication of the Brans-Dicke theory, Dicke himself, along with 
H.M.~Goldenberg\footnote{Dicke and Goldenberg, ``Solar Oblateness and General Relativity,'' Physical Review Letters 18 (1967) 313}
carried out a measurement that seemed to support the theory empirically. Fifty years before, one of the first empirical tests
of general relativity, which it had seemed to pass with flying colors, was the anomalous perihelion precession of Mercury.
The word ``anomalous,'' which is often left out in descriptions of this test, is required because there are
many nonrelativistic reasons why Mercury's orbit precesses, including interactions with the other planets and
the sun's oblate shape. It is only when these other effects are subtracted out that one sees the general-relativistic
effect calculated on page \pageref{sec:schwarzschild-orbits}. The sun's oblateness is difficult to measure
optically, so the original analysis of the data had proceeded by determining the sun's rotational period by observing
sunspots, and then assuming that the sun's bulge was the one found for a rotating fluid in static equilibrium. The result
was an assumed oblateness of about $1\times10^{-5}$. But
we know that the sun's dynamics are more complicated than this, since it has convection currents and magnetic fields.
Dicke, who was already a renowned experimentalist, set out to determine the oblateness by direct optical measurements,
and the result was $(5.0\pm 0.7)\times 10^{-5}$, which, although still very small, was enough to put the observed
perihelion precession out of agreement with general relativity by about 8\%. The perihelion precession predicted by
Brans-Dicke gravity differs from the general relativistic result by a factor of $(4+3\omega)/(6+3\omega)$.
The data therefore appeared to require $\omega\approx 6\pm 1$, which would be inconsistent with general relativity.
<% marg(110) %>
<%
  fig(
    'dicke-oblateness',
    %q{The apparatus used by Dicke and Goldenberg to measure the oblateness of the sun was essentially a telescope with
        a disk inserted in order to black out most of the light from the sun.}
  )
%>
<% end_marg %>
<% end_sec %> % Hints of empirical support
<% begin_sec("Mach's principle is false.") %>
The trouble with the solar oblateness measurements was that they were subject to a large number of possible
systematic errors, and for this reason it was desirable to find a more reliable test of Brans-Dicke gravity.
Not until about 1990 did
a consensus arise, based on measurements of oscillations of the solar surface, that the pre-Dicke value was correct.
In the interim, the confusion had the salutary effect of stimulating a renaissance of theoretical and
experimental work in general relativity. Often if one doesn't
have an alternative theory, one has no reasonable basis on which to design and interpret
experiments to test the original theory.

Currently, the best bound on $\omega$ is based on measurements\footnote{Bertotti, Iess, and Tortora, ``A test of general relativity using radio links with the Cassini spacecraft,''
Nature 425 (2003) 374} of the propagation of radio signals between earth and
the Cassini-Huygens space probe in 2003, which require $\omega>4\times10^4$. This is so much greater than
unity that it is reasonable to take Brans and Dicke at their word that ``in any sensible theory, $\omega$ must be of the general order of magnitude of unity.''
Brans-Dicke fails this test, and is no longer a ``sensible'' candidate for a theory of gravity.
We can now see that Mach's principle, far from being
a fuzzy piece of philosophical navel-gazing, is a testable hypothesis. It has been tested and found to be false, in the following sense.
Brans-Dicke gravity is about as natural a formal implementation of Mach's principle as could be hoped for,
and it gives us a number $\omega$ that parametrizes how Machian the universe is. The empirical value of $\omega$
is so large that it shows our universe to be essentially as non-Machian as general relativity.
<% end_sec %> % Mach's principle is false


<% end_sec %> % Mach's principle revisited

<% begin_sec("Historical note: the steady-state model") %>\label{sec:steady-state}
From 1948 until around the mid-1960s, the Big Bang theory had viable competition in the form of
the steady-state model,\index{steady-state cosmology}
originated by the British trio of Fred Hoyle, Hermann Bondi, and 
Thomas Gold.\index{Hoyle, Fred}\index{Bondi, Hermann}\index{Gold, Thomas}
Legend has it that
they came up with the idea after seeing a horror movie called Dead of Night, in which events from
the beginning of the story repeat themselves later. This led
them to imagine that the universe could, although expanding, remain locally in the same state
at all times. If this were to happen, the empty space being opened up between the galaxies
would have to be filled back in by the spontaneous creation of matter.
The model holds a strong philosophical appeal because it generalizes the Copernican principle
so that it applies not just to conditions everywhere in space but also at all times.

They published the
idea in a pair of back-to-back papers, one by Bondi and 
Gold\footnote{``The Steady-State Theory of the Expanding Universe,'' MNRAS 108 (1948) 252;
\url{adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=1948MNRAS.108..252B}} and one by
Hoyle,\footnote{``A New Model for the Expanding Universe,'' MNRAS 108 (1948) 372,
\url{adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=1948MNRAS.108..372H}
} with comments
appended to the former on the differences between the two approaches. The Bondi-Gold paper is
especially fun to read, because it is written in nontechnical language and shows a type of
daring and creative science that is not often encountered today. Much of it reads like a
catalog of cherished principles of physics that were to be given up,
including Lorentz invariance, general relativity,
the equivalence principle, and possibly the laws of
conservation of charge and mass-energy. The following is a brief presentation (in slightly
different notation) of the Hoyle's more mathematically
detailed ideas, as sketched in his original paper. Although Hoyle eventually fleshed out the ideas more
thoroughly, by the time he had done so the steady-state theory was already on its way to being
crushed under the weight of contrary observations.

Since the model is always to be in the same state, the quantity $\dot{a}/a$ must always be
the same, i.e., the Hubble constant really is a constant over time. This requires exponential
growth, which means that the geometry is that of de Sitter space.
In any model that assigns an infinite age to the universe, one must explain why
the universe has not undergone heat death due to the second law of thermodynamics.
The steady-state model successfully addresses this problem, because the exponential
expansion is rapid enough to prevent thermal equilibrium from happening.

Hoyle sets out to preserve local conservation of energy-momentum, without which
the Einstein field equations become inconsistent.
(This was Hoyle's more conservative approach.
Bondi and Gold advocated replacing general relativity completely rather than modifying it.)
He postulates a massless, chargeless, scalar field $C$, called the ``C field,''
the letter ``C'' standing for ``creation.''
Suppose that the C field's contribution to the stress-energy tensor ends up being that of
a perfect fluid with the same rest frame as the ordinary matter.
The rate of creation of mass-energy is then given by the divergence
$\nabla_\mu T^\mu_t$, and we need this to be zero. As shown in example \ref{eg:de-sitter-cons}
on p.~\pageref{eg:de-sitter-cons}, this requires that our total stress-energy mimic that
of a cosmological constant, with $\rho+P=0$.
Since the ordinary matter has $\rho>0$ and $P>0$,
the C field will either need to contribute negative
energy density or negative pressure.
We'll see below that Hoyle's model is constructed so that the C field has zero energy and negative pressure.
One will often see the C field described incorrectly as having negative energy to cancel out the
positive energy of the matter being created. That wouldn't have worked, because then the total
energy density $\rho$ would always be zero, which is not what we observe. (For example,
the Friedmann equation for $\ddot{a}/a$ relates $\rho$ to the square of the Hubble constant.)

To understand more about why the theory took the form it did, it is helpful to look at some general physical
considerations about symmetry. As Bondi and Gold admit candidly,
any theory of this type is likely to violate Lorentz invariance.
We can observe an evacuated box and wait for hydrogen atoms to appear.
When they appear, they're in some state of motion, at least on the average. This
state of motion defines a preferred frame. In addition to breaking symmetry under Lorentz transformations,
the theory lacks time-reversal invariance (because matter appears but never disappears)
and charge-conjugation symmetry (because matter appears but antimatter doesn't).
All of these asymmetries arise because in this approach, we try to explain the observed
asymmetries of the cosmological state of the universe as arising directly from asymmetries in
the underlying local laws of physics. Such an approach is very different from the modern one,
in which we expect the asymmetries to arise from either boundary conditions or instabilities
(spontaneous symmetry breaking).

Because the C field is massless and chargeless, we would normally expect it
to obey the wave equation $\nabla_a\nabla^a C=0$. Hoyle's field does not, however,
evolve according to any Lorentz-invariant dynamical law. Instead it simply evolves as
$C=t$, where $t$ is a preferred time coordinate. In any cosmological model in which the matter
fields are modeled as perfect fluids, we have a preferred time coordinate which is
the proper time of an observer at rest with respect
to the fluid, and in the Hoyle model we do assume that this is the time $t$ we should use
in defining $C$. However, Hoyle's theory is different because it gives this preferred time a role
in the local laws of physics, thereby breaking Lorentz invariance. 

The value of the scalar field $C$ cannot have any
directly observable effects, since then its time-evolution would distinguish one epoch of the universe
from another.
Instead we form the gradient $\nabla_a C$. This gives a vector, which can be interpreted as a velocity vector defining the
preferred frame of reference. An observer in this frame is at rest relative to the local cosmological
fluid, observes the universe to be homogeneous, and also observes that when new atoms are created
from the vacuum, they are on the average at rest. Thus $\nabla_a C$ is observable.

The contribution of the C field to the stress-energy must be a rank-two tensor, and 
if we want to construct such a tensor, the only good possibility that occurs to me\footnote{
The only other obvious possibility would have been
something like $-k\nabla_a C \nabla_b C$. This would be the stress-energy of a negative-mass
dust, which would be unacceptable for the reasons discussed earlier.} is
$k\nabla_a \nabla_b C$, with $k$ a positive constant.
If the derivatives had been ordinary partial derivatives, the second derivative
would have vanished because $C$ is linear
in time, but the covariant derivatives do not vanish, and in fact the second derivative is a tensor
measuring the rate of cosmological expansion; the trace $\nabla^a\nabla_a C$ is the volume expansion $\Theta$ defined on
p.~\pageref{volume-expansion}. For de Sitter space, $\Theta$ has a constant value equal to three times the Hubble
constant $H_\zu{0}$.
We can now see why we could not take the C field to evolve according to the usual wave
equation $\nabla_a\nabla^a C=0$; if it did, then we would have $\Theta=0$, and the universe would not be expanding.

When we evaluate the second derivative for the de Sitter metric, the only nonvanishing Christoffel
symbols that occur are $\Gamma\indices{^t_{xx}}=\Gamma\indices{^t_{yy}}=\Gamma\indices{^t_{zz}}=\dot{a}a$.
We find $T^t_t=0$ and $T^x_x=T^y_y=T^z_z=k\Theta/3$. Thus the C field's mass-energy density is
zero, while for its pressure we have $P=-k\Theta/3$, which is negative.
% This checks against Hoyle 1948. His C_ab is basically the stress-energy of the C field (no sign flip).
% He gives C_11<0, and C_11 is P (not -P), regardless of signature.

For simplicity, we take the ordinary matter to be dust. The total stress-energy then consists of
an energy density $\rho$ that is due only to the dust, and a negative pressure $P$ that comes
only from the C field. If we require both of these to be constant, take the cosmological constant
to be zero, set $a=e^{H_\zu{0}t}=e^{\Theta t/3}$,
and substitute into the Friedmann equations on p.~\pageref{friedmann-equations}, we
find $P=-\rho$, or $k=3\rho/\Theta=\rho/H_\zu{0}$.

Like the cosmological constant, the C field is taken to be a universally
prescribed property of the vacuum. There is a difference, however, because the cosmological constant's
contribution to the stress-energy is proportional to the metric, which preserves the equivalence
principle. As remarked at the end of the Bondi-Gold paper, the C field violates the equivalence
principle. No calculation is spelled out, but they say based on a personal communication from Hoyle
that the field exerts a force on matter which produces a significant acceleration in an atom,
but a negligible one in a star.

A claimed selling point of the C field was that it would prevent the formation of singularities,
including both a Big Bang singularity and black holes. This is reasonable, 
since the C field violates
all of the energy conditions listed on p.~\pageref{table-of-energy-conditions} except for the trace energy condition.
The Penrose singularity theorem depends on the null energy condition, and the
Hawking singularity theorem requires either the strong or the null energy condition.

Because we can't make the C field obey the proper wave equation for a massless, spin-zero particle,
there is no obvious way to make up a dynamical law for its evolution in order to replace the
fixed relation $C=t$, and we do not expect to have any classical field theory for the C field.
Hoyle did attempt to add dynamics to the model by making it into what is known as a ``direct
field,'' which was a type of action-at-a-distance theory that in the 1960s was believed to be
a good candidate for the fundamental description of the forces of physics. (Quantum field
theory had not developed to the point where it could handle the strong or weak nuclear forces.)
Such theories were shown to be nonviable as quantum theories in 1963 by Currie, Jordan, and
Sudarshan.

The steady-state model began to succumb to contrary evidence when Ryle and coworkers counted radio
sources and found that they did not show the statistical behavior predicted by the model.
The coup de grace came with the discovery of the cosmic microwave background, which demonstrated directly
that the universe had once been much hotter than it is now.
Attempts have been made to produce variations on the model that are consistent with these observations,
but they have not succeeded; for a detailed discussion
see \url{http://www.astro.ucla.edu/~wright/stdystat.htm}.

<% end_sec %>

<% begin_hw_sec %>


<% begin_hw('nuclear-pressure') %>
Verify, as claimed on p.~\pageref{nuclear-pressure}, that the electromagnetic pressure inside
a medium-weight atomic nucleus is on the order of $10^{33}\ \zu{Pa}$.
<% end_hw %>

\vspace{3mm}

<% begin_hw('big-bang-removable') %>
Is the Big Bang singularity removable by the coordinate transformation $t\rightarrow 1/t$?
\hwsoln
<% end_hw %>

\vspace{3mm}

<% begin_hw('verify-milne') %>
Verify the claim made on p.~\pageref{milne} that $a$ is a linear function of time in the case
of the Milne universe, and that $k=-1$.
<% end_hw %>

\vspace{3mm}

<% begin_hw('milne-rope') %>
Examples \ref{eg:girdle} on page \pageref{eg:girdle} and \ref{eg:whip} on page \pageref{eg:whip}
discussed ropes with cosmological lengths. Reexamine these examples in the case of the Milne
universe.
\hwsoln
<% end_hw %>

\vspace{3mm}

<% begin_hw('frw-time-reversal') %>
(a) Show that the Friedmann equations are symmetric under time reversal.
(b) The spontaneous breaking of this symmetry in perpetually expanding solutions was discussed on page \pageref{spontaneous-symm-breaking}.
Use the definition of a manifold to show that this symmetry cannot be restored by gluing together an expanding solution and a contracting
one ``back to back'' to create a single solution on a single, connected manifold.
\hwsoln
<% end_hw %>

\vspace{3mm}

<% begin_hw('field-equations-parity') %>
The Einstein field equations are
\begin{equation*}
  G_{ab} = 8\pi T_{ab} + \Lambda g_{ab}\eqquad,
\end{equation*}
and when it is possible to adopt a frame of reference in which the local mass-energy is at rest on average,
we can interpret the stress-energy tensor as
\begin{equation*}
  T\indices{^\mu_\nu}=\operatorname{diag}(-\rho,P,P,P)\eqquad,
\end{equation*}
where $\rho$ is the mass-energy density and $P$ is the pressure.
Fix some point as the origin of a local Lorentzian coordinate system.
Analyze the properties of these relations under
a reflection such as $x \rightarrow -x$ or $t \rightarrow -t$.
\hwsoln
<% end_hw %>

\vspace{3mm}

<% begin_hw('lambda-violates-sec') %>
(a) Show that a positive cosmological constant violates the strong energy condition in a vacuum.
In applying the definition of the strong energy condition, treat the cosmological constant as a form
of matter, i.e., ``roll in'' the cosmological constant term to the stress-energy term in the field equations.
(b) Comment on how this affects the results of the following paper:
Hawking and Ellis, ``The Cosmic Black-Body Radiation and the Existence of Singularities in Our Universe,''
Astrophysical Journal, 152 (1968) 25,\\ \url{http://articles.adsabs.harvard.edu/f...pJ...152...25H}.
<% end_hw %>

\pagebreak

<% begin_hw('uniform-field-sources') %>\index{uniform gravitational field}\index{gravitational field!uniform}
In problem \ref{hw:uniform-field} on page \pageref{hw:uniform-field}, we analyzed the properties of the metric
\begin{equation*}
  \der s^2 = e^{2gz}\der t^2-\der z^2\eqquad.
\end{equation*}
(a) In that problem we found that this metric had the same properties at all points in space.
Verify in particular that it has the same scalar curvature $R$ at all points in space.\hwendpart
(b) Show that this is a vacuum solution in the two-dimensional $(t,z)$ space.\hwendpart
(c) Suppose we try to generalize this metric to four dimensions as
\begin{equation*}
\der s^2 = e^{2gz}\der t^2-\der x^2 - \der y^2 - \der z^2\eqquad.
\end{equation*}
Show that this requires an Einstein tensor with unphysical  properties.\hwendpart
\hwsoln
<% end_hw %>

<% begin_hw('milne-chain') %>
Consider the following proposal for defeating relativity's prohibition on velocities greater than $c$.
Suppose we make a chain billions of light-years long and attach one end of the chain to a particular
galaxy. At its other end, the chain is free, and it sweeps past the local galaxies at a very high speed.
This speed is proportional to the length of the chain, so by making the chain long enough, we can
make the speed exceed $c$.

Debunk this proposal in the special case of the Milne universe.
<% end_hw %>

<% begin_hw('observable-universe-observer-dependent') %>
Make a rigorous definition of the volume $V$ of the \emph{observable} universe.
Suppose someone asks whether $V$ depends on the observer's state of motion.
Does this question have a well-defined answer? If so, what is it? Can we
calculate $V$'s observer-dependence by applying a Lorentz contraction?\hwsoln
<% end_hw %>

<% begin_hw('flat-perfect-fluid-cosmology') %>
For a perfect fluid, we have $P=w\rho$, where $w$ is a constant. The cases $w=0$ and $w=1/3$ correspond, respectively, to
dust and radiation. Show that for a flat universe with $\Lambda=0$ dominated by a single component that is a perfect fluid,
the solution to the Friedmann equations is of the form $a\propto t^\delta$, and determine the exponent $\delta$.
Check your result in the dust case against the one on p.~\pageref{flat-dust}, then find the exponent in the 
radiation case. Although the $w=-1$ case corresponds to a cosmological constant, show that the solution is not
of this form for $w= -1$.
\hwsoln
<% end_hw %>

<% begin_hw('observable-universe-perfect-fluid') %>
Apply the result of problem \ref{hw:flat-perfect-fluid-cosmology} to generalize the result of example
\ref{eg:observable-universe} on p.~\pageref{eg:observable-universe} for the size of the observable
universe. What is the result in the case of the radiation-dominated universe?
\hwsoln
<% end_hw %>




<% end_hw_sec %>



<% end_chapter %>
