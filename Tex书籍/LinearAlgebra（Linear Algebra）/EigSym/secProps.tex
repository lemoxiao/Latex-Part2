%!TEX root = ../larxxia.tex


\section{Beautiful properties for symmetric matrices}
\label{sec:sm}
\secttoc

\begin{comment}
\pooliv{\S5.4} \layiv{\S7.1} \holti{\S8.3} \cite[\S10]{Davis99a}
\end{comment}



This section starts by exploring two properties for eigenvalues of general matrices, and then proceeds to the special case of real symmetric  matrices which have the beautifully useful properties of always having real eigenvalues and orthogonal eigenvectors.



\subsection{Matrix powers maintain eigenvectors}
\label{sec:mpmev}

Recall that \autoref{sec:im} introduced the inverse of a matrix (\autoref{def:invertible}).
This first theorem links an eigenvalue of zero to the non-existence of an inverse and hence links a zero eigenvalue to problematic linear equations.


\begin{theorem} \label{thm:evalinv} 
A \idx{square matrix} is \idx{invertible} iff \idx{zero} is \emph{not} an \idx{eigenvalue} of the matrix.
\end{theorem}
\begin{proof} 
From \autoref{def:evecval}, zero is an eigenvalue, \(\lambda=0\)\,, iff \(A\xv=0\xv\) has nonzero solutions~\xv; that is, iff the homogeneous system \(A\xv=\ov\) has nonzero solutions~\xv.
But the Unique Solution Theorems~\ref{thm:ftim1} or~\ref{thm:ftim2} assure us that this occurs iff matrix~\(A\) is not invertible.
Consequently a matrix is invertible iff zero is not an eigenvalue.
\end{proof}

\begin{example} \label{eg:}
\begin{itemize}
\item The \(3\times 3\) matrix of \autoref{eg:eig3intro} (also \ref{eg:eig3sp}, \ref{eg:eig3eig} and~\ref{eg:eig3introval}) is not invertible as among its eigenvalues of \(0\), \(1\) and~\(3\) it has zero as an eigenvalue.

\item The plot in the margin shows (unit) vectors~\xv\ (blue), and for some matrix~\(A\) the corresponding vectors~\(A\xv\) (red) adjoined.
\marginpar{\eRose1{-0.5}{-0.5}1}%
%\begin{aside}
%The \script[1]\ function \index{eigshow()@\texttt{eigshow()}}\texttt{eigshow(A)} provides an interactive alternative to this static view.
%\end{aside}%
There are no directions~\xv\ for which \(A\xv=\ov=0\xv\)\,. 
Hence zero cannot be an eigenvalue and the matrix~\(A\) must be invertible.

Similarly for \autoref{eg:eig2pic2}.

\item The \(3\times3\) diagonal matrix of \autoref{eg:espace2d} has eigenvalues of only \(-\tfrac13\) and~\(\tfrac32\).
Since zero is not an eigenvalue, the matrix is invertible.

\item The \(9\times9\) matrix of the Sierpinski network in \autoref{eg:sier2eig} is not invertible as it has zero among its five eigenvalues.

\item The \(2\times 2\) matrix of \autoref{eg:2evals} is invertible as its eigenvalues are \(\lambda=\frac12,\frac32\), neither of which are zero.
Indeed, the matrix
\begin{equation*}
A=\begin{bmatrix} 1&-\frac12\\-\frac12&1 \end{bmatrix},
\quad\text{has inverse }A^{-1}=\begin{bmatrix} \frac43&\frac23\\\frac23&\frac43 \end{bmatrix}
\end{equation*}
as matrix multiplication confirms.

\item The \(2\times2\) non-symmetric matrix of \autoref{eg:ccevals} is invertible because zero is not among its eigenvalues of \(\lambda=\pm i\)\,.
Indeed, the matrix
\begin{equation*}
A=\begin{bmatrix} 0&1\\-1&0 \end{bmatrix},
\quad\text{has inverse }A^{-1}=\begin{bmatrix} 0&-1\\1&0 \end{bmatrix}
\end{equation*}
as matrix multiplication confirms.

\end{itemize}
\end{example}



\begin{example} \label{eg:2x2powmat}
The next theorem considers eigenvalues and eigenvectors of powers of a matrix.  
Two examples are the following.
\begin{itemize}
\item Recall the matrix \(A=\begin{bmatrix} 0&1\\-1&0 \end{bmatrix}\) has eigenvalues \(\lambda=\pm i\)\,.  The square of this matrix
\begin{equation*}
A^2=\begin{bmatrix} 0&1\\-1&0 \end{bmatrix}\begin{bmatrix} 0&1\\-1&0 \end{bmatrix}
=\begin{bmatrix} -1&0\\0&-1 \end{bmatrix}
\end{equation*}
is diagonal so its eigenvalues are the diagonal elements (\autoref{eg:eigndiag}), namely the only eigenvalue is~\(-1\).
Observe that \(A^2\)s eigenvalue, \(-1=(\pm i)^2\), is the square of the eigenvalues of~\(A\).
That the eigenvalues of~\(A^2\) are the square of those of~\(A\) holds generally.

\item Also recall matrix 
\begin{equation*}
A=\begin{bmatrix} 1&-\frac12\\-\frac12&1 \end{bmatrix},
\quad\text{has inverse }A^{-1}=\begin{bmatrix} \frac43&\frac23\\\frac23&\frac43 \end{bmatrix}.
\end{equation*}
Let's determine the eigenvalues of this inverse.
Its characteristic equation (defined in \autoref{pro:eeh}) is
\begin{equation*}
\det(A^{-1}-\lambda I)
=\begin{vmatrix} \frac43-\lambda&\frac23\\\frac23&\frac43-\lambda \end{vmatrix}
=(\tfrac43-\lambda)^2-\tfrac49=0\,.
\end{equation*}
That is, \((\lambda-\tfrac43)^2=\tfrac49\)\,.
\marginpar{\eRose1{-0.5}{-0.5}1}
Taking the square-root of both sides gives \(\lambda-\tfrac43=\pm\tfrac23\)\,; that is, the two eigenvalues of the inverse~\(A^{-1}\) are \(\lambda=\tfrac43\pm\tfrac23=2,\tfrac23\)\,.
Observe these eigenvalues of the inverse are the reciprocals of the eigenvalues \(\frac12,\frac32\) of~\(A\).
This reciprocal relation also holds generally.

The marginal pictures illustrates the reciprocal relation graphically:
the first picture shows~\(A\xv\) for various~\xv, the second picture shows \(A^{-1}\xv\).
\marginpar{\eRose{4/3}{2/3}{2/3}{4/3}}
The eigenvector directions are the same for both matrix and inverse.
But in those eigenvector directions where the matrix stretches, the inverse shrinks, and where the matrix shrinks, the inverse stretches.
In contrast, in directions which are not eigenvectors, the  relationship between~\(A\xv\) and~\(A^{-1}\xv\) is somewhat obscure.
\end{itemize}
\end{example}




\begin{theorem} \label{thm:ematpow} 
Let \(A\) be a \idx{square matrix} with \idx{eigenvalue}~\(\lambda\) and corresponding \idx{eigenvector}~\xv.
\begin{enumerate}
\item\label{thm:ematpow:i} For every positive integer~\(n\), \(\lambda^n\) is an \idx{eigenvalue} of~\(A^n\) with corresponding \idx{eigenvector}~\xv.
\item\label{thm:ematpow:ii} If \(A\) is \idx{invertible}, then \(1/\lambda\) is an \idx{eigenvalue} of~\(A^{-1}\) with corresponding \idx{eigenvector}~\xv.
\item\label{thm:ematpow:iii} If \(A\) is \idx{invertible}, then for every integer~\(n\), \(\lambda^n\) is an \idx{eigenvalue} of~\(A^n\) with corresponding \idx{eigenvector}~\xv.
\end{enumerate}
\end{theorem}
\begin{proof} 
Consider each property in turn.
\begin{itemize}
\item[\ref{thm:ematpow:i}.]  
Firstly, the result hold for \(n=1\) by \autoref{def:evecval}, that \(A\xv=\lambda\xv\)\,.
Secondly, for the case \(n=2\)  consider \(A^2\xv=(AA)\xv=A(A\xv)=A(\lambda\xv)=\lambda(A\xv)=\lambda(\lambda\xv)=(\lambda^2)\xv\)\,.  
Hence by definition~\ref{def:evecval} \(\lambda^2\)~is an eigenvalue of~\(A^2\) corresponding to eigenvector~\xv.
Third, use induction to extend to any power: assume the result for \(n=k\) (and proceed to prove it for \(n=k+1\)).
Consider \(A^{k+1}\xv=(A^kA)\xv=A^k(A\xv)=A^k(\lambda\xv)=\lambda(A^k\xv)=\lambda(\lambda^k\xv)=\lambda^{k+1}\xv\)\,.
Hence by \autoref{def:evecval} \(\lambda^{k+1}\)~is an eigenvalue of~\(A^{k+1}\) corresponding to eigenvector~\xv.
By induction the property~\ref{thm:ematpow:i}.\ holds for all integer \(n\geq1\)\,.

\item[\ref{thm:ematpow:ii}.]  
For invertible~\(A\) we know none of the eigenvalues are zero: thus \(1/\lambda\)~exists.
Pre-multiply \(A\xv=\lambda\xv\) by~\(\frac1\lambda A^{-1}\) to deduce
\(\frac1\lambda A^{-1}A\xv=\frac1\lambda A^{-1}\lambda\xv\)\,, which gives \(\frac1\lambda I\xv=\frac1\lambda \lambda A^{-1}\xv\)\,, that is, \(\frac1\lambda\xv= A^{-1}\xv\)\,.  
Consequently, \(1/\lambda\)~is an \idx{eigenvalue} of~\(A^{-1}\) with corresponding \idx{eigenvector}~\xv.

\item[\ref{thm:ematpow:iii}.] Proved by \autoref{ex:ematpow:iii}.  
\end{itemize}
\end{proof}



\begin{example} \label{eg:2x2sympow}
Recall from \autoref{eg:2evals} that matrix 
\begin{equation*}
A=\begin{bmatrix} 1&-\frac12\\-\frac12&1 \end{bmatrix}
\end{equation*}
has eigenvalues~\(1/2\) and~\(3/2\) with corresponding eigenvectors \((1,1)\) and~\((1,-1)\) respectively.
Confirm matrix~\(A^2\) has eigenvalues which are these squared, and corresponding to the same eigenvectors.
\begin{solution} 
Compute
\begin{equation*}
A^2=\begin{bmatrix} 1&-\frac12\\-\frac12&1 \end{bmatrix}
\begin{bmatrix} 1&-\frac12\\-\frac12&1 \end{bmatrix}
=\begin{bmatrix} \frac54&-1\\-1&\frac54 \end{bmatrix}.
\end{equation*}
Then
\begin{eqnarray*}
&&A^2\begin{bmatrix} 1\\1 \end{bmatrix}
=\begin{bmatrix} \frac14\\ \frac14 \end{bmatrix}
=\tfrac14\begin{bmatrix} 1\\1 \end{bmatrix},
\\&&A^2\begin{bmatrix} 1\\-1 \end{bmatrix}
=\begin{bmatrix} \frac94\\-\frac94 \end{bmatrix}
=\tfrac94\begin{bmatrix} 1\\-1 \end{bmatrix},
\end{eqnarray*}
and so \(A^2\) has eigenvalues~\(1/4=(1/2)^2\) and~\(9/4=(3/2)^2\) with the same corresponding eigenvectors~\((1,1)\) and~\((1,-1)\) respectively.
\end{solution}
\end{example}



\begin{activity}
You are given that \(-3\) and~\(2\) are eigenvalues of the matrix \(A=\begin{bmatrix} 1&2\\2&-2 \end{bmatrix}\).
\begin{itemize}
\item Which of the following matrices has an eigenvalue of~\(8\)?
\actposs[4]{\(A^3\)}{\(A^{-2}\)}{\(A^{-1}\)}{\(A^2\)}
%\partswidth=5em
%\begin{parts}
%\item \(A^{-2}\)
%\item \(A^{-1}\)
%\item \(A^2\)
%\item \(A^3\)\actans
%\end{parts}
\item Further, which of the above matrices has eigenvalue~\(1/9\)?
\end{itemize}
\end{activity}






\begin{example} \label{eg:3x3sympow}
Given that the matrix 
\begin{equation*}
A=\begin{bmatrix} 1&1&0\\1&0&1\\0&1&1 \end{bmatrix}
\end{equation*}
has eigenvalues~\(2\), \(1\) and~\(-1\) with corresponding eigenvectors \((1,1,1)\), \((-1,0,1)\) and~\((1,-2,1)\) respectively.
Confirm matrix~\(A^2\) has eigenvalues which are these squared, and corresponding to the same eigenvectors.
Given the inverse
\begin{equation*}
A^{-1}=\begin{bmatrix} \tfrac12&\tfrac12&-\tfrac12
\\\tfrac12&-\tfrac12&\tfrac12
\\-\tfrac12&\tfrac12&\tfrac12 \end{bmatrix}
\end{equation*}
confirm its eigenvalues are the reciprocals of those of~\(A\), and for corresponding eigenvectors.
\begin{solution} 
\begin{itemize}
\item Compute
\begin{equation*}
A^2=\begin{bmatrix} 1&1&0\\1&0&1\\0&1&1 \end{bmatrix}
\begin{bmatrix} 1&1&0\\1&0&1\\0&1&1 \end{bmatrix}
=\begin{bmatrix} 2&1&1\\1&2&1\\1&1&2 \end{bmatrix}.
\end{equation*}
Then
\begin{eqnarray*}
&&A^2\begin{bmatrix} 1\\1\\1 \end{bmatrix}
=\begin{bmatrix} 4\\4\\4 \end{bmatrix}
=4\begin{bmatrix} 1\\1\\1 \end{bmatrix},
\\&&A^2\begin{bmatrix} -1\\0\\1 \end{bmatrix}
=\begin{bmatrix} -1\\0\\1 \end{bmatrix}
=1\begin{bmatrix} -1\\0\\1 \end{bmatrix},
\\&&A^2\begin{bmatrix} 1\\-2\\1 \end{bmatrix}
=\begin{bmatrix} 1\\-2\\1 \end{bmatrix}
=1\begin{bmatrix} 1\\-2\\1 \end{bmatrix},
\end{eqnarray*}
has eigenvalues~\(4=2^2\) and~\(1=(\pm1)^2\) with corresponding eigenvectors \((1,1,1)\), and the pair \((-1,0,1)\) and~\((1,-2,1)\).
Thus here \(\Span\{(-1,0,1),(1,-2,1)\}\) is the eigenspace of~\(A^2\) corresponding to eigenvalue one.

\item For the inverse
\begin{eqnarray*}
&&A^{-1}\begin{bmatrix} 1\\1\\1 \end{bmatrix}
=\begin{bmatrix} \frac12\\\frac12\\\frac12 \end{bmatrix}
=\frac12\begin{bmatrix} 1\\1\\1 \end{bmatrix},
\\&&A^{-1}\begin{bmatrix} -1\\0\\1 \end{bmatrix}
=\begin{bmatrix} -1\\0\\1 \end{bmatrix}
=1\begin{bmatrix} -1\\0\\1 \end{bmatrix},
\\&&A^{-1}\begin{bmatrix} 1\\-2\\1 \end{bmatrix}
=\begin{bmatrix} -1\\2\\-1 \end{bmatrix}
=(-1)\begin{bmatrix} 1\\-2\\1 \end{bmatrix},
\end{eqnarray*}
has eigenvalues~\(1/2\), \(1=1/1\) and \(-1=1/(-1)\) with corresponding eigenvectors \((1,1,1)\),  \((-1,0,1)\) and~\((1,-2,1)\).

\end{itemize} 
\end{solution}
\end{example}




\begin{example}[long term age structure] \label{eg:ltas}
Recall \autoref{eg:matasp} introduced how to use a \idx{Leslie matrix} to predict the future population of an animal.
In the example, letting \(\xv=(x_1,x_2,x_3)\) be the current number of pups, juveniles, and mature females respectively, then for the Leslie matrix
\begin{equation*}
L=\begin{bmatrix} 0&0&4\\\frac12&0&0\\0&\frac13&\frac13 \end{bmatrix}
\end{equation*} 
the predicted population numbers after a year is \(\xv'=L\xv\), after two years is \(\xv''=L\xv'=L^2\xv\), and so on.
Predict what happens after many generations: does the population die out? grow? oscillate? 
\begin{solution} 
Consider what happens after \(n\)~generations for large~\(n\), say \(n=10\) or~\(100\).
The predicted population is \(\xv^{(n)}=L^n\xv\)\,; that is, the matrix~\(L^n\) transforms the current population to that after \(n\)~generations.
The stretching and/or shrinking of matrix~\(L^n\) is summarised by its eigenvectors and eigenvalues (\autoref{sec:iee}).
By \autoref{thm:ematpow} the eigenvalues of~\(L^n\) are~\(\lambda^n\) in terms of the eigenvalues~\(\lambda\) of~\(L\).
By hand (\autoref{pro:eeh}), the characteristic equation of~\(L\) is
\begin{eqnarray*}
\det(L-\lambda I)
&=&\begin{vmatrix} -\lambda&0&4\\\frac12&-\lambda&0\\0&\frac13&\frac13-\lambda \end{vmatrix}
\\&=&\lambda^2(\tfrac13-\lambda)+0+\tfrac23-0-0-0
\\&=&(1-\lambda)(\lambda^2 +\tfrac23\lambda+\tfrac23)
\\&=&(1-\lambda)\big[(\lambda+\tfrac13)^2+\tfrac59\big]=0
\\\implies&&\lambda=1,(-1\pm i\sqrt5)/3\,.
\end{eqnarray*}
Such complex valued eigenvalues may arise in real applications when the matrix is not symmetric, as here---the next \autoref{thm:realeigsym} proves such complexities do not arise for symmetric matrices.

But the algebra still works with \idx{complex eigenvalue}s (\autoref{ch:gee}).
Here, the eigenvalues of~\(L^n\) are~\(\lambda^n\) (\autoref{thm:ematpow}) namely \(1^n=1\) for all~\(n\) and \([(-1\pm i\sqrt5)/3]^n\).
Because the absolute value \(|(-1\pm i\sqrt5)/3|
=|-1\pm i\sqrt5|/3=\sqrt{1+5}/3=\sqrt{6}/3=\sqrt{2/3}=0.8165\), then the absolute value of \([(-1\pm i\sqrt5)/3]^n\) is~\(0.8165^n\) which becomes negligibly small for large~\(n\); for example, \(0.8165^{34}\approx 0.001\).
Since the eigenvectors of~\(L^n\) are the same as those of~\(L\) (\autoref{thm:ematpow}), these negligibly small eigenvalues of~\(L^n\) imply that any component in the initial population in the direction of the corresponding eigenvectors is shrunk to zero by~\(L^n\).
For large~\(n\), it is only the component in the eigenvector corresponding to eigenvalue \(\lambda=1\) that remains.
Find the eigenvector by solving \((L-I)\xv=\ov\), namely
\begin{equation*}
\begin{bmatrix} -1&0&4\\\frac12&-1&0\\0&\frac13&-\frac23 \end{bmatrix}\xv=\begin{bmatrix} -x_1+4x_3\\ \frac12x_1-x_2\\\frac13x_2-\frac23x_3 \end{bmatrix}=\ov\,.
\end{equation*}
The first row gives that \(x_1=4x_3\)\,, the third row that \(x_2=2x_3\)\,, and the second row confirms these are correct as \(\frac12x_1-x_2=\frac124x_3-2x_3=0\)\,.
Eigenvectors corresponding to \(\lambda=1\) are then of the form \((4x_3,2x_3,x_3)=(4,2,1)x_3\)\,.
Because the corresponding eigenvalue of \(L^n=1^n=1\) the component of~\xv\ in this direction remains in~\(L^n\xv\) whereas all other components decay to zero.
Thus the model predicts that after many generations the population reaches a steady state of the pups, juveniles, and mature females being in the ratio of \(4:2:1\)\,.
\end{solution}
\end{example}




\subsection{Symmetric matrices are orthogonally diagonalisable}
\label{sec:smod}

General matrices may have complex valued eigenvalues (as in Examples~\ref{eg:ccevals} and~\ref{eg:ltas}): that real symmetric matrices always have real eigenvalues (such as in all matrices of Examples~\ref{eg:2x2sympow} and~\ref{eg:3x3sympow}) is a special property that often reflects the physical reality of many applications.

To establish the reality of eigenvalues (\autoref{thm:realeigsym}), the proof invokes contradiction. 
The contradiction is to assume a complex valued eigenvalue exists, and then prove it cannot.
Consequently, the proof of the next \autoref{thm:realeigsym} needs to use some \idx{complex numbers} and some properties of complex numbers.
Recall that any complex number \(z=a+bi\) has a \idx{complex conjugate} \(\bar z=a-bi\) (denoted by the overbar), and that a complex number equals its conjugate only if it is real valued (the imaginary part is zero).
Such properties of complex numbers and operations also hold for complex valued vectors, complex valued matrices, and arithmetic operations with complex valued matrices and vectors.


\begin{comment}
Have not yet thought of reasonable Activities for this subsection.
\end{comment}




\begin{theorem} \label{thm:realeigsym} 
For every real \idx{symmetric matrix}~\(A\), the \idx{eigenvalue}s of~\(A\) are all real.
\end{theorem}


\begin{proof} 
Let \(\lambda\) be any eigenvalue of real matrix~\(A\) with corresponding eigenvector~\xv; that is, \(A\xv=\lambda\xv\) for \(\xv\neq\ov\)\,.
To create a \idx{contradiction} that establishes the theorem: assume the eigenvalue~\(\lambda\) is complex valued, and so correspondingly is the eigenvector~\xv.
First,  the complex conjugate~\(\bar\lambda\) must be another eigenvalue of~\(A\), corresponding to an eigenvector which is the complex conjugate~\(\bar\xv\).
To see this just take the complex conjugate of both sides of \(A\xv=\lambda\xv\)\,:
\begin{equation*}
\overline{A\xv}=\overline{\lambda\xv}
\implies \bar A\bar\xv=\bar\lambda\bar\xv
\implies A\bar\xv=\bar\lambda\bar\xv
\end{equation*}
as matrix~\(A\) is real (\(\bar A=A\)).
Second, consider \(\tr\xv A\bar\xv\) in two different ways:
\begin{eqnarray*}
\tr\xv A\bar\xv&=&\tr\xv(A\bar\xv)
=\tr\xv(\bar A\bar\xv)\quad\text{(as \(A\) is real)}
\\&&{}=\tr\xv(\overline{ A\xv})
=\tr\xv(\overline{ \lambda\xv})
=\tr\xv(\bar\lambda\bar\xv)
=\bar\lambda\tr\xv\bar\xv\,;
\\\tr\xv A\bar\xv&=&(\tr\xv A)\bar\xv
=\tr{(\tr A\xv)}\bar\xv
=\tr{(A\xv)}\bar\xv
\quad\text{(by symmetry)}
\\&&{}=\tr{(\lambda\xv)}\bar\xv
=\lambda\tr\xv\bar\xv\,.
\end{eqnarray*}
Equating the two ends of this identity gives \(\bar\lambda\tr\xv\bar\xv=\lambda\tr\xv\bar\xv\)\,.
Rearrange to \(\bar\lambda\tr\xv\bar\xv-\lambda\tr\xv\bar\xv
=0\)\,, which factors to \((\bar\lambda-\lambda)\tr\xv\bar\xv
=0\)\,.
Because this product is zero, either \(\bar\lambda-\lambda=0\) or \(\tr\xv\bar\xv=0\)\,.
But we next prove the second is impossible, hence the first must hold; that is, \(\bar\lambda-\lambda=0\)\,, equivalently \(\bar\lambda=\lambda\)\,.
Consequently, the eigenvalue~\(\lambda\) must be real---it cannot be complex.

Lastly confirm \(\tr\xv\bar\xv\neq0\)\,.  
The nonzero eigenvector~\xv\ will be generally complex, say 
\begin{eqnarray*}
&&\xv=(a_1+b_1i,a_2+b_2i,\ldots,a_n+b_ni)
\\&\implies&\bar\xv=(a_1-b_1i,a_2-b_2i,\ldots,a_n-b_ni).
\end{eqnarray*}
Then the product
\begin{eqnarray*}
\tr\xv\bar\xv&=&
\begin{bmatrix} a_1+b_1i&a_2+b_2i&\cdots&a_n+b_ni \end{bmatrix}
\begin{bmatrix} a_1-b_1i\\a_2-b_2i\\\vdots\\a_n-b_ni \end{bmatrix}
\\&=&(a_1+b_1i)(a_1-b_1i)+(a_2+b_2i)(a_2-b_2i)
\\&&{}+\cdots+(a_n+b_ni)(a_n-b_ni)
\\&=&(a_1^2+b_1^2)+(a_2^2+b_2^2)+\cdots+(a_n^2+b_n^2)
\\&>&0
\end{eqnarray*}
since \xv~is an eigenvector which necessarily is nonzero and so at least one term in the sum is positive.
\end{proof}


The other property that we have seen graphically for 2D matrices is that the eigenvectors of symmetric matrices are orthogonal.
In \autoref{eg:2x2powmat}, both the matrices~\(A\) and~\(A^{-1}\) in the second part are symmetric and from the marginal illustration their eigenvectors are proportional to \((1,1)\) and~\((-1,1)\) which are orthogonal directions---they are at right angles in the illustration.

\begin{example} \label{eg:}
Recall \autoref{eg:3x3hande} found the \(3\times3\) symmetric matrix
\begin{equation*}
\begin{bmatrix} -2&0&-6\\0&4&6\\-6&6&-9 \end{bmatrix}
\end{equation*}
has eigenspaces \(\EE_0=\Span\{(-6,-3,2)\}\), \(\EE_7=\Span\{(-2,6,3)\}\) and \(\EE_{-14}=\Span\{(3,-2,6)\}\).
These eigenspaces are orthogonal as evidenced by the dot products of the basis vectors:
\begin{eqnarray*}
\EE_0,\EE_7,&&
(-6,-3,2)\cdot(-2,6,3)=12-18+6=0\,;
\\
\EE_7,\EE_{-14},&&
(-2,6,3)\cdot(3,-2,6)=-6-12+18=0\,;
\\
\EE_{-14},\EE_0,&&
(3,-2,6)\cdot(-6,-3,2)=-18+6+12=0\,.
\end{eqnarray*}
\end{example}




\begin{theorem} \label{thm:orthoevec} 
Let \(A\) be a real \idx{symmetric matrix}, then for every two distinct \idx{eigenvalue}s of~\(A\), any corresponding two \idx{eigenvector}s are \idx{orthogonal}.
\end{theorem}
\begin{proof} 
Let eigenvalues \(\lambda_1\neq\lambda_2\)\,, and let \(\xv_1\) and~\(\xv_2\) be corresponding eigenvectors, respectively; that is, \(A\xv_1=\lambda_1\xv_1\) and \(A\xv_2=\lambda_2\xv_2\)\,.
Consider \(\tr\xv_1A\xv_2\) in two different ways:
\begin{eqnarray*}
\tr\xv_1A\xv_2
&=&\tr\xv_1(A\xv_2)
=\tr\xv_1(\lambda_2\xv_2)
=\lambda_2\tr\xv_1\xv_2
=\lambda_2\xv_1\cdot\xv_2\,;
\\\tr\xv_1A\xv_2&=&\tr\xv_1\tr A\xv_2\quad\text{(as \(A\) is symmetric)}
\\&&{}=(\tr\xv_1\tr A)\xv_2
=\tr{(A\xv_1)}\xv_2
\\&&{}=\tr{(\lambda_1\xv_1)}\xv_2
=\lambda_1\tr\xv_1\xv_2
=\lambda_1\xv_1\cdot\xv_2\,.
\end{eqnarray*}
Equating the two ends of this identity gives \(\lambda_2\xv_1\cdot\xv_2=\lambda_1\xv_1\cdot\xv_2\)\,.
Rearrange to \(\lambda_2\xv_1\cdot\xv_2-\lambda_1\xv_1\cdot\xv_2
=(\lambda_2-\lambda_1)(\xv_1\cdot\xv_2)=0\)\,.
Since \(\lambda_1\neq\lambda_2\)\,, the factor \(\lambda_2-\lambda_1\neq0\)\,, and so it follows that the dot product \(\xv_1\cdot\xv_2=0\)\,.
Hence (\autoref{def:orthovec}) the two eigenvectors are orthogonal.
\end{proof}



\begin{example} \label{eg:}
The plots below shows (unit) vectors~\xv\ (blue), and for some matrix~\(A\) (different for different plots) the corresponding vectors~\(A\xv\) (red) adjoined. 
By estimating eigenvectors determine which cases \emph{cannot} be the plot of a real symmetric matrix.
\begin{parts}
%\item \eRose{1.96}{-0.44}{0.56}{0.88}
%\begin{solution} 
%Estimate eigenvectors \((0.8,0.6)\) and \((0.5,0.9)\) which are not orthogonal, so cannot be from a symmetric matrix 
%\end{solution}

\item \eRose{-0.02}{-0.65}{-0.65}{0.61}
\begin{solution} 
Estimate eigenvectors \((0.8,0.5)\) and \((-0.5,0.8)\) which are  orthogonal, so may be a symmetric matrix 
\end{solution}

%\item \eRose{0.57}{-0.88}{-0.88}{0.82}
%\begin{solution} 
%Estimate eigenvectors \((0.8,0.7)\) and \((-0.7,0.8)\) which are orthogonal, so may be a symmetric matrix 
%\end{solution}

\item \eRose{0.57}{-1.71}{-0.06}{0.82}
\begin{solution} 
Estimate eigenvectors \((1,0.1)\) and \((1,-0.3)\) which are not orthogonal, so cannot be from a symmetric matrix 
\end{solution}

\item \eRose{-0.02}{-1.05}{-0.25}{0.61}
\begin{solution} 
Estimate eigenvectors \((1,0.2)\) and \((0.8,-0.7)\) which are not orthogonal, so cannot be from a symmetric matrix 
\end{solution}

\item \eRose{1.96}{0.06}{0.06}{0.88}
\begin{solution} 
Estimate eigenvectors \((0.1,-1)\) and \((1,0.1)\) which are orthogonal, so may be a symmetric matrix 
\end{solution}

\end{parts}
%\begin{comment} try some alternatives
%\begin{verbatim}
%for i=1:30
%a=eye(2)/2+randn(2); b=(a+a')/2;
%[v,d]=eig(a); d=diag(d);
%cost=abs(v(:,1)'*v(:,2));
%if norm(imag(d))==0 & cost>1/2 & d>-1/2, disp([a(:)';b(:)']), end
%end
%\end{verbatim}
%\end{comment}
\end{example}



\begin{example} \label{eg:2x2orthevec}
By hand find eigenvectors corresponding to the two distinct eigenvalues of the following matrices.
Confirm that symmetric matrix~\(A\) has orthogonal eigenvectors, and that non-symmetric matrix~\(B\) does not:
\begin{equation*}
A=\begin{bmatrix} 1&\frac32\\\frac32&-3 \end{bmatrix};\quad
B=\begin{bmatrix} 0&-3\\-2&1 \end{bmatrix}.
\end{equation*}

\begin{solution} 
\begin{itemize}
\item For matrix~\(A\), the eigenvalues come from the characteristic equation
\begin{eqnarray*}
\det(A-\lambda I)&=&(1-\lambda)(-3-\lambda)-\tfrac94
\\&=&\lambda^2+2\lambda-\tfrac{21}4
=(\lambda+1)^2-\tfrac{25}4=0\,,
\end{eqnarray*}
so eigenvalues are \(\lambda=-1\pm\tfrac52=-\tfrac72,\tfrac32\)\,.
\begin{itemize}
\item Corresponding to eigenvalue \(\lambda=-7/2\), eigenvectors~\xv\ satisfy \((A+\tfrac72I)\xv=\ov\)\,, that is
\begin{equation*}
\begin{bmatrix} \frac92&\frac32\\\frac32&\frac12 \end{bmatrix}\xv
=\frac12\begin{bmatrix} 9x_1+3x_2\\3x_1+x_2 \end{bmatrix}
=\begin{bmatrix} 0\\0 \end{bmatrix},
\end{equation*}
giving \(x_2=-3x_1\)\,.  
Eigenvectors must be \(\xv\propto(1,-3)\).
\item Corresponding to eigenvalue \(\lambda=3/2\), eigenvectors~\xv\ satisfy \((A-\tfrac32I)\xv=\ov\)\,, that is
\begin{equation*}
\begin{bmatrix} -\frac12&\frac32\\\frac32&-\frac92 \end{bmatrix}\xv
=\frac12\begin{bmatrix} -x_1+3x_2\\3x_1-9x_2 \end{bmatrix}
=\begin{bmatrix} 0\\0 \end{bmatrix},
\end{equation*}
giving \(x_1=3x_2\)\,.  
Eigenvectors must be \(\xv\propto(3,1)\).
\end{itemize}
The dot product of the two basis eigenvectors is \((1,-3)\cdot(3,1)=3-3=0\) and hence they are orthogonal.

\item For matrix~\(B\), the eigenvalues come from the characteristic equation
\begin{eqnarray*}
\det(B-\lambda I)&=&(-\lambda)(1-\lambda)-6
\\&=&\lambda^2-\lambda-6
=(\lambda-3)(\lambda+2)=0\,,
\end{eqnarray*}
so eigenvalues are \(\lambda=3,-2\)\,.
\begin{itemize}
\item Corresponding to eigenvalue \(\lambda=-2\), eigenvectors~\xv\ satisfy \((B+2I)\xv=\ov\)\,, that is
\begin{equation*}
\begin{bmatrix} 2&-3\\-2&3 \end{bmatrix}\xv
=\begin{bmatrix} 2x_1-3x_2\\-2x_1+3x_2 \end{bmatrix}
=\begin{bmatrix} 0\\0 \end{bmatrix},
\end{equation*}
giving \(x_2=\tfrac23x_1\)\,.  Eigenvectors must be \(\xv\propto(3,2)\).
\item Corresponding to eigenvalue \(\lambda=3\), eigenvectors~\xv\ satisfy \((B-3I)\xv=\ov\)\,, that is
\begin{equation*}
\begin{bmatrix} -3&-3\\-2&-2 \end{bmatrix}\xv
=\begin{bmatrix} -3x_1-3x_2\\-2x_1-2x_2 \end{bmatrix}
=\begin{bmatrix} 0\\0 \end{bmatrix},
\end{equation*}
giving \(x_1=-x_2\)\,.  Eigenvectors must be \(\xv\propto(-1,1)\).
\end{itemize}
The dot product of the two basis eigenvectors is \((3,2)\cdot(-1,1)=-3+2=-1\neq 0\) and hence they are not orthogonal.

\end{itemize}
\end{solution}
\end{example}



\begin{example} \label{eg:4x4orthevec}
Use \script\ to compute eigenvectors of the following matrices.
Confirm the eigenvectors are orthogonal for a symmetric matrix.
\begin{parts}
\item \(\begin{bmatrix} 0 & 3 & 2 & -1
\\ 0 & 3 & 0 & 0
\\ 3 & 0 & -1 & -1
\\ -3 & 1 & 3 & 0 \end{bmatrix}\)
  
\item \(\begin{bmatrix} -6 & 0 & 1 & 1
\\ 0 & 0 & 2 & 2
\\ 1 & 2 & 2 & -1
\\ 1 & 2 & -1 & -1 \end{bmatrix}\)
\end{parts}
\begin{solution} 
For each matrix, enter the matrix as say~\verb|A|, then execute \verb|[V,D]=eig(A)| to give eigenvectors as the columns of~\verb|V|. 
Then confirm orthogonality of all pairs of eigenvectors by computing~\verb|V'*V| and confirming the off-diagonal dot products are zero, or confirm the lack of orthogonality if non-zero.
(In the case of repeated eigenvalues, \script\ generates an orthonormal basis for the corresponding eigenspace so the returned matrix~\verb|V| of eigenvectors is still orthogonal for symmetric~\verb|A|.)
\begin{enumerate}
\item The \script\ code 
\setbox\ajrqrbox\hbox{\qrcode{% non-orthogonality
A=[0 3 2 -1
 0 3 0 0
 3 0 -1 -1
 -3 1 3 0]
[V,D]=eig(A)
V'*V
}}%
\marginpar{\usebox{\ajrqrbox}}%
\begin{verbatim}
A=[0 3 2 -1
 0 3 0 0
 3 0 -1 -1
 -3 1 3 0]
[V,D]=eig(A)
V'*V
\end{verbatim}
gives the following \twodp
\begin{verbatim}
V =
  -0.49  -0.71   0.41   0.74
   0.00   0.00   0.00   0.34
   0.32  -0.71   0.41   0.57
  -0.81  -0.00   0.82  -0.06
D =
  -3.00      0      0      0
      0   2.00      0      0
      0      0   0.00      0
      0      0      0   3.00
\end{verbatim}
so eigenvectors corresponding to the four distinct eigenvalues are
\sloppy
\((-0.49,0,0.32,-0.81)\), \((-0.71,0,-0.71,0)\), \((0.41,0,0.41,0.82)\) and \((0.74,0.34,0.57,-0.6)\).
Then \verb|V'*V| is \twodp
\begin{verbatim}
   1.00   0.11  -0.73  -0.13
   0.11   1.00  -0.58  -0.93
  -0.73  -0.58   1.00   0.49
  -0.13  -0.93   0.49   1.00
\end{verbatim}
As the off-diagonal elements are non-zero,  the pairs of dot products are non-zero indicating the column vectors are not orthogonal.  
Hence the matrix~\verb|A| cannot be symmetric.

\item The \script\ code 
\setbox\ajrqrbox\hbox{\qrcode{% orthogonality
B=[-6 0 1 1
 0 0 2 2
 1 2 2 -1
 1 2 -1 -1]
[V,D]=eig(B)
V'*V
}}%
\marginpar{\usebox{\ajrqrbox}}%
\begin{verbatim}
B=[-6 0 1 1
 0 0 2 2
 1 2 2 -1
 1 2 -1 -1]
[V,D]=eig(B)
V'*V
\end{verbatim}
gives the following \twodp
\begin{verbatim}
V =
   0.94   0.32  -0.04  -0.10
   0.13  -0.63  -0.53  -0.55
  -0.17   0.32   0.43  -0.83
  -0.25   0.63  -0.73  -0.08
D =
  -6.45      0      0      0
      0  -3.00      0      0
      0      0   1.11      0
      0      0      0   3.34
\end{verbatim}
so eigenvectors corresponding to the four distinct eigenvalues are
\((0.94,0.13,-0.17,-0.25)\), \((0.32,-0.63,0.32,0.63)\), \((-0.04,-0.53,0.43,-0.73)\) and \((-0.10,-0.55,-0.83,-0.08)\).
Then \verb|V'*V| is \twodp
\begin{verbatim}
   1.00  -0.00  -0.00  -0.00
  -0.00   1.00   0.00  -0.00
  -0.00   0.00   1.00  -0.00
  -0.00  -0.00  -0.00   1.00
\end{verbatim}
As the off-diagonal elements are zero, the pairs of dot products are zero indicating the column vectors are orthogonal.  
The symmetry of this matrix~\verb|B| requires such orthogonality.
\end{enumerate}
\end{solution}
%\begin{comment}
%\begin{verbatim}
%n=4
%for i=1:10
%a=2*randn(n); a=round((a+a')/2); 
%[v,d]=eig(a);
%if imag(d)==zeros(n), A=a,V=v,D=d, end
%end
%\end{verbatim}
%\end{comment}
\end{example}





Recall that to find eigenvalues by hand for \(2\times2\) or \(3\times 3\) matrices we solve a quadratic or cubic characteristic equation, respectively.
Thus we find at most two or three eigenvalues, respectively.
Further, when we ask \script\ to compute eigenvalues of an \(n\times n\) matrix, it always returns \(n\)~eigenvalues in an \(n\times n\) diagonal matrix.


\begin{theorem} \label{thm:lenlam}
Every \(n\times n\) real \idx{symmetric matrix}~\(A\) has at most \(n\)~distinct \idx{eigenvalue}s.
\end{theorem}
\begin{proof} 
Invoke the \idx{pigeonhole principle} and \idx{contradiction}. 
Assume there are more than \(n\)~distinct eigenvalues, then there would be more than \(n\)~eigenvectors corresponding to distinct eigenvalues.
\autoref{thm:orthoevec} asserts all such eigenvectors are orthogonal. 
But there cannot be more than \(n\)~vectors in an orthogonal set in~\(\RR^n\) (\autoref{thm:orthcomp}).
Hence the assumption is wrong: there cannot be any more than \(n\)~distinct eigenvalues.
\end{proof}


The previous theorem establishes there are at most \(n\)~distinct \idx{eigenvalue}s (here for symmetric matrices, but \autoref{thm:geecp} establishes it is true for general matrices).  
Now we establish that typically there exist \(n\)~distinct \idx{eigenvalue}s of an \(n\times n\) matrix---here symmetric.

\begin{comment}
The problem case of repeated singular values and repeated eigenvalues appears to be not adequately dealt with by any first textbook that I have read.
\pooliv{p.294} discusses algebraic and geometric {multiplicity}, but best left for eigenproblem of general matrices, if necessary.
\end{comment}



\autoref{eg:symsigns} started this chapter by observing that in an \svd\ of a symmetric matrix, \(A=\usv\), the columns of~\(U\) appear to be (almost) always plus\slash minus the corresponding columns of~\(V\).
Exceptions possibly arise in the degenerate cases when two or more singular values are identical.
We now prove this close relation between~\(U\) and~\(V\) in all non-degenerate cases.


\begin{theorem} \label{thm:smevec}
Let \(A\) be an \(n\times n\) real \idx{symmetric matrix} with \svd\ \(A=\usv\).
If all the \idx{singular value}s are distinct or \idx{zero}, \(\sigma_1>\cdots>\sigma_r>\sigma_{r+1}=\cdots=\sigma_n=0\)\,, then \(\vv_j\)~is an \idx{eigenvector} of~\(A\) corresponding to an \idx{eigenvalue} of either \(\lambda_j=+\sigma_j\) or \(\lambda_j=-\sigma_j\) (not both).
\end{theorem}

If non-zero singular values are duplicated, then one can always choose an \svd\ so the result of this theorem still holds.
However, the proof is too involved to give here.

\begin{aside} This proof modifies parts of the proof of the \svd\ \autoref{thm:svd} to the specific case of a symmetric matrix.
\end{aside}
\begin{proof} 
First, for any zero singular value, \(\sigma_j=0\)\,, then the result is immediate as from the \svd, \(AV=US\)\, the \(j\)th~column gives \(A\vv_j=0\uv_j=\ov=0\vv_j\) for the nonzero~\(\vv_j\).

Second, for the singular values \(\sigma_j>0\) we use a form of \idx{induction} allied with a \idx{contradiction} to prove \(\uv_j=\pm \vv_j\).
By \idx{contradiction}, suppose \(\uv_1\neq\pm\vv_1\)\,, then we can write \(\uv_1=\vv_1\cos t+\wv\sin t\) for some vector~\wv\ orthogonal to~\(\vv_1\) (\(\wv:=\Perp_{\vv_1}\uv_1\)) and for angle \(0< t<\pi\) (as illustrated in the margin).
\marginpar{\begin{tikzpicture}
  \begin{axis}[tiny,font=\footnotesize
  ,axis equal,axis lines=none,samples=2, thick ]
  \addplot[black,mark=*]coordinates {(0,0)};
  \addplot[red,quiver={u=12/13,v=5/13},-stealth]coordinates {(0,0)};
  \node[below] at (axis cs:0.923,0.385) {$\vec v_1$};
  \addplot[blue,quiver={u=3/5,v=4/5},-stealth]coordinates {(0,0)};
  \node[right] at (axis cs:0.6,0.8) {$\vec u_1$};
  \addplot[brown,thick,quiver={u=-0.385,v=0.923},-stealth]coordinates {(0,0)};
  \node[right] at (axis cs:-0.385,0.923) {$\vec w$};
  \node at (axis cs:0.3,0.24) {$t$};
  \end{axis}
\end{tikzpicture}}
Multiply by~\(A\) giving the identity \(A\uv_1=A\vv_1\cos t+A\wv\sin t\)\,.
Now the first column of \(AV=US\) gives \(A\vv_1=\sigma_1\uv_1\)\,.
Also for the symmetric matrix \(A=\tr A=\tr{(\usv)}=V\tr S\tr U=VS\tr U\) is an alternative \svd\ of~\(A\):  
so \(AU=VS\) giving in its first column \(A\uv_1=\sigma_1\vv_1\)\,.
That is, the identity becomes \(\sigma_1\vv_1=\sigma_1\uv_1\cos t+(A\wv)\sin t\)\,.
Further, \(A\wv\)~is orthogonal to~\(\uv_1\) by the proof of the \svd\ \autoref{thm:svd} (since \wv~is orthogonal to~\(\vv_1\)).  
Equate the lengths of both sides: \(\sigma_1^2=\sigma_1^2\cos^2 t+|A\wv|^2\sin^2 t\) which rearranging implies \((\sigma_1^2-|A\wv|^2) \sin^2 t=0\)\,.
For angles \(0< t<\pi\) this implies \(|A\wv|=\sigma_1\) for a vector orthogonal to~\(\vv_1\) which implies the singular value~\(\sigma_1\) is repeated.  
This contradicts the supposition; hence \(\uv_1=\pm\vv_1\) (for one of the signs, not both).

Recall the \idx{induction} proof for the \svd\ \autoref{thm:svd}  (\autoref{sec:dpsvdt}).  
Here, since \(\uv_1=\pm\vv_1\) we can and do choose \(\bar U=\bar V\)\,.  
Hence \(B=\tr{\bar U}A\bar V =\tr{\bar V}A\bar V \) is symmetric as \(\tr{B} =\tr{\tr{\bar V}A\bar V} =\tr{\bar V}\tr A\bar V =\tr{\bar V}A\bar V =B\)\,.
Consequently, the same argument applies at all steps in the induction for the proof of an \svd\ and hence establishes \(\uv_j=\pm\vv_j\) (each for one of the signs, not both).

Third and lastly,  from the \(j\)th~column of \(AV=US\)\,, \(A\vv_j=\sigma_j\uv_j=\sigma_j(\pm \vv_j)=\lambda_j\vv_j\) for \idx{eigenvalue}~\(\lambda_j\) one of~\(\pm \sigma_j\) but not both.
\end{proof}

\begin{comment}
For duplicated \idx{singular value}s, there are \idx{eigenvector}s in the span of the \idx{singular vector}s.  
But appears difficult to prove.  
Could come down to solutions of \(Q^2=I\) for orthogonal matrix~\(Q\).
Equivalent to proving the Fundamental Theorem of Algebra that there are \(n\)~zeros of an \(n\)th~degree polynomial \pooliv{p.D8}.
\end{comment}


Recall that for any real matrix~\(A\) an \svd\ is \(A=\usv\).
But specifically for symmetric~\(A\), the proof of the previous theorem~\ref{thm:smevec} identified that the columns of~\(US\),~\(\sigma_j\uv_j\), are generally the same as~\(\lambda_j\vv_j\) and hence are the columns of~\(VD\) where \(D=\diag(\hlist\lambda n)\).
In which case the \svd\ becomes \(A=VD\tr V\) and so is intimately connected to the following definition.


\begin{definition} \label{def:odsble} 
A real \idx{square matrix}~\(A\) is \bfidx{orthogonally diagonalisable} if there exists an \idx{orthogonal matrix}~\(V\) and a \idx{diagonal matrix}~\(D\) such that \(\tr VAV=D\), equivalently \(AV=VD\), equivalently \(A=VD\tr V\) is a factorisation of~\(A\).
\end{definition}

The equivalences in this definition arise immediately from the orthogonality of matrix~\(V\) (\autoref{def:orthog}): pre-multiply \(\tr VAV=D\) by~\(V\) gives \(V\tr VAV=AV=VD\); and so on.

\begin{example} \label{eg:}
\begin{enumerate}
\item\label{eg:2x2orthdiag} Recall from \autoref{eg:2x2orthevec} that the symmetric matrix \(A=\begin{bmatrix} 1&\frac32\\\frac32&-3 \end{bmatrix}\) has eigenvalues \(\lambda=-\tfrac72,\tfrac32\) with corresponding orthogonal eigenvectors \((1,-3)\) and~\((3,1)\).
Normalise these eigenvectors to unit length as the columns of the orthogonal matrix
\begin{eqnarray*}
V&=&\begin{bmatrix} \frac1{\sqrt{10}} & \frac3{\sqrt{10}}
\\ -\frac3{\sqrt{10}}& \frac1{\sqrt{10}} \end{bmatrix}
=\frac1{\sqrt{10}}\begin{bmatrix} 1&3\\-3&1 \end{bmatrix}
\quad\text{then}
\\\tr VAV&=&\frac1{\sqrt{10}}\begin{bmatrix} 1&-3\\3&1 \end{bmatrix}
\begin{bmatrix} 1&\frac32\\\frac32&-3 \end{bmatrix}
\frac1{\sqrt{10}}\begin{bmatrix} 1&3\\-3&1 \end{bmatrix}
\\&=&\frac1{10}\begin{bmatrix} -\frac72&\frac{21}2\\\frac92&\frac32 \end{bmatrix}
\begin{bmatrix} 1&3\\-3&1 \end{bmatrix}
\\&=&\frac1{10}\begin{bmatrix} -35&0\\0&15 \end{bmatrix}
=\begin{bmatrix} -\frac72&0\\0&\frac32 \end{bmatrix}.
\end{eqnarray*}
Hence this matrix is orthogonally diagonalisable.

\item Recall from \autoref{eg:4x4orthevec} that the symmetric matrix
\begin{equation*}
B=\begin{bmatrix} -6 & 0 & 1 & 1
\\ 0 & 0 & 2 & 2
\\ 1 & 2 & 2 & -1
\\ 1 & 2 & -1 & -1 \end{bmatrix}
\end{equation*}
has orthogonal eigenvectors computed by \script\ into the orthogonal matrix~\verb|V|.
By additionally computing \verb|V'*B*V| we get the following diagonal result \twodp
\setbox\ajrqrbox\hbox{\qrcode{% orthogonality
B=[-6 0 1 1
 0 0 2 2
 1 2 2 -1
 1 2 -1 -1]
[V,D]=eig(B)
V'*B*V
}}%
\marginpar{\usebox{\ajrqrbox}}%
\begin{verbatim}
ans =
  -6.45   0.00   0.00   0.00
   0.00  -3.00   0.00  -0.00
   0.00   0.00   1.11  -0.00
  -0.00  -0.00  -0.00   3.34
\end{verbatim}
and see that this matrix~\(B\) is orthogonally diagonalisable.
\end{enumerate}
\end{example}



These examples of orthogonal diagonalisation invoke symmetric matrices.
Also, the connection between an \svd\ and orthogonal matrices was previously discussed only for symmetric matrices. 
The next theorem establishes that all real symmetric matrices are orthogonally diagonalisable, and vice versa.
That is, eigenvectors of a matrix form an orthogonal set if and only if the matrix is symmetric.
 

\begin{theorem}[spectral] \label{thm:symspec} 
For every real \idx{square matrix}~\(A\), 
matrix~\(A\) is symmetric iff it is \idx{orthogonally diagonalisable}.
\end{theorem}

\begin{proof} 
The ``if'' and the ``only if'' lead to two parts in the proof.
\begin{itemize}
\item If matrix~\(A\) is orthogonally diagonalisable, then \(A=VD\tr V\) for orthogonal~\(V\) and diagonal~\(D\) (and recall that for a diagonal matrix, \(\tr D=D\)).
Consider
\begin{equation*}
\tr A=\tr{(VD\tr V)}
=\tr{{\tr V}}\tr D\tr V
=VD\tr V=A\,.
\end{equation*}
Consequently the matrix~\(A\) is symmetric.

\item \autoref{thm:smevec} establishes the converse for the generic case of distinct singular values.
If matrix~\(A\) is symmetric, then \autoref{thm:smevec} asserts an \svd\ \(A=\usv\) has matrix~\(U\) such that columns \(\uv_j=\pm\vv_j\)\,.
That is, we can write \(U=VR\) for diagonal matrix \(R=\diag(\pm1,\pm1,\ldots,\pm1)\) for appropriately chosen signs.
Then by the \svd\ \(A=\usv=VRS\tr V=VD\tr V\) for diagonal matrix \(D=RS=\diag(\pm\sigma_1,\pm\sigma_2,\ldots,\pm\sigma_n)\) for the same pattern of signs.
Hence matrix~\(A\) is orthogonally diagonalisable.
\end{itemize}
We omit proving the degenerate case when non-zero singular values are repeated.
\end{proof}

\begin{comment}
Further material could include the following---maybe exercises.
For every \idx{symmetric matrix}, \(A^k=VD^k\tr V\).
Also, every \idx{square matrix} has a \idx{polar decomposition} \(A=RQ\) for symmetric positive semi-definite~\(R\) and orthogonal~\(Q\) (since \(A=\usv=(US\tr U)(U\tr V)\)) \pooliv{p.610}.  
\cite{Higham86} mentions applications to the Orthogonal Procrustes problem (perhaps in approx matrices), Aerospace, Optimisation, Matrix square root, but these look too hard for this level.
\end{comment}







\subsection{Change orthonormal basis to classify quadratics}
\label{sec:cobcqs}


The following preliminary example illustrates the important principle, 
applicable throughout mathematics, that we often either choose or change to a coordinate system in which the mathematical algebra is simplest.
\begin{aside}
This is optional subsection which has many uses---although not an application itself as it does not involve real data.
\end{aside}


\begin{example}[choose useful coordinates] \label{eg:cuceh}
\newcommand{\myellipse}[1]{\rotatebox{30}{
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-1.2, ymax=1.5
  , xlabel={$x$}, ylabel={$y$}
  ,axis lines=#1
  ]
  \addplot[blue,thick,smooth,domain=0:360] ({2*cos(x)},{sin(x)});
  \end{axis}
\end{tikzpicture}}}%
\newcommand{\myhyper}[1]{\rotatebox{-30}{
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.7, ymin=-3, ymax=3.5
  , xlabel={$x$}, ylabel={$y$}
  ,axis lines=#1
  ]
  \addplot[blue,thick,smooth,domain=-2:2] {sqrt(1+x^2*2)};
  \addplot[blue,thick,smooth,domain=-2:2] {-sqrt(1+x^2*2)};
  \end{axis}
\end{tikzpicture}}}%
Consider the following two quadratic curves. 
For each curve draw a coordinate system in which the algebraic description of the curve would be most straightforward.
\begin{parts}
\item Ellipse \myellipse{none}
\item Hyperbola \myhyper{none}
\end{parts}

\begin{solution} Among various possibilities are the following.
\begin{parts}
\item Ellipse \myellipse{middle}

In this coordinate system the ellipse is algebraically \((x/2)^2+y^2=1\)\,.
\item Hyperbola \myhyper{middle}

In this coordinate system the hyperbola is algebraically \(y^2=1+2x^2\).
\end{parts}
\end{solution}
\end{example}

Now let's proceed to see how to implement in algebra this geometric idea of choosing good coordinates to fit a given physical curve. 


\subsubsection{Graph quadratic equations}
\label{sec:gqe}

\autoref{eg:cuceh} illustrated an ellipse and a hyperbola.  These curves are examples of the so-called \bfidx{conic section}s which arise as solutions of the \idx{quadratic equation} in two variables, say~\(x\) and~\(y\),
\begin{equation}
ax^2+bxy+cy^2+dx+ey+f=0 \label{eq:gqe}
\end{equation}
(where \(a,b,c\) cannot all be zero).
As invoked in the example, the canonical simplest algebraic form of such curves are the following.
The challenge of this subsection is to choose good new coordinates so that a given quadratic equation~\eqref{eq:gqe} becomes one of these recognised canonical forms.
\begin{description}
\item[Ellipse or circle] \(\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\)
\begin{itemize}
\item ellipse \(a>b\) \begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-1.2, ymax=1.5
  , xlabel={$x$}, ylabel={$y$}
  ,xtick={-2,2},xticklabels={$-a$,$a$}
  ,ytick={-1,1},yticklabels={$-b$,$b$}
  ,axis lines=middle  ]
  \addplot[thick,blue,smooth,domain=0:360] ({2*cos(x)},{sin(x)});
  \end{axis}
\end{tikzpicture}

\item the circle \(a=b\) \begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-1.2, xmax=1.5, ymin=-1.2, ymax=1.5
  , xlabel={$x$}, ylabel={$y$}
  ,xtick={-1,1},xticklabels={$-a$,$a$}
  ,ytick={-1,1},yticklabels={$-a$,$a$}
  ,axis lines=middle  ]
  \addplot[thick,blue,smooth,domain=0:360] ({cos(x)},{sin(x)});
  \end{axis}
\end{tikzpicture}

\item ellipse \(a<b\) \begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-1.3, xmax=1.5, ymin=-1.7, ymax=1.9
  , xlabel={$x$}, ylabel={$y$}
  ,xtick={-1,1},xticklabels={$-a$,$a$}
  ,ytick={-1.5,1.5},yticklabels={$-b$,$b$}
  ,axis lines=middle  ]
  \addplot[thick,blue,smooth,domain=0:360] ({cos(x)},{1.5*sin(x)});
  \end{axis}
\end{tikzpicture}

\end{itemize}


\item[Hyperbola] \(\frac{x^2}{a^2}-\frac{y^2}{b^2}=1\) or  \(-\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\)
\begin{itemize}
\item \(\frac{x^2}{a^2}-\frac{y^2}{b^2}=1\)  \begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5
  , xlabel={$x$}, ylabel={$y$}
  ,xtick={-1,1},yticklabels={{},{}}
  ,ytick={-1,1},xticklabels={$-a$,$a$}
  ,axis lines=middle ]
  \addplot[thick,blue,smooth,domain=-2:2] ({sqrt(1+x^2)},{x});
  \addplot[thick,blue,smooth,domain=-2:2] ({-sqrt(1+x^2)},{x});
  \end{axis}
\end{tikzpicture}

\item \(-\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\)  \begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5
  , xlabel={$x$}, ylabel={$y$}
  ,xtick={-1,1},xticklabels={{},{}}
  ,ytick={-1,1},yticklabels={$-b$,$b$}
  ,axis lines=middle ]
  \addplot[thick,blue,smooth,domain=-2:2] {sqrt(1+x^2)};
  \addplot[thick,blue,smooth,domain=-2:2] {-sqrt(1+x^2)};
  \end{axis}
\end{tikzpicture}

\end{itemize}

\item[Parabola] \(y=ax^2\) or  \(x=ay^2\)
\begin{itemize}
\item \(y=ax^2\)
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5
  , xlabel={$x$}, ylabel={$y$}, xtick=\empty, ytick=\empty
  ,axis lines=middle, legend pos=outer north east ]
  \addplot[thick,blue,smooth,domain=-2:2] {x^2};
  \addlegendentry{$a>0$};
  \addplot[thick,red,smooth,domain=-2:2] {-x^2};
  \addlegendentry{$a<0$};
  \end{axis}
\end{tikzpicture}

\item \(x=ay^2\)
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5
  , xlabel={$x$}, ylabel={$y$}, xtick=\empty, ytick=\empty
  ,axis lines=middle, legend pos=outer north east ]
  \addplot[thick,blue,smooth,domain=-2:2] ({x^2},{x});
  \addlegendentry{$a>0$};
  \addplot[thick,red,smooth,domain=-2:2] ({-x^2},{x});
  \addlegendentry{$a<0$};
  \end{axis}
\end{tikzpicture}

\end{itemize}
\end{description}


\autoref{eg:cuceh} implicitly has two steps: first, we decide upon an orientation for the coordinate axes; second, we decides that the coordinate system should be `centred' in the picture.
Algebra follows the same two steps.

\begin{example}[centre coordinates] \label{eg:}
By shifting coordinates, identify the conic section whose equation is
\begin{equation*}
2x^2+y^2-4x+4y+2=0\,.
\end{equation*}
\begin{solution} 
Group the linear terms with corresponding quadratic powers and seek to rewrite as a perfect square: the equation is
\begin{eqnarray*}&&
(2x^2-4x)+(y^2+4y)+2=0
\\\iff&&
2(x^2-2x)+(y^2+4y)=-2
\\\iff&&
2(x^2-2x+1)+(y^2+4y+4)=-2+2+4
\\\iff&&
2(x-1)^2+(y+2)^2=4\,.
\end{eqnarray*}
Thus changing to a new (dashed) coordinate system \(x'=x-1\) and \(y'=y+2\)\,, that is, choosing the origin of the dashed coordinate system at \((x,y)=(1,-2)\), the quadratic equation becomes
\marginpar{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle
  ]
  \addplot[blue,smooth,thick,domain=0:360] ({1+1.414*cos(x)},{-2+2*sin(x)});
  \addplot[brown,quiver={u=3.45,v=0},-stealth] coordinates {(-0.5,-2)};
  \node[brown,above] at (axis cs:2.7,-2) {$x'$};
  \addplot[brown,quiver={u=0,v=5},-stealth] coordinates {(1,-4.2)};
  \node[brown,right] at (axis cs:1,0.5) {$y'$};
  \end{axis}
\end{tikzpicture}}%
\begin{equation*}
2{x'}^2+{y'}^2=4\,, \quad\text{that is, }
\frac{{x'}^2}{2}+\frac{{y'}^2}{4}=1\,.
\end{equation*}
In this new coordinate system the equation is that of an ellipse with horizontal axis of half-length~\(\sqrt2\) and vertical axis of half-length~\(\sqrt4=2\) (as illustrated in the margin).
\end{solution}
\end{example}


\begin{example}[rotate coordinates] \label{eg:csrc2}
By rotating the coordinate system, identify the conic section whose equation is
\begin{equation*}
x^2+3xy-3y^2-\tfrac12=0\,.
\end{equation*}
(There are no terms linear in~\(x\) and~\(y\) so we do not shift coordinates.)
\begin{solution} 
The equation contains the product~\(xy\).
To identify the conic we must eliminate the \(xy\)~term.
To use matrix algebra, and in terms of the vector \(\xv=(x,y)\), recognise that the quadratic terms may be written as \(\tr\xv A\xv\) for symmetric matrix
\begin{eqnarray*}
A=\begin{bmatrix} 1&\frac32\\\frac32&-3 \end{bmatrix}
&&\text{as then}
\\\tr\xv A\xv
&=&\tr\xv\begin{bmatrix} 1&\frac32\\\frac32&-3 \end{bmatrix}\xv
\\&=&\begin{bmatrix} x&y \end{bmatrix}\begin{bmatrix} x+\frac32y\\\frac32x-3y \end{bmatrix}
\\&=&x(x+\tfrac32y)+y(\tfrac32x-3y)
\\&=&x^2+3xy-3y^2.
\end{eqnarray*}
(The matrix form \(\tr\xv A\xv\) splits the cross-product term~\(3xy\) into two equal halves represented by the two off-diagonal elements~\(\tfrac32\) in matrix~\(A\)).
\def\temp#1{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle
  , xtick={-1,1,2}, ytick={-2,-1,1,2}  ]
  \addplot[brown,quiver={u=1,v=-3},-stealth] coordinates {(-1/3,3/3)};
  \node[brown,right] at (axis cs:0.566,-1.7) {$x'$};
  \addplot[brown,quiver={u=0.316,v=-0.949},-stealth,thick] coordinates {(0,0)};
  \node[brown,right] at (axis cs:0.316,-0.949) {$\vec v_1$};
  \addplot[brown,quiver={u=3,v=1},-stealth] coordinates {(-3/3,-1/3)};
  \node[brown,above] at (axis cs:1.7,0.566) {$y'$};
  \addplot[brown,quiver={u=0.949,v=0.316},-stealth,thick] coordinates {(0,0)};
  \node[brown,above] at (axis cs:0.949,0.316) {$\vec v_2$};
  \ifnum#1=1
  \addplot[blue,thick,domain=-1.5:1.5] ({(x+3*sqrt(2*0.5/3+7*x^2/3))/sqrt(10)}
  ,{(-3*x+sqrt(2*0.5/3+7*x^2/3))/sqrt(10)});
  \addplot[blue,thick,domain=-1:1] ({(x-3*sqrt(2*0.5/3+7*x^2/3))/sqrt(10)}
  ,{(-3*x-sqrt(2*0.5/3+7*x^2/3))/sqrt(10)});
  \fi
  \end{axis}
\end{tikzpicture}}%
\marginpar{\temp0}%
Suppose we change to some new (dashed) coordinate system with its standard unit vectors~\(\vv_1\) and~\(\vv_2\) as illustrated in the margin.
The vectors in the plane will be written as the linear combination \(\xv=\vv_1x'+\vv_2y'\).
That is, \(\xv=V\xv'\) for new coordinate vector \(\xv'=(x',y')\) and matrix \(V=\begin{bmatrix} \vv_1&\vv_2 \end{bmatrix}\).

In the new coordinate system, related to the old by \(\xv=V\xv'\), the quadratic terms 
\begin{equation*}
\tr\xv A\xv
=\tr{(V\xv')} A(V\xv')
=\tr{\xv'}\tr VAV\xv'
=\tr{\xv'}(\tr VAV)\xv'.
\end{equation*}
Thus choose~\(V\) to simplify \(\tr VAV\).
Because matrix~\(A\) is symmetric, \autoref{thm:symspec} asserts it is orthogonally diagonalisable (using eigenvectors).
Indeed, \autoref{eg:2x2orthdiag} orthogonally diagonalised this particular matrix~\(A\), via its eigenvalues and eigenvectors, using the orthogonal matrix
\begin{equation*}
V=\begin{bmatrix} \vv_1&\vv_2 \end{bmatrix}
=\begin{bmatrix} \frac1{\sqrt{10}} & \frac3{\sqrt{10}}
\\ -\frac3{\sqrt{10}}& \frac1{\sqrt{10}} \end{bmatrix}.
\end{equation*}
Using this~\(V\), in the new dashed coordinate system (illustrated) the quadratic terms in the equation become
\marginpar{\temp1}%
\begin{equation*}
\tr{\xv'}(\tr VAV)\xv'
=\tr{\xv'}D\xv'
=\tr{\xv'}\begin{bmatrix} -\tfrac72&0\\0&\tfrac32 \end{bmatrix}\xv'
=-\tfrac72{x'}^2+\tfrac32{y'}^2
\end{equation*}
Hence the quadratic equation becomes
\begin{equation*}
-\tfrac72{x'}^2+\tfrac32{y'}^2-\tfrac12=0
\iff -7{x'}^2+3{y'}^2=1
\end{equation*}
which is the equation of a hyperbola intersecting the \(y'\)-axis at \(y'=\pm1/\sqrt3\)\,, as illustrated in the margin.
\end{solution}
\end{example}





\begin{example} \label{eg:cstrrc2}
Identify the conic section whose equation is
\begin{equation*}
x^2-xy+y^2+\tfrac5{2\sqrt2}x-\tfrac7{2\sqrt2}y+\tfrac18=0\,.
\end{equation*}
\begin{solution} 
When there are both the cross-product~\(xy\) and linear terms, it is  easier to first rotate coordinates, and second shift coordinates. 
\begin{enumerate}
\item Rewrite the quadratic terms using vector \(\xv=(x,y)\) and splitting the cross-product into two equal halfs:
\begin{eqnarray*}
x^2-xy+y^2
&=&x(x-\tfrac12y)+y(-\tfrac12x+y)
\\&=&\begin{bmatrix} x&y \end{bmatrix}
\begin{bmatrix} x-\tfrac12y\\-\tfrac12x+y \end{bmatrix}
\\&=&\tr\xv 
\begin{bmatrix} 1&-\tfrac12\\-\tfrac12&1 \end{bmatrix}
\begin{bmatrix} x\\y \end{bmatrix}
\\&=&\tr\xv A\xv
\quad\text{for matrix }A=
\begin{bmatrix} 1&-\tfrac12\\-\tfrac12&1 \end{bmatrix}.
\end{eqnarray*}
Recall that \autoref{eg:2evals} found the eigenvalues of this matrix are \(\lambda=\tfrac12,\tfrac32\) with corresponding orthonormal eigenvectors \(\vv_1=(1,1)/\sqrt2\) and \(\vv_2=(-1,1)/\sqrt2\), respectively.
\def\temp#1{\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  , xlabel={$x$}, ylabel={$y$}, axis lines=middle
  , xtick={-2,-1,1,2}, ytick={-2,-1,1,2}
  ]
  \addplot[brown,quiver={u=3,v=3},-stealth] coordinates {(-1,-1)};
  \node[brown,left] at (axis cs:2,2) {$x'$};
  \addplot[brown,quiver={u=0.707,v=0.707},-stealth,thick] coordinates {(0,0)};
  \node[brown,right] at (axis cs:0.707,0.707) {$\vec v_1$};
  \addplot[brown,quiver={u=-3.2,v=3.2},-stealth] coordinates {(1,-1)};
  \node[brown,below] at (axis cs:-2,2) {$y'$};
  \addplot[brown,quiver={u=-0.707,v=0.707},-stealth,thick] coordinates {(0,0)};
  \node[brown,left] at (axis cs:-0.707,0.707) {$\vec v_2$};
  \ifnum#1>0
  \addplot[red,quiver={u=3.2,v=3.2},-stealth] coordinates {(-0.354-1.5,1.061-1.5)};
  \node[red,below] at (axis cs:1.246,2.661) {$x''$};
  \addplot[red,quiver={u=-3.2,v=3.2},-stealth] coordinates {(-0.354+1.8,1.061-1.8)};
  \node[red,right] at (axis cs:-1.554,2.261) {$y''$};
  \fi
  \ifnum#1>1
  \addplot[blue,smooth,thick,domain=0:360] 
  ({-0.354+0.707*sqrt(3)*cos(x)-0.707*sin(x)}
  ,{ 1.061+0.707*sqrt(3)*cos(x)+0.707*sin(x)});
  \fi
  \end{axis}
\end{tikzpicture}}%
\marginpar{\temp0}%
Let's change to a new (dashed) coordinate system~\((x',y')\) with \(\vv_1\)~and~\(\vv_2\) as its standard unit vectors (as illustrated in the margin). 
Then throughout the 2D-plane every vector\slash position
\begin{equation*}
\xv=\vv_1x'+\vv_2y'=\begin{bmatrix} \vv_1&\vv_2 \end{bmatrix}\begin{bmatrix} x'\\y' \end{bmatrix}=V\xv'
\end{equation*}
for orthogonal matrix 
\begin{equation*}
V=\begin{bmatrix} \vv_1&\vv_2 \end{bmatrix}
=\frac1{\sqrt2}\begin{bmatrix} 1&-1\\1&1 \end{bmatrix}.
\end{equation*}
In the new coordinates: 
\begin{itemize}
\item the quadratic terms
\begin{eqnarray*}
x^2-xy+y^2&=&\tr\xv A\xv
\\&=&\tr{(V\xv')}A(V\xv')
\\&=&\tr{\xv'}\tr VAV\xv'
\\&=&\tr{\xv'}\begin{bmatrix} \tfrac12&0\\0&\tfrac32 \end{bmatrix}\xv'
\quad(\text{as }\tr VAV=D)
\\&=&\tfrac12{x'}^2+\tfrac32{y'}^2;
\end{eqnarray*}
\item whereas the linear terms
\begin{eqnarray*}
\tfrac5{2\sqrt2}x-\tfrac7{2\sqrt2}y
&=&\begin{bmatrix} \tfrac5{2\sqrt2}&-\tfrac7{2\sqrt2} \end{bmatrix}\xv
\\&=&\begin{bmatrix} \tfrac5{2\sqrt2}&-\tfrac7{2\sqrt2} \end{bmatrix}V\xv'
\\&=&\begin{bmatrix} -\tfrac12&-3 \end{bmatrix}\xv'
\\&=&-\tfrac12x'-3y';
\end{eqnarray*}
\item so the quadratic equation transforms to
\begin{equation*}
\tfrac12{x'}^2+\tfrac32{y'}^2-\tfrac12x'-3y'+\tfrac18=0\,.
\end{equation*}
\end{itemize}

\item The second step is to shift coordinates via completing the squares:
\begin{eqnarray*}
&&\tfrac12{x'}^2+\tfrac32{y'}^2-\tfrac12x'-3y'+\tfrac18=0
\\\iff&&
\tfrac12({x'}^2-x')+\tfrac32({y'}^2-2y')=-\tfrac18
\\\iff&&
\tfrac12({x'}^2-x'+\tfrac14)+\tfrac32({y'}^2-2y'+1)=-\tfrac18+\tfrac18+\tfrac32
\\\iff&&
\tfrac12(x'-\tfrac12)^2+\tfrac32(y'-1)^2=\tfrac32
\end{eqnarray*}
\marginpar{\temp1}%
Thus let's change to a new (double dashed) coordinate system \(x''=x'-\tfrac12\) and \(y''=y'-1\) (equivalently, choose the origin of a new coordinate system to be at \((x',y')=(\tfrac12,1)\) as illustrated in the margin).
In this new coordinate system the quadratic equation becomes
\begin{equation*}
\tfrac12{x''}^2+\tfrac32{y''}^2=\tfrac32,
\quad\text{that is } \frac{{x''}^2}3+\frac{{y''}^2}1=1\,.
\end{equation*}
\marginpar{\temp2}%
In this new coordinate system the equation is that of an ellipse with \(x''\)-axis of half-length~\(\sqrt3\) and \(y''\)-axis of half-length~\(1\) (as illustrated in the margin).
\end{enumerate}
\end{solution}
\end{example}




\begin{comment}
Should include classification of 3D surfaces, but write later. 
Further motivate the following quadratic forms by mentioning some more applications.
\end{comment}





\subsubsection{Simplify quadratic forms}

To understand the response and strength of built structures like bridges, buildings and cars, engineers need to analyse the dynamics of energy distribution in the structure.
The potential energy in such structures is expressed and analysed as the following quadratic form.
Such quadratic forms are also important in distinguishing maxima from minima in economic optimisation.

\begin{definition} \label{def:qufo}
A \bfidx{quadratic form} in variables \(\xv\in\RR^n\) is a function \(q:\RR^n\to\RR\) that may be written as \(q(\xv)=\tr\xv A\xv\) for some real symmetric \(n\times n\) matrix~\(A\).
\end{definition}

\begin{example} \label{eg:}
\begin{enumerate}
\item The dot product of a vector with itself is a quadratic form.
For all \(\xv\in\RR^n\) consider
\begin{equation*}
\xv\cdot\xv=\tr\xv\xv=\tr\xv I_n\xv\,,
\end{equation*}
which is the quadratic form associated with the identity matrix~\(I_n\).

\item \autoref{eg:csrc2} found the hyperbola satisfying equation \(x^2+3xy-3y^2-\tfrac12=0\)\,.
This equation may be written in terms of a quadratic form as \(\tr\xv A\xv-\tfrac12=0\) for vector \(\xv=(x,y)\) and symmetric matrix \(A=\begin{bmatrix} 1&\tfrac32\\\tfrac32&-3 \end{bmatrix}\).

\item \autoref{eg:cstrrc2} found the ellipse satisfying the equation
\(x^2-xy+y^2+\tfrac5{2\sqrt2}x-\tfrac7{2\sqrt2}y+\tfrac18=0\) via writing the quadratic part of the equation as \(\tr\xv A\xv\) for vector \(\xv=(x,y)\) and symmetric matrix \(A=\begin{bmatrix} 1&-\tfrac12\\-\tfrac12&1 \end{bmatrix}\).
\end{enumerate}
\end{example}


\begin{theorem}[principal axes theorem] \label{thm:patqform}
For every \idx{quadratic form}, there exists an \idx{orthogonal} coordinate system that diagonalises the quadratic form.
Specifically, for the quadratic form~\(\tr\xv A\xv\) find the \idx{eigenvalue}s~\hlist\lambda n\ and \idx{orthonormal} \idx{eigenvector}s~\hlist\vv n\ of symmetric~\(A\), and then in the new coordinate system~\((\hlist yn)\) with \idx{unit vector}s \(\{\hlist\vv n\}\) the quadratic form has the \bfidx{canonical form} \(\tr\xv A\xv=\lambda_1y_1^2+\lambda_2y_2^2+\cdots+\lambda_ny_n^2\).
\end{theorem}

\begin{proof} 
In the new coordinate system~\((\hlist yn)\) the orthonormal vectors~\hlist\vv n\ (called the \bfidx{principal axes}) act as the standard unit vectors.
Hence any vector \(\xv\in\RR^n\) may be written as a linear combination
\begin{equation*}
\xv=\lincomb y\vv n
=\begin{bmatrix} \vv_1&\vv_2&\cdots&\vv_n \end{bmatrix}\begin{bmatrix} y_1\\y_2\\\vdots\\y_n \end{bmatrix}
=V\yv
\end{equation*}
for orthogonal matrix \(V=\begin{bmatrix} \vv_1&\vv_2&\cdots&\vv_n \end{bmatrix}\) and vector \(\yv=(\hlist yn)\).
Then the quadratic form 
\begin{equation*}
\tr\xv A\xv=\tr{(V\yv)} A(V\yv)=\tr\yv \tr VAV\yv=\tr\yv D\yv\,,
\end{equation*}
since \(\tr VAV=D=\diag(\hlist\lambda n)\) by \autoref{thm:symspec}.
Consequently,
\begin{equation*}
\tr\xv A\xv=\tr\yv D\yv=\lambda_1y_1^2+\lambda_2y_2^2+\cdots+\lambda_ny_n^2.
\end{equation*}
\end{proof}



\begin{comment}
Maybe applications to moment of inertia? and the rotating lunch box?
extreme stresses in a solid?
Could link to SVD and extreme values of length \(|A\xv|\): but where is it best established?
\end{comment}





\begin{example} \label{eg:}
Consider the quadratic form \(f(x,y)=x^2+3xy-3y^2\). 
That is, consider \(f(\xv)=\tr\xv A\xv\) for \(\xv=(x,y)\) and matrix \(A=\begin{bmatrix} 1&3/2\\3/2&-3 \end{bmatrix}\).
The following illustration plots the surface~\(f(x,y)\).
\begin{center}
\qview{28}{33}{\begin{tikzpicture} 
\begin{axis}[footnotesize,font=\footnotesize,view={\q}{60}
    ,domain=-1.3:1.3,zmin=-5
    ,xlabel={$x$},ylabel={$y$},label shift={-1.5ex}]
    \addplot3[surf,opacity=0.5] {x^2+3*x*y-3*y^2};
    \addplot3[samples y=0,domain=0:360,smooth,thick] 
    ({cos(x)},{sin(x)},{cos(x)^2+3*cos(x)*sin(x)-3*sin(x)^2});
    \addplot3[samples y=0,domain=0:360,smooth] 
    ({cos(x)},{sin(x)},{-5});
\end{axis}
\end{tikzpicture}}
\end{center}
Also plotted in black is the curve of values of~\(f(x,y)\) on the unit circle \(x^2+y^2=1\) (also shown); that is, \(f(\xv)\)~for unit vectors~\xv.
Find the maxima and minima of~\(f\) on this unit circle (for unit vectors~\xv).
Relate to the eigenvalues of \autoref{eg:2x2orthevec}.
\begin{solution} 
Let's express the unit vectors as \(\xv=(\cos t,\sin t)\) and consider~\(f\) as a function of~\(t\).
Then 
\begin{eqnarray*}
f(t)&=&x^2+3xy-3y^2\quad(\text{for }x=\cos t\text{ and }y=\sin t)
\\&=&\cos^2t+3\cos t\sin t-3\sin^2t
\\&=&\tfrac12+\tfrac12\cos2t+\tfrac32\sin2t -\tfrac32+\tfrac32\cos2t
\\&=&-1+2\cos2t+\tfrac32\sin2t\,.
\end{eqnarray*}
From calculus, maxima and minima occur when the derivative is zero, \(df/dt=0\)\,.
Here \(df/dt=-4\sin2t+3\cos2t\) which to be zero requires \(4\sin2t=3\cos2t\)\,, that is, \(\tan2t=\tfrac34\)\,.
From the classic Pythagorean triplet of the \(3:4:5\) triangle, this tangent being~\(3/4\) requires either \begin{itemize}
\item \(\cos2t=\tfrac45\) and \(\sin2t=\tfrac35\) giving the function \(f(t)=-1+2\cdot\tfrac45+\tfrac32\cdot\tfrac35
=\tfrac32=1.5\)\,,
\item or the negative \(\cos2t=-\tfrac45\) and \(\sin2t=-\tfrac35\) giving the function \(f(t)=-1-2\cdot\tfrac45-\tfrac32\cdot\tfrac35
=-\tfrac72=-3.5\)\,.
\end{itemize}
These maximum and minimum values of~\(f\) seem reasonable from the plot.
We observe that these extreme values are precisely the two eigenvalues of the matrix~\(A\)---the next theorem shows this connection is no accident.
\end{solution}
\end{example}





\begin{theorem}%[Rayleigh's inequality] %?
\label{thm:optqform}
Let~\(A\) be an \(n\times n\) symmetric matrix with \idx{eigenvalue}s \(\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n\) (sorted). 
Then for all \idx{unit vector}s \(\xv\in\RR^n\) (that is, \(|\xv|=1\)), the \idx{quadratic form}~\(\tr\xv A\xv\) has the following properties:
\begin{enumerate}
\item \(\lambda_1\leq \tr\xv A\xv\leq\lambda_n\)\,;
\item the minimum of~\(\tr\xv A\xv\) is~\(\lambda_1\), and occurs when \(\xv\) is a unit eigenvector corresponding to~\(\lambda_1\);
\item the maximum of~\(\tr\xv A\xv\) is~\(\lambda_n\), and occurs when \(\xv\) is a unit eigenvector corresponding to~\(\lambda_n\).
\end{enumerate}
\end{theorem}

\begin{proof} 
Change to an orthogonal coordinate system~\yv\ that diagonalises the matrix~\(A\) (\autoref{thm:patqform}): say coordinates \(\yv=V\xv\) for orthogonal matrix~\(V\) whose columns are orthogonal eigenvectors of~\(A\) in order so that \(D=\tr VAV=\diag(\hlist\lambda n)\).
Then the quadratic form 
\begin{equation*}
\tr\xv A\xv=\tr{(V\yv)} A(V\yv)=\tr\yv \tr VAV\yv=\tr\yv D\yv\,.
\end{equation*}
Since \(V\)~is orthogonal it preserves lengths (\autoref{thm:orthog:iv}) so the unit vector condition \(|\xv|=1\) is the same as \(|\yv|=1\)\,.
\begin{enumerate}
\item To prove the lower bound, consider
\begin{eqnarray*}
\tr\xv A\xv&=&\tr\yv D\yv
\\&=&\lambda_1y_1^2+\lambda_2y_2^2+\cdots+\lambda_ny_n^2
\\&=&\lambda_1y_1^2+\lambda_1y_2^2+\cdots+\lambda_1y_n^2
\\&&\quad{}
+\underbrace{(\lambda_2-\lambda_1)y_2^2}_{\geq0}
+\cdots+\underbrace{(\lambda_n-\lambda_1)y_n^2}_{\geq0}
\\&\geq&\lambda_1y_1^2+\lambda_1y_2^2+\cdots+\lambda_1y_n^2
\\&&{}=\lambda_1(y_1^2+y_2^2+\cdots+y_n^2)
\\&&{}=\lambda_1|\yv|^2
\\&&{}=\lambda_1\,.
\end{eqnarray*}
Similarly for the upper bound (\autoref{ex:optqform}).  
Thus \(\lambda_1\leq \tr\xv A\xv\leq\lambda_n\) for all unit vectors~\xv.

\item Let \(\vv_1\) be a unit eigenvector of~\(A\) corresponding to the minimal eigenvalue~\(\lambda_1\); that is, \(A\vv_1=\lambda_1\vv_1\) and \(|\vv_1|=1\)\,.
Then, setting \(\xv=\vv_1\)\,, the quadratic form
\begin{equation*}
\tr\xv A\xv=\tr\vv_1A\vv_1
=\tr\vv_1\lambda_1\vv_1
=\lambda_1(\tr\vv_1\vv_1)
=\lambda_1|\vv_1|^2
=\lambda_1\,.
\end{equation*}
Thus the quadratic form~\(\tr\xv A\xv\) takes on the minimum value~\(\lambda_1\) and it occurs when \(\xv=\vv_1\) (at least).

\item \autoref{ex:optqform} proves the maximum value occurs.
\end{enumerate}
\end{proof}


\begin{activity}
Recall \autoref{eg:3x3hande} found that the \(3\times3\) symmetric matrix
\begin{equation*}
A=\begin{bmatrix} -2&0&-6\\0&4&6\\-6&6&-9 \end{bmatrix}
\end{equation*}
has eigenvalues \(7\), \(0\) and~\(-14\).
\begin{itemize}
\item What is the maximum of the quadratic form \(\tr\xv A\xv\) over unit vectors~\xv?
\actposs[4]{\(7\)}{\(-14\)}{\(0\)}{\(14\)}
%\partswidth=5em
%\begin{parts}
%\item \(-14\)
%\item \(0\)
%\item \(7\)
%\item \(14\)
%\end{parts}
\item Further, what is the minimum of the quadratic form \(\tr\xv A\xv\) over unit vectors~\xv?
\end{itemize}
\end{activity}



\begin{comment}
Could define positive/negative semi-definite forms/matrices.
But seems incidental so do not write yet.
\end{comment}




\begin{comment} \label{ex:} 
Could have an optional application section on the following problem: useful whenever we can assess any pair of items, but do not have a framework for their absolute position.
Find \(m\) points in \(n\)D given that you know their distance
matrix and given that the mean location is precisely zero
\cite[p.83]{Hopcroft2014}. 
See \verb|multiDscaling.m|
\end{comment}







\subsection{Exercises}


\begin{exercise} \label{ex:} 
Each plot below shows (unit) vectors~\xv\ (blue), and for some \(2\times 2\) matrix~\(A\) the corresponding vectors~\(A\xv\) (red) adjoined. 
By assessing whether there are any zero eigenvalues, estimate if the matrix~\(A\) is invertible or not.

\begin{parts}
%\def\eRosesize{small}
\item\eRose{1}{-0.8}{-0.8}{1.5}%sym
\answer{invertible}

\item\eRose{0.5}{0.4}{1}{0.8}
\answer{not invertible}

\item\eRose{-0.5}{0.2}{1.3}{0.4}
\answer{invertible}

\item\eRose{0.2}{-0.5}{-0.4}{1.0}%sym
\answer{not invertible}

\item\eRose{1.0}{-0.25}{-1.2}{0.3}
\answer{not invertible}

\item\eRose{-0.2}{0.2}{-0.3}{-0.45}
\answer{invertible}

\end{parts}
\end{exercise}


\begin{exercise} \label{ex:} 
For each of the following symmetric matrices: from a hand derivation of the characteristic equation (defined in \autoref{pro:eeh}), determine whether each matrix has a zero eigenvalue or not, and hence determine whether it is invertible or not.
\begin{parts}
\item \(\begin{bmatrix} -1/2 & -3/4
\\ -3/4 & -1/2 \end{bmatrix}\)
\answer{eigenvalues \(-5/4,1/4\) so invertible.}

\item \(\begin{bmatrix} 4 & -2
\\ -2 & 1 \end{bmatrix}\)
\answer{eigenvalues \(0,5\) so not invertible.}

\item \(\begin{bmatrix} 0 & -2/5
\\ -2/5 & 3/5 \end{bmatrix}\)
\answer{eigenvalues \(-1/5,4/5\) so invertible.}

\item \(\begin{bmatrix} 2 & 1 & -2
\\ 1 & 3 & -1
\\ -2 & -1 & 2 \end{bmatrix}\)
\answer{eigenvalues \( 0,2,5\) so not invertible.}

\item \(\begin{bmatrix} -2 & -1 & 1
\\ -1 & -0 & -1
\\ 1 & -1 & -2 \end{bmatrix}\)
\answer{eigenvalues \(-3,-2,1\) so invertible.}

\item \(\begin{bmatrix} 2 & 1 & 1
\\ 1 & 2 & 1
\\ 1 & 1 & 2 \end{bmatrix}\)
\answer{eigenvalues \(1,4\) so invertible.}

\item \(\begin{bmatrix} -1/2 & 3/2 & 1
\\ 3/2 & -3 & -3/2
\\ 1 & -3/2 & -1/2 \end{bmatrix}\)
\answer{eigenvalues \(-9/2,0,1/2\) so not invertible.}

\item \(\begin{bmatrix} 1 & -1 & -1
\\ -1 & -1/2 & 1/2
\\ -1 & 1/2 & -1/2 \end{bmatrix}\)
\answer{eigenvalues \(-1,2\) so invertible.}

\end{parts}
%\begin{comment}
%obtain via brutal search
%\begin{verbatim}
%format rat
%n=3
%for i=1:10^(n-1)
%k=ceil(5*rand);
%a=eye(n)/2+randn(n); a=round((a+a')*k)/k;
%d=eig(a)'; [dn,dd]=rat(d,1e-7);
%if max(abs(dd))<10, AD=[a;d], end
%end
%\end{verbatim}
%\end{comment}
\end{exercise}





\begin{exercise} \label{ex:smoevs} 
For each of the following matrices, find by hand the eigenvalues and eigenvectors.  
Using these eigenvectors, confirm that the eigenvalues of the matrix squared are the square of its eigenvalues.
If the matrix has an inverse, what are the eigenvalues of the inverse?
%\begin{verbatim}
%format rat,n=3
%for i=1:10^n
%k=ceil(2*rand); a=eye(n)/2+randn(n); a=0+round((a+a')*k)/k;
%[v,d]=eig(a); [dn,dd]=rat(diag(d),1e-7);
%if max(abs(dd))<10, A=a, d=diag(d)', vd=diag(1./max(abs(v)))*(v'), break, end
%end
%\end{verbatim}
\begin{parts}
\item \(\eAii=\begin{bmatrix} 0&-2
\\-2&3 \end{bmatrix}\)
\answer{Eigenvalues \(-1,4\), and corresponding eigenvectors proportional to \((2,1)\), \((-1,2)\).
The inverse has eigenvalues \(-1,1/4\).}

\item \(\eAii=\begin{bmatrix} 5/2&-2
\\-2&5/2 \end{bmatrix}\)
\answer{Eigenvalues \(1/2,9/2\), and corresponding eigenvectors proportional to \((1,1)\), \((-1,1)\).
The inverse has eigenvalues \(2,2/9\).}

\item \(\eAii=\begin{bmatrix} 3&8
\\8&-9 \end{bmatrix}\)
\answer{Eigenvalues \(-13,7\), and corresponding eigenvectors proportional to \((-1,2)\), \((2,1)\).
The inverse has eigenvalues \(-1/13,1/7\).}

\item \(\eAii=\begin{bmatrix} -2&1
\\1&14/5 \end{bmatrix}\)
\answer{Eigenvalues \(-11/5,3\), and corresponding eigenvectors proportional to \((-5,1)\), \((1,5)\).
The inverse has eigenvalues \(-5/11,1/3\).}

\item \(\eAii=\begin{bmatrix} -1&-2&0
\\-2&0&2
\\0&2&1 \end{bmatrix}\)
\answer{Eigenvalues \(-3,0,3\), and corresponding eigenvectors proportional to \((2,2,-1)\), \((2,-1,2)\), \((-1,2,2)\).
The inverse does not exist.}

\item \(\eAii=\begin{bmatrix} 2&1&3
\\1&0&-1
\\3&-1&2 \end{bmatrix}\)
\answer{Eigenvalues \(-2,1,5\), and corresponding eigenvectors proportional to \((-1,1,1)\), \((1,2,-1)\), \((1,0,1)\).
The inverse has eigenvalues \(-1/2,1,1/5\).}

\item \(\eAii=\begin{bmatrix} 0&-1&-1
\\-1&1&0
\\-1&0&1 \end{bmatrix}\)
\answer{Eigenvalues \(-1,1,2\), and corresponding eigenvectors proportional to \((2,1,1)\), \((0,-1,1)\), \((1,-1,-1)\).
The inverse has eigenvalues \(-1,1,1/2\).}

\item \(\eAii=\begin{bmatrix} -1&3/2&3/2
\\3/2&-3&-1/2
\\3/2&-1/2&3 \end{bmatrix}\)
\answer{Eigenvalues \(-4,-1/2,7/2\), and corresponding eigenvectors proportional to \((-3,5,1)\), \((3,2,-1)\), \((1,0,3)\).
The inverse has eigenvalues \(-1/4,-2,2/7\).}

\end{parts}
\end{exercise}





\begin{exercise} \label{ex:} 
Each plot below shows (unit) vectors~\xv\ (blue), and for some \(2\times 2\) matrix~\(A\) the corresponding vectors~\(A\xv\) (red) adjoined. 
For each plot of a matrix~\(A\) there is a companion plot of the inverse matrix~\(A^{-1}\).
By roughly estimating eigenvalues and eigenvectors by eye, identify the pairs of plots corresponding to each matrix and its inverse.
\begin{parts}
\item \eRose{1.9}{-1.9}{-1.9}{3.3}
\item \eRose{2.7}{1.2}{1.2}{-2.}
\item \eRose{0.3}{0.7}{0.7}{0.1}
\item \eRose{0.2924}{0.17544}{0.17544}{-0.39474}
\item \eRose{0.93333}{0.2}{0.2}{0.4}
\item \eRose{1.2}{-0.6}{-0.6}{2.8}
\item \eRose{0.836364}{-0.036364}{-0.036364}{0.436364}
\item \eRose{1.2406}{0.71429}{0.71429}{0.71429}
\item \eRose{-0.21739}{1.52174}{1.52174}{-0.65217}
\item \eRose{1.2}{0.1}{0.1}{2.3}
\end{parts}
\answer{a-h, b-d, c-i, e-f, g-j}
%\begin{comment} try some, choose, then random order
%\begin{verbatim}
%for i=1:9
%a=eye(2)/2+randn(2)/2; a=round((a+a')*10)/10;
%b=inv(a);
%Ainv=[a(:)';b(:)']
%end
%\end{verbatim}
%\end{comment}
\end{exercise}





\begin{exercise} \label{ex:} 
For the symmetric matrices of \autoref{ex:smoevs}, confirm that eigenvectors corresponding to distinct eigenvalues are orthogonal (\autoref{thm:orthoevec}).  Show your working.
\end{exercise}






\begin{exercise} \label{ex:} 
For an \(n\times n\) symmetric matrix,
\begin{itemize}
\item  eigenvectors corresponding to different eigenvalues are orthogonal (\autoref{thm:orthoevec}), and
\item  there are generally \(n\)~eigenvalues.
\end{itemize}
Which of the illustrated 2D examples of \autoref{ex:evecpic} appear to come from symmetric matrices, and which appear to come from non-symmetric matrices?
\end{exercise}





\begin{exercise} \label{ex:smnoevs} 
For each of the following \emph{non-symmetric} matrices, confirm that eigenvectors corresponding to distinct eigenvalues are \emph{not} orthogonal.  Show and comment on your working.
Find eigenvectors by hand for \(2\times 2\) and \(3\times 3\) matrices, and compute with \script\ for \(3\times 3\) matrices and larger (using \verb|eig()| and \verb|V'*V|).
%\begin{verbatim}
%format rat,n=2
%for i=1:10^n
%a=0+round(randn(n)*4); 
%[v,d]=eig(a); [dn,dd]=rat(diag(d),1e-7);
%if max(abs(dd))<10, A=a, d=diag(d)', vd=diag(1./max(abs(v)))*(v'), break, end
%end
%\end{verbatim}
\begin{parts}
\item \(\eAii=\begin{bmatrix} -2&2
\\3&-3 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((1,1)\), \((-2,3)\).}

\item \(\eAii=\begin{bmatrix}1&-3
\\-2&2 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((3,2)\), \((1,-1)\).}

\item \(\eAii=\begin{bmatrix} 1&-2&6
\\4&3&2
\\-3&1&-8 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((-2,3,1)\), \((2,-2,-1)\), \((-13,3,14)\).}

\item \(\eAii=\begin{bmatrix} -1&-2&9
\\3&-6&3
\\0&0&3 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((1,1,0)\), \((2,3,0)\), \((25,13,14)\).}

%\begin{verbatim}
%format short, n=5; A=eye(n)+round(randn(n)*3), [v,d]=eig(A); d=diag(d)', format bank, vd=v'
%\end{verbatim}

\item \(\eAii=\begin{bmatrix} 2&1&-2&1
\\1&-3&4&-4
\\3&2&4&-5
\\-3&-1&-3&0 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((-.39,.44,.76,-.28)\), \((.58,-.41,-.68,.18)\), \((.22,-.94,.23,.11)\), \((.21,-.53,.53,.62)\) \twodp.}

\item \(\eAii=\begin{bmatrix} -4&0&-5&7
\\2&3&1&-3
\\1&4&-2&4
\\1&-3&4&2 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((-.89,.27,-.24,.28)\), \((-.86,.28,.43,-.01)\), \((-.38,.68,.62,.09)\), \((.44,-.45,.21,.75)\) \twodp.}

\item \(\eAii=\begin{bmatrix} 1&4&3&1&-1
\\5&1&6&-0&1
\\0&4&-3&1&4
\\-3&2&1&4&-1
\\-2&2&2&2&1 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((.01,-.63,.78,.04,-.05)\), \((-.59,.49,.21,-.46,-.39)\), \((.57,.74,.33,-.01,.15)\), \((-.46,-.46,.07,.62,.43)\), \((-.52,-.07,.32,-.53,.59)\) \twodp.}

\item \(\eAii=\begin{bmatrix} 2&0&2&-1&-1
\\2&1&-1&2&-0
\\5&-2&6&2&-1
\\4&0&-2&6&-5
\\2&0&-5&-3&-5 \end{bmatrix}\)
\answer{Eigenvectors proportional to \((.20,-.19,-.10,.32,.90)\), \((.40,-.53,-.35,.12,.65)\), \((-.47,-.83,.03,.18,-.24)\), \((-.24,-.06,-.82,-.39,.35)\), \((.28,-.27,.26,-.87,.17)\) \twodp.}

\end{parts}
\end{exercise}




\begin{exercise} \label{ex:} 
For the symmetric matrices of \autoref{ex:smoevs}, use \script\ to compute an \svd\ (\(\usv\)) of each matrix.
Confirm that each column of~\(V\) is an eigenvector of the matrix (that is, proportional to what the exercise found) and the corresponding singular value is the magnitude of the corresponding eigenvalue (\autoref{thm:smevec}).
Show and discuss your working.
\end{exercise}




\begin{exercise} \label{ex:} 
To complement the previous exercise, for each of the \emph{non-symmetric} matrices of \autoref{ex:smnoevs}, use \script\ to compute an \svd\ (\(\usv\)) of each matrix.
Confirm that each column of~\(V\) is \emph{not} an eigenvector of the matrix, and the singular values do \emph{not} appear closely related to the eigenvalues.
Show and discuss your working.
\end{exercise}











\begin{exercise} \label{ex:} 
Let \(A\) be an \(m\times n\)  matrix with \svd\ \(A=\usv\).
Prove that for any \(j=1,2,\ldots,n\)\,, the \(j\)th~column of~\(V\),~\(\vv_j\), is an eigenvector of the \(n\times n\) {symmetric matrix}~\(\tr AA\) corresponding to the eigenvalue \(\lambda_j=\sigma_j^2\) (or \(\lambda_j=0\) if \(m<j\leq n\)).
%Further, if \(A\)~is  an \(n\times n\) \idx{symmetric matrix}, then \(\vv_j\)~is an eigenvector of~\(A^2\) corresponding to the eigenvalue~\(\sigma_j^2\).
\end{exercise}



\begin{exercise} \label{ex:ematpow:iii} 
Prove Theorem part~\ref{thm:ematpow:iii} using parts~\ref{thm:ematpow:i} and~\ref{thm:ematpow:ii}: that if matrix~\(A\) is {invertible}, then for any integer~\(n\), \(\lambda^n\) is an {eigenvalue} of~\(A^n\) with corresponding {eigenvector}~\xv.
\end{exercise}





\begin{exercise} \label{ex:} 
For each of the following matrices, give reasons as to whether the matrix is \idx{orthogonally diagonalisable}, and if it is then find an orthogonal matrix~\(V\) that does so and the corresponding diagonal matrix~\(D\).
Use \script\ for the larger matrices.
%\begin{verbatim}
%format rat,n=4
%for i=1:10^n
%k=ceil(3*rand); a=eye(n)/2+randn(n); a=0+round((a+(rand<1)*a')*k)/k;
%[v,d]=eig(a); [dn,dd]=rat(diag(d),1e-7);
%if max(abs(dd))<10, A=a, d=diag(d)', vv=v*diag(1./max(abs(v))), break, end
%end
%\end{verbatim}

\begin{parts}
\item \(\eAii=\begin{bmatrix} 0&-2/3
\\-1&-1/3 \end{bmatrix}\)
\answer{Not symmetric, so not orthogonally diagonalisable.}

\item \(\eAii=\begin{bmatrix} 2/3&2
\\2&7/3 \end{bmatrix}\)
\answer{\(V=\frac1{\sqrt{13}}\begin{bmatrix} -3&2\\2&3 \end{bmatrix}\), 
and \(D=\diag(-2/3,11/3)\).}

\item \(\eAii=\begin{bmatrix} 2&0&1
\\0&2&-1
\\1&-1&3 \end{bmatrix}\)
\answer{\(V=\begin{bmatrix} -\fs3&\fs2&\fs6
\\\fs3&\fs2&-\fs6
\\\fs3&0&\fs[2]6 \end{bmatrix}\), 
and \(D=\diag(1,2,4)\).}

\item \(\eAii=\begin{bmatrix} 2&0&2
\\1&-1&0
\\-1&0&-1 \end{bmatrix}\)
\answer{Not symmetric, so not orthogonally diagonalisable.}

\item \(\eAii=\begin{bmatrix} 0&-2&-2&0
\\-2&0&-2&0
\\-2&-2&2&2
\\0&0&2&-2 \end{bmatrix}\)
\answer{\(V=\begin{bmatrix} -.50&-.41&.71&-.29
\\-.50&-.41&-.71&-.29
\\-.50&.00&.00&.87
\\.50&-.82&.00&.29 \end{bmatrix}\) \twodp, 
and \(D=\diag(-4,-2,2,4)\).}

\item \(\eAii=\begin{bmatrix} -1&0&0&0
\\0&3&-2&-2
\\0&-2&0&1
\\0&-2&1&0 \end{bmatrix}\)
\answer{\(V=\begin{bmatrix} .00&1.00&.00&.00
\\.06&.00&.57&-.82
\\-.64&.00&.65&.41
\\.76&.00&.50&.41 \end{bmatrix}\) \twodp, 
and \(D=\diag(-1,-1,-1,5)\).}

\item \(\eAii=\begin{bmatrix} 3&1&1&1&-1
\\-1&0&0&0&1
\\0&0&0&0&1
\\0&1&-1&1&-1
\\0&0&0&0&1 \end{bmatrix}\)
\answer{Not symmetric, so not orthogonally diagonalisable.}

\item \(\eAii=\begin{bmatrix} 3&0&2&0&0
\\0&3&1&0&-1
\\2&1&1&0&-1
\\0&0&0&3&2
\\0&-1&-1&2&1 \end{bmatrix}\)
\answer{\(V=\begin{bmatrix} -.32&.37&.41&-.63&.45
\\-.00&.37&-.82&-.00&.45
\\.63&-.55&-.00&-.32&.45
\\-.32&-.37&-.41&-.63&-.45
\\.63&.55&.00&-.32&-.45 \end{bmatrix}\) \twodp, 
and \(D=\diag(-1,0,3,4,5)\).}

\end{parts}
\end{exercise}





\begin{exercise} \label{ex:} 
Let matrix~\(A\) be invertible and orthogonally diagonalisable.  
Show that the inverse~\(A^{-1}\) is \idx{orthogonally diagonalisable}.
\end{exercise}




\begin{exercise} \label{ex:} 
Suppose matrices~\(A\) and~\(B\) are orthogonally diagonalisable by the same orthogonal matrix~\(V\).  
Show that \(AB=BA\) and that the product~\(AB\) is \idx{orthogonally diagonalisable}.
\end{exercise}




\begin{exercise} \label{ex:} 
For each of the given symmetric matrices, say~\(A\), find a symmetric matrix~\(X\) such that \(X^2=A\)\,.  
That is, find a \idx{square-root} of the matrix.
%\begin{verbatim}
%format rat,n=2
%for i=1:10^n
%k=ceil(3*rand); a=eye(n)/2+randn(n); a=0+round((a+a')*k)/k;
%[v,d]=eig(a); [dn,dd]=rat(diag(d),1e-7);
%if max(abs(dd))<10, A=a^2, X=a, d=diag(d)', vv=v*diag(1./max(abs(v))), break, end
%end
%\end{verbatim}
\begin{parts}
\item \(\eAii=\begin{bmatrix} 5/2&3/2
\\3/2&5/2 \end{bmatrix}\)
\answer{\(X=\begin{bmatrix} 3/2&1/2
\\1/2&3/2 \end{bmatrix}\)}

\item \(\eAii=\begin{bmatrix} 6&-5&-5
\\-5&10&1
\\-5&1&10 \end{bmatrix}\)
\answer{\(X=\begin{bmatrix} 2&-1&-1
\\-1&3&0
\\-1&0&3 \end{bmatrix}\)}

\item \(\eAii=\begin{bmatrix} 2&1&3
\\1&2&3
\\3&3&6 \end{bmatrix}\)
\answer{\(X=\begin{bmatrix} 1&0&1
\\0&1&1
\\1&1&2 \end{bmatrix}\)}

\item How many possible answers are there for each of the given matrices?  Why?
\answer{four, eight and four respectively.}

\item For all symmetric matrices~\(A\), show that if every eigenvalue of~\(A\) is non-negative, then there exists a symmetric matrix~\(X\) such that \(A=X^2\).  

\item Continuing the previous part, how many such matrices~\(X\) exist?  Justify your answer.

\end{parts}
\end{exercise}






\begin{exercise} \label{ex:} 
For each of the following \idx{conic section}s, draw a pair of \idx{coordinate axes} for a coordinate system in which the algebraic description of the curves should be simplest.
\newcommand{\myellipse}[3]{\rotatebox{#3}{
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  ,axis lines=none ]
  \addplot[blue,thick,smooth,domain=0:360] ({#1*cos(x)},{#2*sin(x)});
  \end{axis}
\end{tikzpicture}}}%
\newcommand{\myhyper}[3]{\rotatebox{#3}{
\begin{tikzpicture}
  \begin{axis}[footnotesize,font=\footnotesize ,axis equal image
  ,axis lines=none ]
  \addplot[blue,thick,smooth,domain=-2:2] {#1*sqrt(1+x^2*#2)};
  \addplot[blue,thick,smooth,domain=-2:2] {-#1*sqrt(1+x^2*#2)};
  \end{axis}
\end{tikzpicture}}}%

\begin{parts}
\item \myellipse{1}{3}{60}
\item \myhyper{1}{3}{20}
\item \myellipse{3.01}{1.37}{10}
\item \myhyper{1.70}{1.96}{76}
\item \myellipse{2.42}{3.72}{58}
\item \myhyper{2.63}{2.31}{150}
\end{parts}
\end{exercise}





\begin{exercise} \label{ex:} 
By shifting to a new \idx{coordinate axes}, find the canonical form of each of the following quadratic equations, and hence describe each curve. 
%\begin{verbatim}
%cs=round(randn(1,5)*4), md=-cs(3:4)./cs(1:2)/2, absq=(-cs(5)+md.^2*cs(1:2)')./cs(1:2)
%\end{verbatim}
\begin{enumerate}
\item \(-4x^2+5y^2+4x+4y-1=0\)
\answer{hyperbola, centred \((1/2,-2/5)\), \(-\frac{{x'}^2}{1/5}+\frac{{y'}^2}{4/25}=1\)}

\item \(-4y^2-6x -4y +2=0\)
\answer{parabola, base \((1/2,-1/2)\), \({x'}=-\frac23{{y'}^2}\)}

\item \(5x^2-y^2 -2y +4=0\)
\answer{hyperbola, centred \((0,-1)\), \(-\frac{{x'}^2}{1}+\frac{{y'}^2}{5}=1\)}

\item \(3x^2 +5y^2 +6x +4y +2=0\)
\answer{ellipse, centred \((-1,-2/5)\), \( \frac{{x'}^2}{3/5}+\frac{{y'}^2}{9/25}=1\)}

\item \(-2x^2 -4y^2 +7x -2y +1=0\)
\answer{ellipse, centred \((7/4,-1/4)\), \( \frac{{x'}^2}{59/16}+\frac{{y'}^2}{59/32}=1\)}

\item \(-9x^2-y^2 -3y +6=0\)
\answer{ellipse, centred \((0,-3/2)\), \( \frac{{x'}^2}{11/12}+\frac{{y'}^2}{33/4}=1\)}

\item \(-x^2-4x -8y +4=0\)
\answer{parabola, base \((-2,1)\), \({y'}=-\frac18{{x'}^2}\)}

\item \(-8x^2-y^2-2x +2y -3=0\)
\answer{No solution points exist.}

\end{enumerate}
\end{exercise}






\begin{exercise} \label{ex:} 
By rotating to new \idx{coordinate axes}, identify each of the following \idx{conic section}s.  
Write the quadratic terms in the form \(\tr\xv A\xv\), and use the eigenvalues and eigenvectors of matrix~\(A\).
%\begin{verbatim}
%for i=1:99,c=0+round(randn(1,4)*4),A=[c(1) c(2)/2;c(2)/2 c(3)],[V,D]=eig(A);VV=sign(V).*V.^2,d=diag(D)',absq=-c(4)./d,[vn,vd]=rat(VV,1e-7);if max(vd(:))<11,break,end,end
%\end{verbatim}

\begin{parts}
\item \(4xy +3y^2 -3=0\)
\answer{With axes \(\iv'=(-\fs[2]5,\fs5)\) and \(\jv'=(\fs5,\fs[2]5)\), 
\( -\frac{{x'}^2}{3}+\frac{{y'}^2}{3/4}=1\) is hyperbola.}

\item \(3x^2 +8xy -3y^2 =0\)
\answer{With axes \(\iv'=(\fs5,-\fs[2]5)\) and \(\jv'=(\fs[2]5,\fs5)\), 
\( -5{{x'}^2}{}+5{{y'}^2}{}=0\) is pair of straight lines.}

\item \(2x^2 -3xy +6y^2 -5=0\)
\answer{In axes \(\iv'=(\fs[3]{10},\fs{10})\) and \(\jv'=(-\fs{10},\fs[3]{10})\), 
\( \frac{{x'}^2}{10/3}+\frac{{y'}^2}{10/13}=1\) is ellipse.}

\item \(-4x^2 +3xy -4y^2 -2=0\)
\answer{No solution points exist.}

\item \(-4x^2 +3xy -4y^2 +11=0\)
\answer{In axes \(\iv'=(\fs2,-\fs2)\) and \(\jv'=(\fs2,\fs2)\), 
\( \frac{{x'}^2}{2}+\frac{{y'}^2}{22/5}=1\) is ellipse.}

\item \(3x^2 -2xy +3y^2 -6=0\)
\answer{In axes \(\iv'=(\fs2,\fs2)\) and \(\jv'=(-\fs2,\fs2)\), 
\( \frac{{x'}^2}{3}+\frac{{y'}^2}{3/2}=1\) is ellipse.}

\item \(-x^2 +2xy -y^2 +5=0\)
\answer{In axes \(\iv'=(\fs2,-\fs2)\) and \(\jv'=(\fs2,\fs2)\), 
\( \frac{{x'}^2}{5/2}=1\) is pair of parallel lines.}

\item \(-x^2 -4xy +2y^2 -6=0\)
\answer{In axes \(\iv'=(\fs[2]5,\fs5)\) and \(\jv'=(-\fs5,\fs[2]5)\), 
\( -\frac{{x'}^2}{3}+\frac{{y'}^2}{2}=1\) is hyperbola.}


\end{parts}
\end{exercise}





\begin{exercise} \label{ex:} 
By rotating and shifting to new coordinate axes, identify each of the following conic sections from its equation.
%\begin{verbatim}
%for i=1:99,c=0+round(randn(1,4)*4),A=[c(1) c(2)/2;c(2)/2 c(3)],[V,D]=eig(A);VV=sign(V).*V.^2,d=diag(D)',absq=-c(4)./d,[vn,vd]=rat(VV,1e-7);if max(vd(:))<11,break,end,end
%a=round(randn(2,1)*6)/2, angle=num2str(atan(V(2)/V(1))*180/pi)
%cs=[c(1:3) -[2 1].*c(1:2)*a -[1 2].*c(2:3)*a c(4)+a'*A*a]
%[X,Y]=meshgrid(linspace(-1,1,41)*7);
%f=cs(1)*X.^2+cs(2)*X.*Y+cs(3)*Y.^2+cs(4)*X+cs(5)*Y+cs(6);
%contour(X,Y,f,[-10 -1 -0.1 0 0.1 1 10]), axis equal
%\end{verbatim}
\begin{enumerate}
\item \( -2x^2 -5xy -2y^2 -\frac{33}2x -15y -32=0\)
\answer{hyperbola centred \((-1,-5/2)\) at angle~\(45^\circ\)}

\item \( -7x^2 +3xy -3y^2 -52x +\frac{33}2y -\frac{381}4=0\)
\answer{ellipse centred \((-7/2,1)\) at angle~\(78^\circ\)}

\item \( -4xy -3y^2 -18x -11y +\frac{37}4=0\)
\answer{hyperbola centred \((4,-9/2)\) at angle~\(-27^\circ\)}

\item \( 2x^2 -y^2 +10x +6y +\frac{11}2=0\)
\answer{hyperbola centred \((-5/2,3)\) at angle~\(0^\circ\)}

\item \( -2x^2 +5xy -2y^2 -\frac{13}2x +\frac52y +\frac{31}4=0\)
\answer{hyperbola centred \((3/2,5/2)\) at angle~\(45^\circ\)}

\item \(-4xy +3y^2 +18x -13y +\frac34=0\)
\answer{hyperbola centred \((7/2,9/2)\) at angle~\(26^\circ\)}

\item \( 6x^2 +6y^2 -12x -42y +\frac{155}2=0\)
\answer{circle centred \((1,7/2)\)}

\item \( 2x^2 -4xy +5y^2 +34x -52y +\frac{335}2=0\)
\answer{ellipse centred \((-11/2,3)\) at angle~\(26^\circ\)}


\end{enumerate}
\end{exercise}






\begin{exercise} \label{ex:} 
For each of the following matrices, say~\(A\), consider the quadratic form \(q(\xv)=\tr\xv A\xv\).
Find coordinate axes, the \idx{principal axes}, such that the quadratic has the \idx{canonical form} in the new coordinates \hlist yn.
Use eigenvalues and eigenvectors, and use \script\ for the larger matrices.
Over all unit vectors, what is the maximum value of~\(q(\xv)\)? and what is the minimum value?

%\begin{verbatim}
%n=2
%for i=1:999
%a=randn(n); A=0+round((a+a')*3); [V,D]=eig(A);
%VV=V.*abs(V); [vvn,vvd]=rat(VV,1e-7);
%if max(vvd(:))<13,A=A,num2str(diag(D)',"%7.2f"), VV=VV, break, end
%end
%\end{verbatim}
\begin{parts}
\item \(\eAii=\begin{bmatrix} 0&-5
\\-5&0 \end{bmatrix}\)
\answer{\(q= -5y_1^2 +5y_2^2\), max\(=5\), min=\(-5\)}

\item \(\eAii=\begin{bmatrix} 3&2
\\2&0 \end{bmatrix}\)
\answer{\(q= -y_1^2 +4y_2^2\), max\(=4\), min=\(-1\)}

\item \(\eAii=\begin{bmatrix} -1&2&-2
\\2&0&0
\\-2&0&-2 \end{bmatrix}\)
\answer{\(q= -4y_1^2 -y_2^2 +2y_3^2\), max\(=2\), min=\(-4\)}

\item \(\eAii=\begin{bmatrix} 2&1&-1
\\1&3&2
\\-1&2&3 \end{bmatrix}\)
\answer{\(q= 3y_2^2 +5y_3^2\), max\(=5\), min=\(0\)}

\item \(\eAii=\begin{bmatrix} 6&0&-3&1
\\0&-1&5&-3
\\-3&5&-4&-7
\\1&-3&-7&0 \end{bmatrix}\)
\answer{\twodp\ \(q= -10.20y_1^2 -3.56y_2^2 +4.81y_3^2 +9.95y_4^2\), max\(=9.95\), min=\(-10.20\)}

\item \(\eAii=\begin{bmatrix} -5&-1&1&2
\\-1&2&4&1
\\1&4&-7&7
\\2&1&7&2 \end{bmatrix}\)
\answer{\twodp\ \(q= -11.53y_1^2 -5.72y_2^2 +1.67y_3^2 +7.58y_4^2 y_5^2\), max\(=7.58\), min=\(-11.53\)}

\item \(\eAii=\begin{bmatrix} 1&1&-1&-6&-7
\\1&0&3&3&-6
\\-1&3&12&4&-4
\\-6&3&4&1&-2
\\-7&-6&-4&-2&3 \end{bmatrix}\)
\answer{\(q= -8.82y_1^2 -4.71y_2^2 +3.04y_3^2 +10.17y_4^2 +17.03y_5^2\), max\(=17.03\), min=\(-8.82\)}

\item \(\eAii=\begin{bmatrix} 12&-3&-3&-4&-6
\\-3&0&0&2&-5
\\-3&0&-4&1&-3
\\-4&2&1&-5&2
\\-6&-5&-3&2&0 \end{bmatrix}\)
\answer{\twodp\ \(q= -9.74y_1^2 -5.69y_2^2 -2.65y_3^2 +5.32y_4^2 +15.76y_5^2\), max\(=15.76\), min=\(-9.74\)}


\end{parts}
\end{exercise}







\begin{exercise} \label{ex:} 
For any given \(n\times n\) symmetric matrix~\(A\) consider the quadratic form \(q(\xv)=\tr\xv A\xv\). 
For general vectors~\xv, not necessarily unit vectors, what is the maximum value of~\(q(\xv)\) in terms of~\(|\xv|\)? and what is the minimum value of~\(q(\xv)\)?  
Justify your answer.
\end{exercise}





\begin{exercise} \label{ex:optqform} 
Complete the proof of \autoref{thm:optqform} by detailing the proof of the upper bound, and that the upper bound is achieved for an appropriate unit eigenvector.
\end{exercise}




\begin{exercise} \label{ex:} 
For a symmetric matrix, discuss the similarities and differences between the \svd\ and the diagonalisation factorisation, the singular values and the eigenvalues, and the singular vectors and the eigenvectors.
\end{exercise}



\begin{comment}%{ED498555.pdf}
why, what caused X?
how did X occur?
what-if? what-if-not?
how does X compare with Y?
what is the evidence for X?
why is X important?
\end{comment}


